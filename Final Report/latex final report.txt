\documentclass{report}

\usepackage{blindtext}
\usepackage{titlesec}
\usepackage{listings}
\usepackage{geometry}
\usepackage{mathtools}
\usepackage[utf8]{inputenc}
\usepackage{siunitx}
\usepackage{soul}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{float}
\usepackage{units}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{minted}
\usepackage{caption}
\usepackage{csquotes}
\usepackage{nomencl}
\usepackage{fancyhdr}
 
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{\rightmark}
\fancyhead[R]{\thepage}

\renewcommand{\sectionmark}[1]{\markright{\thesection~#1}}

\usepackage[
backend=biber,
style=numeric,
sorting=none        % sort by order of appearance
]{biblatex}
\addbibresource{biblio.bib}

\setlength{\parskip}{0.7em}   		%paragraph separation = 0.9em
\setlength{\parindent}{0pt} 		% 0 paragraph indent


\begin{document}


\begin{titlepage}
\newgeometry{top=36mm,bottom=36mm,left=40mm,right=40mm}
\setlength{\parindent}{0pt}
\setlength{\parskip}{0pt}

{
\Large
\raggedright
  Imperial College London\\[17pt]
  Department of Electrical and Electronic Engineering\\[17pt]
  Final Year Project 2017: Final Report\\[17pt]
}

\rule{\columnwidth}{3pt}
\vfill
\centering
\includegraphics[width=0.7\columnwidth,height=60mm,keepaspectratio]{ic_crest.png}
\vfill
\setlength{\tabcolsep}{0pt}

\begin{tabular}{p{40mm}p{\dimexpr\columnwidth-40mm}}
Project Title: & \textbf{Quality-preserving Speech Intelligibility Enhancement using a Kalman Filter} \\[12pt]
Student: & \textbf{Jia Ying Goh} \\[12pt]
CID: & \textbf{00749529} \\[12pt]
Course: & \textbf{4T} \\[12pt]
Project Supervisor: & \textbf{Brookes, D.M.} \\[12pt]
Second Marker: & \textbf{Evers, C.} \\[12pt]
\end{tabular}
\end{titlepage}

\restoregeometry


\pagenumbering{roman}
\chapter*{Abstract}
\thispagestyle{plain}
\addcontentsline{toc}{chapter}{Abstract}
Speech enhancement algorithms aim to reduce the background noise of a noise-corrupted speech input without distorting the original clean speech. In real-world situations, this can be very challenging. Although many algorithms have been developed to improve the Signal-to-Noise Ratio (SNR) of the noisy input, they also introduce speech distortion and artifacts such as musical noise, damaging speech quality and intelligibility. Recently, there has been growing psychoacoustic and physiological evidence to support the use of the modulation domain for speech enhancement, where the modulation domain is defined as the temporal variations of the acoustic spectral components. This report proposes modifications to existing modulation-domain speech processing methods, where an Ideal Binary Mask (IBM) will be applied to training samples of noisy speech to obtain averaged statistical information that can then be applied on new test samples. The goal is to use this data to improve the performance of an existing modulation-domain Kalman Filter (MDKF). The performance of these proposed modifications is assessed by measuring the segmental SNR (segSNR), speech quality (using Perceptual Evaluation of Speech Quality or PESQ) and speech intelligibility (Short-Time Objective Intelligibility or STOI) of the enhanced speech. Results show that the developed algorithms provide varying degrees of improvements in both speech quality and intelligibility over a range of input noise levels.
\clearpage

\chapter*{Acknowledgements}
\addcontentsline{toc}{chapter}{Acknowledgements}
Firstly, I would like to express my sincere gratitude to my project supervisor, Mr Mike Brookes. His patient guidance and understanding has been very helpful throughout the course of the project, and meetings with him have always produced useful ideas and feedback.

Next, I would like to express my appreciation to my family for their understanding and support, allowing me to successfully complete the project.

Last but not least, I would like to thank my friends who have been by my side to support and motivate me throughout the course of the project.


\newpage
\tableofcontents
\clearpage

 
\newpage
\addcontentsline{toc}{chapter}{\listfigurename}
\listoffigures
 
 
\newpage
\addcontentsline{toc}{chapter}{\listtablename}
\listoftables


\nomlabelwidth=30mm				% indentation between nomenclature acronym and description
\makenomenclature
\nomenclature{\textbf{LMS}}{Least Mean Squares}
\nomenclature{\textbf{NLMS}}{Normalised Least Mean Squares}
\nomenclature{\textbf{RLS}}{Recursive Least Squares}
\nomenclature{\textbf{STFT}}{Short-Time Fourier Transform}
\nomenclature{\textbf{MMSE}}{Minimum Mean Squared Error}
\nomenclature{\textbf{MS}}{Minimum Statistics}
\nomenclature{\textbf{VAD}}{Voice Activity Detector}
\nomenclature{\textbf{IBM}}{Ideal Binary Mask}
\nomenclature{\textbf{TBM}}{Target Binary Mask}
\nomenclature{\textbf{AMS}}{Analysis-Modification-Synthesis}
\nomenclature{\textbf{KF}}{Kalman Filter}
\nomenclature{\textbf{LPC}}{Linear Prediction Coefficients}
\nomenclature{\textbf{TDKF}}{Time-Domain Kalman Filter}
\nomenclature{\textbf{MDKF}}{Modulation-Domain Kalman Filter}
\nomenclature{\textbf{PESQ}}{Perceptual Evaluation of Speech Quality}
\nomenclature{\textbf{STOI}}{Short-Time Objective Intelligibility}
\nomenclature{\textbf{SNR}}{Signal-to-Noise Ratio}
\nomenclature{\textbf{segSNR}}{Segmental Signal-to-Noise Ratio}
\nomenclature{\textbf{fwSNRseg}}{Frequency-Weighted Signal-to-Noise Ratio}
\nomenclature{\textbf{MOS}}{Mean Opinion Score}
\nomenclature{\textbf{PDF}}{Probability Density Function}
\nomenclature{\textbf{ML}}{Maximum Likelihood}
\nomenclature{\textbf{PSD}}{Power Spectral Density}
\nomenclature{\textbf{SPP}}{Speech Presence Probability}
\printnomenclature


\chapter{Introduction}
\pagenumbering{arabic}

\section{Motivation}

In today's highly interconnected world, communication between people, as well as with the world around them, is a major and critical aspect of their lives. Among the methods of communication (including but not limited to speech, text, images and bodily cues), speech generally stands out as the most efficient. Other methods such as visual indicators are sometimes useful to communicate ideas and thoughts, but a complex message is often best brought across via speech.

Applications utilising speech are thus widespread and numerous, and are generally designed to make use of clean speech. In a real-world environment, however, when speech is recorded, the recording inherently picks up not just the speech signal of interest, but also undesired background noise and channel noise. This damages the quality and intelligibility of the recorded speech, which poses a major problem for these applications requiring undamaged speech. Speech enhancement is hence often needed, with the goal of restoring the desired speech signal from the noisy mix, ideally by eliminating this noise while retaining the quality and intelligibility of the original speech signal.

There are various types of noise, including but not limited to additive noise, convolution noise and transcoding noise \cite{Loizou2007}. Additive acoustic noise that is uncorrelated with the speech signal generally degrades the intelligibility and quality of the perceived speech, and in cases of large noise may dominate and mask out the original speech. Convolution noise, on the other hand, manifests as reverberation, which is introduced by acoustic reflection, degrading intelligibility. Unlike additive noise, reverberation is highly correlated with the speech signal. Finally, transcoding noise can occur due to amplitude clipping in a microphone and appears as distortion. This project is concerned with the removal of additive acoustic noise.

Speech enhancement methods can be broadly classified into two types. Single-channel methods consider a single signal source. On the other hand, multi-channel methods consider multiple speech signals obtained from multiple microphones, where additional noise reduction can be achieved using information unavailable to a method relying on a single source, such as phase alignment from multiple microphones, leading to better overall noise reduction. However, this introduces additional costs and complexity, and in many applications such as hearing aids and mobile phones, single-channel methods are necessary due to constraints such as size. This project focuses on single-channel speech enhancement methods.

However, speech enhancement is complex. Traditional speech enhancement techniques such as spectral subtraction have very successfully improved speech quality by attenuating noise, but they tend to introduce speech spectral distortion \cite{FukaneSahare2011}, thus damaging its intelligibility. This project therefore aims to modify existing techniques to improve both the quality and intelligibility of speech.

\section{Project Objectives}

In this project, the objective is to improve both speech quality and intelligibility by modifying an existing speech enhancement algorithm. Standard tests for quality and intelligibility, include the Perceptual Evaluation of Speech Quality (PESQ, \cite{PESQ}) and Short-Time Objective Intelligibility (STOI, \cite{STOI}), will be used to quantify the enhanced speech. 

Specifically, this project aims to modify an existing speech enhancement algorithm based on a Kalman filter, by further incorporating additional information obtained from a so-called ``ideal binary mask''. The proposed method is to scale the predicted value in the Kalman filter and modify its variance by an amount pre-determined from training data, with the goal that PESQ and STOI increase.

\section{Project Scope}

This project assumes the binary mask is already provided; how it is generated is therefore out of scope of this project. This project focuses on incorporating a given estimated binary mask into an existing Kalman filter speech enhancement implementation.

This project makes use of MATLAB and signal processing techniques. In particular, the project utilises VOICEBOX, a speech processing toolbox for MATLAB \cite{VOICEBOX}, which is included in the Imperial College London Software Library.

\section{Report Overview}

This report is categorised into a few main chapters. Chapter 1 introduces and provides context to the problem, while providing a high-level overview of the project objectives and scope. Chapter 2 provides detailed description of the required background information, particularly providing in-depth information regarding the relevant algorithms and performance evaluation methods.

Chapter 3 analyses the project, breaking down the requirements into specific deliverables. It also describes the implementation of the baseline algorithm and how the improvements will be made. Chapter 4 briefly covers the testing methodology and how the algorithms will be evaluated.

Chapter 5 covers the first proposed modification, whereby an ideal binary mask is applied to a set of training speech samples. This gives useful statistical information that is then used to improve on a baseline Kalman filter implementation. Its performance relative to existing methods is evaluated and discussed.

Chapter 6 explores a different approach. Here, the linear prediction coefficients used in a Kalman filter algorithm are modified by using a weighted sum to calculate the autocorrelation function, with weights determined from an ideal binary mask applied to the input signal. As before, this enhancer is evaluated and discussed relative to existing methods.

A third modification is proposed in Chapter 7. Noise estimation is an important part of speech enhancement, and a method is proposed to incorporate binary mask information to improve the accuracy of noise estimation in a Kalman filter-based speech enhancer. Finally, Chapter 8 concludes the project and briefly explores possible areas for future work.


\chapter{Background}

The goal of a speech enhancement algorithm is to reduce the background noise of a noise-corrupted speech input without distorting the underlying clean speech \cite{SpeechEnhancement2005}. Traditionally, speech enhancement algorithms for noise reduction can be grouped into three main categories: noise reduction via filtering techniques, noise reduction via spectral restoration, and speech-model-based noise reduction methods \cite{Springer2007}. Overall, speech enhancement techniques aim to improve the speech using audio signal processing techniques.

Some widely-used speech enhancement methods are described in this chapter. This chapter also provides the background to some relevant aspects of speech enhancement, including possible operating domains, noise estimation, and performance evaluation measures.

\section{Enhancement Domains}

Speech enhancement can be performed in one of several domains. The following sections briefly describe these domains.

\subsection{Time Domain}

In the time domain, speech is usually enhanced using fixed or adaptive filtering techniques \cite{Widrow1975}. Fixed filters require prior knowledge of both the clean signal and noise, while this is not required for adaptive filters, which are able to adjust their parameters according to an optimisation algorithm, with little to no knowledge of the signal or noise characteristics, thus being more practical.

There are different approaches to adaptive filtering, one of which is the Least-Mean-Squares (LMS) filter. LMS algorithms aim to mimic a desired filter by finding a set of filter coefficients to minimise the mean squared error, where the error is the difference between the desired and actual signal \cite{WidrowHoff60}. The basic idea is to iteratively update the filter coefficients to approach the optimum coefficients, using a certain step size at each iteration. The LMS is a stochastic gradient descent approach, meaning that it is adapted based on the current error. It is, however, sensitive to input scaling, making it difficult to find an optimum step size to guarantee convergence. This limitation motivated the development of a variant, the Normalised Least Mean Squares (NLMS) algorithm, which is a variant of LMS that solves this problem by normalising with the power of the input \cite{DAG2013}.

Another popular approach is the Recursive Least Squares (RLS) algorithm, which recursively finds the filter coefficients to minimize a weighted least squares cost function relating to the input signal. This is unlike LMS and NLMS, which aim to reduce the mean squared error. Compared to LMS and NLMS, the RLS exhibits very fast convergence, but at the cost of higher computational complexity.

Generally, adaptive filters are used when there is some quantity to be minimised. For example, an adaptive filter can be implemented iteratively with a time delay to estimate and remove mains noise, which is periodic and thus can be estimated from previous samples. This is not as applicable in typical speech enhancement, which is concerned with reducing random noise.

\subsection{Time-Frequency Domain}\label{TF}

Speech enhancement can be performed in the time-frequency (T-F) domain, which analyses signals in both time and frequency domains simultaneously, using various T-F representations \cite{Cohen94}. Assuming speech is quasi-stationary over sufficiently short periods \cite{Riley1989book}, the noisy input speech signal is divided into overlapping short frames, typically using a Hamming window, where the frame length is a compromise between temporal and frequency resolution \cite{GriffinLim84}. These frames will be called acoustic frames, and are separate from the modulation frames referred to in the modulation domain in Section \ref{ModulationDomain}. Performing the Fourier transform on these frames produces a T-F matrix, on which processing can then be done. This entire process is called the short-time Fourier Transform (STFT).

Generally, T-F enhancement methods apply a gain function to suppress T-F regions which are noise-dominated while preserving speech-dominated regions, typically on the magnitude spectrum only. Computing the gain function depends on the noisy power spectrum, which needs to be estimated separately. After processing, inverse STFT followed by overlap-add reconstruction is performed to produce the enhanced time-domain speech signal. This approach works because speech is relatively sparse, due to limitations of the human ability in terms of speaking and listening i.e. with reasonable levels of noise, the speech can be divided into speech-dominated and noise-dominated regions.

Although this approach can improve the calculated signal-to-noise (SNR) of noisy speech, it can lead to undesired ``musical noise'' artifacts. These appear as isolated spectral components of noise and manifest as brief tones in the enhanced speech, which are generally deemed unnatural and disturbing \cite{Cappe94}. This is because the amplitude of the short-time spectrum exhibits large fluctuations in noisy regions. After processing, the enhanced spectrogram consists of randomly located spectral peaks corresponding to the maxima of the original spectrogram, where the regions between these peaks have been suppressed as they are close to or below the averaged estimated noise spectrum. The result is residual noise comprising of sinusoids of random frequencies between each time frame.

This is used in the setup of the algorithm described in Section \ref{MDKF}.

\subsection{Modulation Domain}\label{ModulationDomain}

Modulation-domain processing starts off similarly to T-F processing, in that the noisy input signal undergoes STFT analysis to produce time-varying frequency components.

For speech enhancement, the amplitudes envelope of each frequency band is regarded as one modulation signal; the spectral amplitudes of each frequency band are windowed into overlapping modulation frames, with a separate modulation frame length and frame overlap compared to the acoustic frame length and overlap of STFT, where each acoustic frame provides one modulation-domain sample for each frequency bin. If each modulation frame contains $M$ samples (i.e. $M$ acoustic frames form one modulation frame) and each acoustic frame contains $N$ time-domain samples, each modulation frame is constructed from $MN$ time-domain samples. The modulation-domain signal has a frequency determined by the acoustic frame increment: since each acoustic frame provides one modulation sample, successive modulation samples are spaced apart by the acoustic frame shift. If the time-domain signal has a sampling frequency of $f_s$ Hz and the acoustic frame shift is $L$ Hz, the modulation-domain sampling frequency is $(f_s/L)$ Hz.

A processing algorithm then estimates the modulation frames of clean speech, which are then overlap-added to reconstruct the modified modulation signals. Combining this with the phase spectrum of the noisy input signal and performing the inverse STFT then produces the enhanced time-domain speech signal.

Even though the acoustic envelope directly contains the speech information, the temporal dynamics of the envelope better represent the information contained in speech \cite{Hermansky98}. These dynamics, which are at significantly lower frequencies than the speech signal itself, are provided in the modulation spectrum, suggesting that working in the modulation domain for speech processing can produce better results. More detailed description is provided in Section \ref{KF}.

\section{Noise Estimation}\label{noiseEstimation}

Noise estimation is an important part of speech processing. In many algorithms including those described in this report, performance is heavily affected by the accuracy of the noise estimation. The algorithms proposed in this project are rely on the estimation of the spectral noise power.

When estimating noise power, because the noise power may change rapidly over time, the spectral estimate has to be updated as often as possible. In speech enhancement, an overestimate or underestimate of the noise power will lead to an over-suppression or under-suppression of the noisy signal, which can lead to excessive residual noise or reduced intelligibility in the enhanced signal.

One way to estimate spectral noise power is to exploit the time periods where speech is absent, which requires detection of speech presence using a voice activity detector (VAD) \cite{VAD}. This method encounters problems when the noise is non-stationary, however, as a sudden increase in noise power may instead be interpreted as the onset of speech. Additionally, if the noise power varies during speech presence, this change can only be detected with some time delay.

To improve noise estimation, methods have been proposed based on minimum statistics (MS) \cite{noiseMS}. The method in \cite{noiseMS} does not use a VAD. Instead, it tracks spectral minima in each frequency band without distinguishing between between speech presence and absence. The power spectrum of the noisy signal is tracked frame-by-frame and observed over a short time period. A general assumption is that within the observed time frame, speech is absent for a non-zero portion of the total time period. The spectral noise power is then obtained from the minimum of the estimated power spectrum of the noisy signal. Similar to VADs, if the noise power rises within the observed time period, it can be tracked with some time delay. Due to the delay, the local noise power estimates tend to be underestimated. This results in residual noise and musical noise artifacts when the noise estimation is used in a speech enhancer.

A more recent method to estimate the noise power spectral density (PSD) is to use a minimum mean-squared-error (MMSE) optimal estimation method \cite{MMSEnoise}, which can be interpreted as a VAD-based noise power estimator \cite{MMSEnoiseBetter}. Generally, MMSE-based estimators are more robust to non-stationary noise and are less computationally intensive as compared to MS-based methods \cite{MMSEnoise}. The estimator in \cite{MMSEnoiseBetter} replaces the hard VAD of the estimator in \cite{MMSEnoise} with a soft speech presence probability (SPP) with a fixed \textit{a priori} SNR as a parameter of the likelihood of speech presence, using a value typical in speech presence. This soft estimation overcomes the issue of random fluctuations in the estimated SNR. This modification automatically makes the estimator unbiased, and it retains similar performance while achieving lower computational complexity compared to \cite{MMSEnoise}.

\section{Spectral Subtraction}\label{specSub}

Spectral subtraction is a widely-used filtering technique which operates in the time-frequency domain. In this method, stationary or slowly-varying noise is attenuated from noisy speech by subtracting the magnitude noise spectrum, estimated during periods where speech is absent \cite{Boll1979}. It is also possible to estimate the noise using a secondary sensor \cite{Widrow1975}. The estimated noise spectrum is then subtracted from the noisy spectrum to produce an approximated spectrum of the clean speech. The spectral error can then be computed and reduced separately. The algorithm can be further refined by incorporating residual noise reduction and non-speech signal attenuation \cite{Boll1979}.

Spectral subtraction works on the back of a few assumptions: firstly, that the background noise is additive to the clean signal \cite{Boll1979}. This assumption means that the complex spectrum of the input noisy signal can be expressed as the sum of the speech spectrum and the noise spectrum. Next, it is assumed that the noise is a stationary or a slowly varying process (locally stationary). This allows the algorithm enough time to accurately formulate an updated estimate for the new noise magnitude spectrum before speech activity starts again. Lastly, the underlying assumption is that noise can be significantly reduced by removing its effect in the magnitude spectrum only i.e. phase spectrum is untouched, and the estimate of the clean speech magnitude spectrum is combined with the phase spectrum of the noisy input signal \cite{ADSP2009}.

As mentioned in Section \ref{TF}, the local stationarity assumption means the processing should be done on small-enough chunks of the input. Therefore, the input must first be split into overlapping frames using overlap-add processing. In the final step after processing, these frames are reassembled to form the continuous output signal.

To avoid signal distortion introduced by data segmentation \cite{Aydin2000}, each frame is first multiplied by a windowing function before performing the Fourier Transform (typically using the Fast Fourier Transform or FFT). The output signal is then formed by the sum of these overlapping windowed frames. After processing, when the signal is being reassembled, the window is applied again.

For the signal to remain undistorted, multiplying by these windows should not change its magnitude. To achieve this, particular overlap factor/window pairs must be used; for example, if a Hamming window is chosen, applying the window twice requires that the overlapped windows approximately sum to unity for an overlap factor of 4 i.e. each windowed frame overlaps each of its neighbours by 50\%, ensuring the output signal remains undistorted.

Spectral subtraction is popular largely because it is simple and easy to implement, requiring mainly the forward and inverse Fourier Transforms. However, this comes at a cost to performance. Subtracting the noise spectrum from the noisy input spectrum introduces distortion in the signal, known as musical noise \cite{Loizou2007}, as mentioned in Section \ref{TF}. Variations have been developed in attempts to mitigate this. A common variation involves over-subtraction and a noise floor. This method involves an over-subtraction factor, whereby an overestimate of the noise power spectrum is subtracted from that of the input, and using a noise spectral floor, which prevents the processed spectrum from going below a preset minimum value, to control both the amount of residual noise and musical noise \cite{Berouti1979}. However, it is generally evaluated that these modifications improve speech quality further but do not significantly affect the intelligibility of the input signals \cite{Loizou2007}.

% \section{MMSE Speech Enhancement}

\section{Ideal Binary Mask}\label{IBM}

Sound is generated by acoustic sources, and these sources are typically complex, containing multiple frequency components. In a typical environment, multiple acoustic sources are simultaneously active, including undesired background noise, and a listener's ear will pick up only the sum of all these sources. There are various types of corrupting background noise, including but not limited to acoustic noise (e.g. vehicle vibration), speech-shaped noise, industrial noise and multi-talker babble (e.g. noisy cafeteria with other speakers) \cite{Kozou2005}. For the listener to distinguish between the different sounds in the incoming mix, such as picking out a particular speaker in a busy supermarket, the incoming audio signal has to be partitioned and categorised accurately into individual sounds.
 
Human beings have auditory systems that are remarkably capable at doing this, and are thus able to understand speech in many of these noisy conditions. The signal separation process, known as auditory scene analysis, is typically performed in two stages, to understand the message spoken by the target speaker. Firstly, the input sound is decomposed into a matrix of time-frequency (T-F) units, where each unit represents the signal occurring at a particular instance in time with a particular frequency component. These T-F units are then analysed, and the auditory system utilises a combination of cues, learned patterns and other prior knowledge about the target to pick out the T-F units of the target signal, and group these individual units into a single recognisable ``image'' of the desired signal \cite{Bregman1990}. Essentially, the auditory system employs an analysis-synthesis strategy to organise the input into separate streams corresponding to different audio sources.

To model the human auditory system, computational auditory scene analysis (CASA) was proposed to approach sound separation in two stages: segregation and grouping \cite{WangBrown2006}. The aim of using these CASA techniques was to pick out the target signal from the noisy mix, and the computational method of choice was the ideal T-F binary mask \cite{Wang2005}.

The ideal binary mask (IBM) is defined in the T-F domain as a matrix of binary numbers, and is constructed by comparing the local signal-to-noise ratio (SNR), defined as the difference between the target signal energy and the noise energy, in each T-F unit against a threshold known as a local criterion (LC). In the IBM, the T-F units with local SNR exceeding the LC (in decibels) are assigned $1$, and $0$ otherwise. If a $0$ dB SNR threshold is used to generate the mask, a T-F unit being assigned $1$ indicates that the energy of the target signal is stronger than that of the interference (masker) within that particular T-F unit, which is a particularly intuitive implementation. Let $T(t,f)$ and $M(t,f)$ denote the target and masker signal power measured in dB respectively, at time $t$ and frequency $f$; the IBM is then defined as

\begin{equation}\label{eq:IBM}
IBM(t,f) = 
\begin{cases}
    1 & \text{if $T(t,f)-M(t,f)>LC$} \\
    0 & \text{otherwise}
\end{cases}
\end{equation}

This mask can then be applied to the T-F representation of the incoming noisy signal; it acts as a selective filter, allowing some parts of the signal to pass through (those T-F units assigned to $1$) while eliminating other parts (those assigned to $0$). This means that at each T-F unit, the IBM either retains target energy or discards interference energy. The IBM therefore offers an indication of the T-F areas of audible target speech, and offers significant improvements in intelligibility \cite{Brungart2006}. 

\begin{figure}[ht!]
\centering
\includegraphics[width=0.75\linewidth]{IBM_example1}
\captionof{figure}{Top to bottom: clean speech, noisy speech, IBM and IBM-processed speech}
\label{fig:IBM_eg1}
\end{figure}

An example of the IBM at work is shown in Figure \ref{fig:IBM_eg1}, with a clean sentence obtained from the TIMIT database \cite{TIMIT}. From top to bottom, the spectrograms shown are that of: a) clean speech; b) clean speech corrupted with white Gaussian noise at $5$ dB SNR; c) IBM constructed using LC threshold of $0$ dB, where black pixels denote $1$ (target stronger than interference masker) and white pixels denote $0$ (target weaker than masker); d) segregated mixture obtained with the $0$ dB LC IBM, obtained by multiplying the spectrograms in (b) and (c), one T-F unit at a time.

The $0$ dB LC IBM, a particularly simple and intuitive comparison, is theoretically optimal in terms of SNR gain (\cite{HuWang2004}, \cite{Ellis2006}); Figure \ref{fig:IBM_eg1} shows its good performance, whereby the spectrogram of the processed speech is nearly identical to that of clean speech. It was later shown that while it is not optimal due to certain constraints, it performs almost as well as the proposed alternative, and is in fact more practical for real-world implementation \cite{LiWang2009}. Multiple studies have examined further the effects of the LC, input SNR level and masker type on the performance of the IBM. For example, a technique called ideal T-F segregation (ITFS) has been effective in making use of the IBM to improve the intelligibility of human speech masked by competing voices \cite{Brungart2006}. It is argued that the ITFS removes informational masking caused by the IBM-eliminated T-F units with large masker energy, where informational masking refers to the inability to accurately distinguish the target signal from the noisy mixture.

To demonstrate the benefits of IBM processing, various studies carried out intelligibility tests, in which listeners listen to a set of IBM-processed sentences and write down the words they hear; results produced are in terms of the percentage of words identified correctly.

A typical IBM intelligibility test result is shown in Figure \ref{fig:IBM_int}, where UN represents the unprocessed noisy speech (replicated from \cite{LiLoizou2008}). Here, STFT was used to process the input noisy signal, using multitalker babble as the masker. As shown, the performance peaks out between roughly $-20$ dB and $5$ dB for an input SNR of $-5$ dB, with a slightly smaller range for an input SNR of $-10$ dB.

\begin{figure}[ht!]
\centering
\includegraphics[width=0.64\linewidth]{IBM_intelligibilityTest}
\captionof{figure}{Performance (percentage of words identified accurately) as a function of LC (dB) for two input SNR levels, masked in multitalker babble (replicated from \cite{LiLoizou2008})}
\label{fig:IBM_int}
\end{figure}

Large intelligibility gains were demonstrated in \cite{LiLoizou2008}, but the range of LC values for near-perfect intelligibility (performance plateaus of $\approx100$\% accuracy) were different to that in \cite{Brungart2006}. Attributing this to differences in the setup and signals used, it was suggested that the pattern of the IBM was the critical factor for intelligibility, rather than the local SNR of individual T-F units \cite{LiLoizou2008}.

The significant improvements to intelligibility made IBM a notable candidate for speech enhancement applications such as hearing aids, provided the IBM could be approximated to a high degree of accuracy. However, to apply it, it is important to understand how IBM enhances intelligibility. In \cite{Brungart2006}, it is argued that the IBM suppresses informational masking by directing the listener's attention to the T-F units containing target information i.e. \emph{where} the target signal is, in a T-F auditory space \cite{LiLoizou2008}. This led to the conclusion that listeners need not extract specific knowledge from individual T-F units, but rather the overall pattern of the IBM, i.e. pattern of target-dominated and masker-dominated T-F units, was the most important factor for intelligibility, which was also concluded in \cite{LiLoizou2008}. However, this interpretation is limited to the range of LCs where the IBM pattern represents the T-F units that are audible to normal human listeners i.e. LCs close to 0 dB \cite{KBP10}.

An alternative ideal mask definition was proposed in \cite{Anzalone2006}, which also produced large intelligibility improvements. This alternative mask was named the target binary mask (TBM), as the mask was calculated based on the target signal only. TBM depends on the long-term spectrum of the target speaker, and compares with an average spectrum i.e. a time-invariant threshold. The mask pattern naturally resembles the target signal and is unaffected by the masker specifically. Instead, the TBM generated in this manner can be applied to a mixture of the target signal and a different masker. On the other hand, the IBM pattern depends on the masking signal; IBM compares with the actual noise in the T-F units, which is time-dependent.

In certain applications, it may be easier to estimate the TBM than the IBM, and so it was of interest to investigate the intelligibility performance of the TBM: it was shown that the TBM has comparable performance to the IBM \cite{KBP09}. A noise-robust method based on target sound estimation to estimate the TBM was proposed in \cite{KLPB10}.

\subsection{Musical Noise}

The largest calculated SNR gain is achieved by using a fully-binary mask of $1$s and $0$s. However, doing so generally degrades the quality of the enhanced speech due to the introduction of musical noise, which refers to random, short tone-like bursts that, in some situations, can be more bothersome than the original noise. In \cite{StokesIBM}, the mask values were instead $1$ and $0.1$: if the mask indicates that speech is present, the gain is $1$, otherwise the gain is set to $0.1$ instead of cutting completely. By reducing the severity of the mask switching, the amount of musical noise artifacts introduced by the mask was reduced overall.

\subsection{Practical Considerations}

By definition, the IBM depends on oracle knowledge, as the mask is constructed based on the target and interfering signals before mixing. In a real-world situation, the target signal is of course unavailable, meaning the IBM has to be estimated from noisy data only. In the presence of significant noise, this can be a difficult task, and it is impossible to compute the IBM for all T-F units with complete accuracy. The effect of overall binary mask estimation error was investigated further in \cite{LiLoizou2008}, and it was demonstrated that the estimation needs to be very accurate overall. As an example, $>90\%$ accuracy is required to estimate the IBM for the case of $-5$ dB input masked with multitalker babble to yield significant gains in intelligibility.

While it is of interest to further investigate the effects of estimation uncertainty and error on speech intelligibility improvements, this project focuses on the Kalman filter algorithm, and assumes that an ideal or estimated binary mask has already been computed and is available.

\section{Linear Prediction Analysis}\label{LPC}

In linear prediction, future values of a signal are estimated as a linear combination of previous samples. In speech enhancement, the temporal variation of a speech signal is modelled using a linear predictive model. Equation \ref{eq:general_speechLinearPredictor} shows a $p$-th order all-pole linear predictor (autoregressive model), where the current state $\hat{s}(n)$ is estimated as a linear combination of $p$ previous states \cite{LPCanalysis}.

\begin{equation}\label{eq:general_speechLinearPredictor}
\hat{s}(n)=-\sum_{k=1}^{p}a_k s(n-k)
\end{equation}

where $s(n)$ is a speech sequence that is windowed to ensure the quasi-stationarity of the speech. In speech enhancement, obtaining an accurate model of the speech is useful for noise attenuation. This entails obtaining accurate estimates of the coefficients $a_k$ in Equation \ref{eq:general_speechLinearPredictor}, known collectively as the linear prediction coefficients (LPCs). This method is known as linear prediction analysis, where the goal is to obtain estimates of $a_k$ that minimise the mean squared error \cite{LPCanalysis}.

The prediction error (prediction residual) is computed as the difference between the actual sample $s(n)$ and the predicted sample $\hat{s}(n)$:

\begin{equation}\label{eq:LPC_predictionerror}
e(n)=s(n)-\hat{s}(n)=s(n)+\sum_{k=1}^{p}a_k s(n-k)
\end{equation}

A popular method to minimise the mean squared error is the least squares autocorrelation method, achieved by minimising the total prediction error $E$ over all samples:

\begin{equation}\label{eq:LPC_totalpredictionerror}
E=\sum_{n}e^2(n)=\sum_{n}[s(n)+\sum_{k=1}^{p}a_k s(n-k)]^2
\end{equation}

The coefficients $a_k$ that minimise Equation \ref{eq:LPC_totalpredictionerror} can be found by taking the derivative of $E$ with respect to $a_k$ and setting it to $0$, thus solving:

\begin{equation}\label{eq:derivativeE}
\frac{\partial E}{\partial a_i}=0, \quad i=1,2,...,p
\end{equation}

From Equations \ref{eq:LPC_totalpredictionerror} and \ref{eq:derivativeE}, we have

\begin{equation}\label{eq:differentialExpression}
\sum_{n}s(n-i)s(n)=\sum_{k=1}^{p}a_k \sum_{n}s(n-i)s(n-k), \quad i=1,2,...,p
\end{equation}

which produces a set of $p$ linear equations and $p$ unknowns which can be solved for the LPC coefficients $\{a_k, k=1,2,...,p\}$ that minimise $E$ in Equation \ref{eq:LPC_totalpredictionerror}.

In the autocorrelation method, the error is assumed to be minimised over the infinite duration $-\infty<n<\infty$ \cite{LPCanalysis}. In practice, there is typically a finite interval of interest: $0<n<N-1$. Equation \ref{eq:differentialExpression} can then be written as:

\begin{equation}\label{eq:diffEqnAutoCorr}
R(i)=\sum_{k=1}^{p}a_kR(i-k), \quad i=1,2,...,p
\end{equation}

where

\begin{equation}\label{eq:autoCorr}
R(i)=\sum_{n=i}^{N-1}s(n-i)s(n)
\end{equation}

is the autocorrelation function of the signal $s(n)$. The coefficients $R(i-k)$ form the autocorrelation matrix $\textbf{R}$, which is a symmetric Toeplitz matrix \cite{LPCanalysis} since the autocorrelation function is even i.e. $R(i)=R(-i)$. In matrix notation, Equation \ref{eq:diffEqnAutoCorr} can be written as:

\begin{equation}\label{eq:autoCorrMatrix}
\textbf{R}\textbf{A}=-\textbf{r}
\end{equation}

where \textbf{A} contains the desired LPC coefficients $\{a_k, k=1,2,...,p\}$. These coefficients can then obtained by solving the matrix inverse equation:

\begin{equation}\label{eq:matrixgetLPCs}
\textbf{A}=-\textbf{R}^{-1}\textbf{r}
\end{equation}

Finally, the prediction residual (Equation \ref{eq:LPC_predictionerror}) can be obtained by inverse filtering the noisy speech signal using the computed LPCs \cite{LPCresidual}. 

LPC analysis is based on the source-filter model of speech production, which models speech as a combination of a source of sound (vocal cords) and a linear filter (vocal tract)  \cite{VowelsBook}. This assumes that speech is produced by a buzzer at the end of a tube, which is a close approximation of real speech production \cite{PhoneticsBook}. The glottis, which produces the buzz, is the opening between the vocal folds; as the vocal folds vibrate, a ``buzzing'' sound is produced, which is what we term ``pronunciation'' \cite{PhonologyBook}. This buzz is characterised by its intensity (loudness) and frequency (pitch). The vocal tract, comprising the throat and mouth, forms the tube, which is characterised by its resonances, which produce formants (enhanced frequency harmonics) in the speech. The vocal tract transfer function can be modelled by an all-pole filter, which we use to estimate the current state of speech as a linear function of previous states.

In this project, LPC analysis is performed on modulation frames in the modulation domain, using the autocorrelation method described in \cite{LPCanalysis}. The prediction residual is used to calculate the excitation variance in the algorithm described in Section \ref{MDKF}, which is the basis of the algorithms discussed in this report.

\section{Kalman Filter}\label{KF}

The Kalman filter \cite{Kalman1960} is a recursive optimal data processing algorithm. Under certain assumptions, it is optimal with respect to any practical measure. This is because the Kalman filter (KF) makes use of all data available to it, processing all available information to estimate the current value of the desired variables. In the context of speech enhancement, speech signals are modelled as autoregressive processes using the state space method, where the processed speech is recursively estimated, one sample at a time \cite{WuChen1998}.

The filter has a recursive ``predictor-corrector'' structure \cite{Maybeck1979}; firstly, a prediction of the desired variable at the next measurement time is made, based on all previously available data, producing a prediction value and its associated uncertainty. When the next measurement is actually taken, the difference between the measurement and the predicted value is used to ``correct'' the prediction, to produce the new estimate. Note that this recorded measurement comes with its associated uncertainty, arising from imperfections of measuring instruments. The new estimate is thus updated using a linear combination of the prediction and the measurement, with more weight given to estimates with lower uncertainty.

The KF was initially proposed for speech enhancement by Paliwal and Basu in 1987 \cite{PaliwalBasu1987}, where excellent noise reduction was achieved when linear prediction coefficients (LPCs) were estimated from clean speech. The KF is of particular interest for speech enhancement, as the speech model is inbuilt into the KF recursion equations, and the enhanced speech contains little to no musical noise, assuming clean LPCs are available \cite{MBG2006}; the performance of the KF is highly dependent on the accurate estimation of LPCs. However, for practical use, these parameters have to be estimated from noisy speech since the clean speech is not known \textit{a priori}, causing a significant drop in performance. Better performance has been demonstrated in variations of the original KF algorithm, such as a cascaded estimator/encoder structure which improves LPC estimates \cite{Gibson1991}.

In recent years, the focus has shifted away from the traditional KF methods which utilise the acoustic domain, defined as the short-time Fourier Transform (STFT) of the signal. Instead, there has been growing interest in the modulation domain, defined as the variation over time of the magnitude spectrum at all acoustic frequencies \cite{Atlas2003}. Studies have increasingly shown the importance of the modulation domain for speech analysis; for example, very low frequency modulations of sound have been shown to be the fundamental carriers of information in speech \cite{Atlas2003}, due to physiological limitations on how rapidly the vocal tract is able to change with time \cite{PWS2010}. The slowly-varying modulation domain hence represents how the vocal tract changes over time \cite{SP11}.

The KF is capable of handling non-stationary signals as well as estimating both magnitude and phase spectra \cite{Li2006Phd}, which puts it at an advantage over STFT-based, acoustic domain-based methods for speech processing, as phase information has been shown to be more important in the modulation domain than in the acoustic domain \cite{GreenBergArai2001}. It was also noted in \cite{SP11} that the low order linear predictor KF was more appropriate for enhancing slower-varying modulating signals than for enhancing time-domain speech, as the time-domain signals contain long-term correlation which the low order linear predictor cannot capture. This is important for the KF, as its optimality works on the basis of incorporating and using all data available to the algorithm. These results suggest the use of the KF in the modulation domain as an improved method of speech enhancement \cite{SP11}.

\subsection{Modulation-Domain Kalman Filter}\label{MDKF}

The modulation-domain KF (MDKF) is an adaptive minimum mean-squared error (MMSE) estimator that uses the statistics of time-varying changes in the magnitude spectrum of both speech and noise \cite{SP11}. In the MDKF, an analysis-modification-synthesis (AMS) framework is used to obtain the modulation domain in three steps. In the analysis stage, the input speech signal is processed using STFT; next, the noisy input spectrum undergoes some modification or processing; and lastly, the output processed signal is synthesised by inverse STFT followed by the overlap-add method. 

\subsubsection{Analysis-modification-synthesis framework in the acoustic domain}

Considering an additive noise model, where $y(n)$, $x(n)$ and $v(n)$ represent zero-mean signals of noisy speech, clean speech and noise respectively:

\begin{equation}\label{eq:additive_noise_time}
y(n)=x(n)+v(n)
\end{equation}

Assuming speech is quasi-stationary means that it can be analysed in frames using the STFT (analysis), thus obtaining the STFT of the noisy signal $y(n)$:

\begin{equation}\label{eq:noisy_STFT_conv}
Y(n,k)=\sum_{l=-\infty}^{\infty}y(l)w(n-l)e^{-j\frac{2\pi kl}{N}}
\end{equation}

which can be represented using STFT analysis as Equation \ref{eq:additive_noise_STFT}:

\begin{equation}\label{eq:additive_noise_STFT}
Y(n,k)=X(n,k)+V(n,k)
\end{equation}

where $Y(n,k)$, $X(n,k)$ and $V(n,k)$ denote the STFTs of noisy speech, clean speech and noise respectively and $k$ refers to the discrete acoustic frequency index, $N$ is the acoustic frame duration in number of samples and $w(n)$ is a window analysis function. For speech enhancement, a Hamming window is typically used. Note that this model is noise-additive in the complex STFT domain.

Each one of $Y(n,k)$, $X(n,k)$ and $V(n,k)$ is a complex spectrum, and can be expressed in terms of their acoustic magnitude and acoustic phase spectra. For example, $Y(n,k)$ can be represented as:

\begin{equation}\label{eq:noisy_STFT_complex}
Y(n,k)=|Y(n,k)|e^{j\angle Y(n,k)}
\end{equation}

where $|Y(n,k)|$ is the acoustic magnitude spectrum and $\angle Y(n,k)$ is the acoustic phase spectrum.

Traditionally, AMS-based methods only modify the noisy acoustic magnitude spectrum $|Y(n,k)|$ to obtain a processed magnitude spectrum $|\hat{X}(n,k)|$; the modified spectrum is thus obtained by combining the enhanced magnitude spectrum with the original noisy phase spectrum $\angle Y(n,k)$:

\begin{equation}\label{eq:AMS_STFT}
\hat{X}(n,k)=|\hat{X}(n,k)|e^{j\angle Y(n,k)}
\end{equation}

The enhanced speech $\hat{x}(n)$ is then reconstructed by performing the inverse STFT of the enhanced acoustic spectrum $\hat{X}(n,k)$ followed by synthesis windowing and overlap-add \cite{Quatieri2002}.

\subsubsection{Kalman filter model in the modulation domain}

As briefly introduced in Section \ref{ModulationDomain}, in the modulation domain, the acoustic magnitude spectrum of noisy speech is interpreted as a series of modulating signals spanning across time, where each modulating signal $|Y(n,k)|$ represents the variation of one frequency component over time, with $k=1,2,...,N$ where $N$ is the number of frequency bins. Each modulating signal is individually processed with a separate KF \cite{SP11}.

To visualise this, imagine that a time-domain noisy speech signal sampled at $8$ kHz is windowed with a $64$ ms frame (window) length and $4$ ms frame shift. Taking the STFT, each window is analysed individually: the samples within a $64$ ms window are viewed as a frequency-domain signal with (for example) $256$ frequency bins. When the next window is taken (original window shifted by $4$ ms), the samples are again analysed into a set of $256$ frequency bins. Doing this for the entire signal produces $256$ time-varying signals (modulating signals), one for each frequency component and processed with its own KF, where the samples in each signal are $4$ ms apart. Within each KF, the modulating signal is further windowed, but the signal now has a much lower frequency: in this case, $\frac{1}{0.004}=250$ Hz. Assuming a modulating window of $64$ ms, each window only contains $\frac{64}{4}=16$ samples, compared to $512$ samples for a $64$ ms window of a $8$ kHz time-domain signal.

Returning to the model, an additive noise model is assumed for each modulating signal, assuming white Gaussian noise (Equation \ref{eq:MDKF_additive_noise}). Recall that the noisy phase spectrum is left untouched.

\begin{equation}\label{eq:MDKF_additive_noise}
|Y(n,k)|=|X(n,k)|+|V(n,k)|
\end{equation}

In the KF autoregressive model, a $p$-order linear predictor is used to model the evolution of speech over time (Equation \ref{eq:KF_linear_predictor_speech}), where ${a_{j,k}; j=1,2,...,p}$ are the LPCs and $W(n,k)$ is a random white excitation with a variance of $\sigma^2_{W(k)}$.

\begin{equation}\label{eq:KF_linear_predictor_speech}
|X(n,k)|=-\sum_{j=1}^{p}a_{j,k}|X(n-j,k)|+W(n,k)
\end{equation}

Including the noise signal, the overall state space representation for noisy speech can be written as:

\begin{equation}\label{eq:KF_X_statespace}
\textbf{X}(n,k)=\textbf{A}(k)\textbf{X}(n-1,k)+\textbf{d}W(n,k)
\end{equation}

\begin{equation}\label{eq:KF_Y_statespace}
|Y(n,k)|=\textbf{d}^{T}\textbf{X}(n,k)+|V(n,k)|
\end{equation}

where $\textbf{X}(n,k)=[|X(n,k)|,|X(n-1,k)|,...|X(n-p+1,k)|]^T$ is the clean speech modulation state vector, $\textbf{d}=[1,0,...,0]^T$ is the measurement vector for both the excitation noise $W(n,k)$ and observation, and $\textbf{A}(k)$ is the state transition matrix utilising the LPCs:

\begin{equation}\label{eq:KF_statetransition}
\textbf{A}(k)=
\begin{bmatrix}
    -a_{1,k} & -a_{2,k} & \dots  & -a_{p-1,k} & -a_{p,k} \\
    1        & 0        & \dots  & 0          & 0        \\
    0        & 1        & \dots  & 0          & 0        \\
    \vdots   & \vdots   & \ddots & \vdots     & \vdots   \\
    0        & 0        & \dots  & 1          & 0        \\
\end{bmatrix}
\end{equation}

The Kalman filter recursively calculates a linear unbiased MMSE estimate $\hat{\textbf{X}}(n|n,k)$ of the $k$-th modulation state vector at time $n$, given the noisy modulating signal up to and including time $n$ (i.e. $|Y(1,k)|,|Y(2,k)|,...|Y(n,k)|$) using the following equations:

\begin{equation}\label{eq:MDKF_predictP}
\textbf{P}(n|n-1,k)=\textbf{A}(k)\textbf{P}(n-1|n-1,k)\textbf{A}(k)^T+\sigma^2_{W(k)}\textbf{d}\textbf{d}^T
\end{equation}

\begin{equation}\label{eq:MDKF_predictXhat}
\hat{\textbf{X}}(n|n-1,k)=\textbf{A}(k)\hat{\textbf{X}}(n-1|n-1,k)
\end{equation}

\begin{equation}\label{eq:MDKF_updateK}
\textbf{K}(n,k)=\textbf{P}(n|n-1,k)\textbf{d}[\sigma^2_{V(k)}+\textbf{d}^T\textbf{P}(n|n-1,k)\textbf{d}]^{-1}
\end{equation}

\begin{equation}\label{eq:MDKF_updateP}
\textbf{P}(n|n,k)=[\textbf{I}-\textbf{K}(n,k)\textbf{d}^T]\textbf{P}(n|n-1,k)
\end{equation}

\begin{equation}\label{eq:MDKF_updateXhat}
\hat{\textbf{X}}(n|n,k)=\hat{\textbf{X}}(n|n-1,k)+\textbf{K}(n,k)[|Y(n,k)|-\textbf{d}^T\hat{\textbf{X}}(n|n-1,k)]
\end{equation}

where $\sigma^2_{V(k)}$ is the variance of the corrupting noise and $\textbf{P}(n|n,k)$ is the error covariance matrix. These equations can be categorised into two main steps: prediction and updating. Equations \ref{eq:MDKF_predictP} and \ref{eq:MDKF_predictXhat} predict the error covariance and state based on past samples respectively, while the other equations update the Kalman gain, error covariance and state based on the predicted values.

In particular, Equation \ref{eq:MDKF_updateXhat} is the main updating step, whereby a linear combination of the estimate based on previous samples $|\hat{X}(n|n-1,k)|$ and the current measurement $|Y(n,k)|$ is used to compute the current estimate $|\hat{X}(n|n,k)|$. To view this more clearly, we can rewrite Equation \ref{eq:MDKF_updateXhat} as:

\begin{equation}\label{eq:MDKF_updateXhat_linearCombi}
\hat{\textbf{X}}(n|n,k)=[\textbf{I}-\textbf{K}(n,k)\textbf{d}^T]\hat{\textbf{X}}(n|n-1,k)+\textbf{K}(n,k)|Y(n,k)|
\end{equation}

The accuracy of the weighted sum producing the updated state is critical in determining the correctness of the algorithm. Hence, it is imperative that the noise power is estimated as accurately as possible. There are a number of ways to do so, as discussed in Section \ref{noiseEstimation}.

As the algorithm is running, each modulating signal $|Y(n,k)|$ is windowed into modulation frames, and the LPCs and excitation variance $\sigma^2_{W(k)}$ are estimated. Within each frame, the LPCs are kept constant, whereas the Kalman gain $\textbf{K}(n,k)$, error covariance matrix $\textbf{P}(n|n,k)$ and estimated state vector $\hat{\textbf{X}}(n|n,k)$ are updated every sample, regardless of frame.

\subsection{Comparison with Time-Domain Kalman Filter}

For comparison purposes, the time-domain Kalman filter (TDKF) equations are shown below:

\begin{equation}\label{eq:TDKF_predictP}
\textbf{P}(n|n-1)=\textbf{A}\textbf{P}(n-1|n-1)\textbf{A}^T+\sigma^2_{w}\textbf{d}\textbf{d}^T
\end{equation}

\begin{equation}\label{eq:TDKF_predictXhat}
\hat{\textbf{x}}(n|n-1)=\textbf{A}\hat{\textbf{x}}(n-1|n-1)
\end{equation}

\begin{equation}\label{eq:TDKF_updateK}
\textbf{K}(n)=\textbf{P}(n|n-1)\textbf{d}[\sigma^2_{v}+\textbf{d}^T\textbf{P}(n|n-1)\textbf{d}]^{-1}
\end{equation}

\begin{equation}\label{eq:TDKF_updateP}
\textbf{P}(n|n)=[\textbf{I}-\textbf{K}(n)\textbf{d}^T]\textbf{P}(n|n-1)
\end{equation}

\begin{equation}\label{eq:TDKF_updateXhat}
\hat{\textbf{x}}(n|n)=\hat{\textbf{x}}(n|n-1)+\textbf{K}(n)[y(n)-\textbf{d}^T\hat{\textbf{x}}(n|n-1)]
\end{equation}

where $\hat{\textbf{x}}(n|n-1)$ and $\hat{\textbf{x}}(n|n)$ are the \textit{a priori} (predicted) and \textit{a posteriori} (updated) state vectors respectively, $\textbf{P}(n|n-1)$ and $\textbf{P}(n|n)$ are the \textit{a priori} and \textit{a posteriori} error covariance matrices respectively, $\textbf{K}(n)$ is the Kalman gain, and $\sigma^2_{v}$, $\sigma^2_{w}$ are the noise and excitation variances respectively.

Figure \ref{fig:compareTDKF_MDKF} compares the spectrograms of TDKF (order $18$) and MDKF (order $2$) applied on speech from the TIMIT database \cite{TIMIT}, corrupted by white Gaussian noise at $5$ dB SNR, and sampled at $16$ kHz. For the purposes of comparing performance limits, clean speech LPCs were used in the filters. Generally, both algorithms perform well in removing noise, especially when speech is absent. However, there is visibly some noise in the TDKF-enhanced speech; particularly, frequency components above $1.8$ kHz have been noticeably degraded by noise. A listening test confirmed this, detecting the presence of high-frequency artifacts.

\begin{figure}[ht!]
\centering
\includegraphics[width=1\linewidth]{compareTDKF_MDKF}
\captionof{figure}{Top row: clean speech (left), speech corrupted with white Gaussian noise (right); bottom row: TDKF-enhanced speech (left), MDKF-enhanced speech (right)}
\label{fig:compareTDKF_MDKF}
\end{figure}

This TDKF noise is a limitation of using the KF for speech enhancement. Similarly to how we rearranged Equation \ref{eq:MDKF_updateXhat}, Equation \ref{eq:TDKF_updateXhat} can be rewritten to show that the enhanced output is a weighted combination of the estimated speech and measured speech, where the relative weight depends on the Kalman gain $\textbf{K}(n)$. When speech is absent, $\textbf{P}(n|n-1)=\textbf{0}$, meaning that $\textbf{K}(n)=\textbf{0}$ and the estimated state is equal to the predicted state, being unaffected by the noisy measurement.

When speech is present, however, the algorithm does not work quite so perfectly. Using a low model order means that the TDKF linear predictor cannot fully replicate the harmonic structure of speech, which requires autocorrelation lags in the order of the number of samples in a pitch period \cite{SP11}. The prediction thus has unvoiced and noise-like characteristics, and the result is that the updated output only preserves the speech component below $1.8$ kHz \cite{SP11}. The resultant noise will be especially prevalent in regions of low SNR, where the prediction is weighted more heavily due to Equation \ref{eq:TDKF_updateK} producing a smaller $\textbf{K}(n)$.

MDKF has an inherent advantage over the TDKF due to the linear predictor model used. Compared to the TDKF, the MDKF models the temporal variation of the acoustic magnitude spectrum of speech, which represents the changes in the vocal tract over time. This is more comprehensive since the low-order MDKF linear predictors are able to model the modulating signal dynamics, due to physiological limitations of how quickly the vocal tract can change \cite{PWS2010}. This also better represents speech information overall, as it has been shown that low-frequency modulations of sound are the fundamental carriers of speech information \cite{Atlas2003}.

\subsection{Performance of MDKF}

Overall, experimental results from the TIMIT corpus (Figure \ref{fig:compareTDKF_MDKF}) show that under ideal conditions where clean speech LPCs can be obtained accurately, the linear predictor is sufficient to model the modulating signals of clean speech. As described earlier, the vocal tract tends to change slowly due to physiological constraints, and thus low LPC orders ($p=2$) were found to be sufficient. Using this, the MDKF was by far the best performing algorithm, doing better than all acoustic and time-domain methods tested, including the TDKF, for both white and coloured noise \cite{SP11}. This was despite both algorithms having access to clean speech LPCs.

However, clean speech is not available in reality; the presence of noise generally degrades the LPC estimates, worsening the performance of the MDKF algorithm. In \cite{SP11}, a practical MDKF algorithm was evaluated, which used an acoustic-domain pre-processor for LPC estimation to reduce the effect of noise degradation.

\section{Speech Quality}

The perceived overall speech quality is how ``good'' the quality of the speech is. The definition of ``good'' is typically left to the listener, who then gives a score to the speech. Methods to assess speech quality can be grouped into subjective and objective measures.

\subsection{Subjective Speech Quality Measures}

Subjective quality measures typically compare the original and processed speech by a listener or a group of listeners. The listeners subjectively rate or rank the speech according to a predetermined scale. Since every listener is unique, their ratings will vary; this variation in results can be reduced by averaging the scores from a group of listeners.

\subsubsection{Mean Opinion Score}

A widely used subjective quality measure is the Mean Opinion Score (MOS) \cite{ITU830}. Each listener gives a numeric MOS score, typically in the range $1-5$, where $1$ is the lowest perceived quality and $5$ is the highest perceived quality. The ``Absolute Category Rating'' scale is commonly used, as shown in Table \ref{table:ACR_MOS} \cite{ITU800}. The overall score is obtained by averaging the ratings from all listeners, representing an overall perceived quality of the speech. With a large number of speech files, this test can be costly and time consuming.

\begin{table}[ht!]
\centering
\begin{tabular}{|c|c|}
\hline
\textbf{Rating} & \textbf{Label} \\ \hline
1               & Excellent      \\ \hline
2               & Good           \\ \hline
3               & Fair           \\ \hline
4               & Poor           \\ \hline
5               & Bad            \\ \hline
\end{tabular}
\caption{Categories of MOS: Absolute Category Rating}
\label{table:ACR_MOS}
\end{table}

\subsection{Objective Speech Quality Measures}

On the other hand, objective speech quality use physical measurements and some calculated values from these measurements. Typically, these calculations compare objective measurements for the reference clean speech and the distorted speech.

Many of the objective measures are highly correlated with subjective measures; it is thus common for a test to use objective measures to estimate subjective methods, which are usually more time-consuming and costly as they involve human listeners. However, as noted previously, there are situations in which high objective scores do not produce high subjective scores and vice versa.

\subsubsection{SNR}

Signal-to-Noise Ratio (SNR) is one of the oldest and most widely used objective quality methods. It has low computational complexity, but requires both clean and distorted speech. The classic formula is calculated (in dB) as:

\begin{equation}\label{eq:SNR}
SNR=10\log_{10}\frac{\sum\limits_{n=1}^{N}x^2(n)}{\sum\limits_{n=1}^{N}\{x(n)-\hat{x}(n)\}^2}
\end{equation}

where $x(n)$ is the clean speech, $\hat{x}(n)$ is the distorted speech and $N$ is the number of time-domain samples.

However, this formula does not represent actual speech quality well as it averages over the entire signal even though speech is non-stationary. Speech energy fluctuates over time, and this formula is dominated by parts where speech energy is large and noise energy is small, which is not representative of the entire signal.

Modifications have thus been proposed. To better represent the temporal variation of speech, segmental SNR (segSNR) was proposed to calculate SNR in short frames and take the average:

\begin{equation}\label{eq:segSNR}
SNR_{seg}=\frac{10}{M}\sum_{m=0}^{M-1}\log_{10}\frac{\sum\limits_{n=Lm}^{Lm+L-1}x^2(n)}{\sum\limits_{n=Lm}^{Lm+L-1}\{x(n)-\hat{x}(n)\}^2}
\end{equation}

where $L$ is the frame length in number of samples and $M$ is the number of frames in the signal ($N=ML$). The logarithm of the ratio is computed before averaging, which means that frames with unusually large ratios are weighted less while frames with lower ratios are weighted more. This matches the perceptual quality better, ensuring that frames with large speech and low noise do not unfairly dominate the overall ratio.

However, if the speech contains too much silence, the overall segSNR value decreases significantly as silent frames usually give large negative segSNR values. In this case, silent frames should be excluded from the averaging by using speech activity detectors. Similarly, frames which exhibit excessively large or small speech values should also be excluded. These modifications give segSNR values that match the subjective quality better. As a result, segSNR often has upper and lower bounds of $35$ dB and $-10$ dB respectively \cite{HP1998}.

A separate variation of SNR is the frequency-weighed SNR (fwSNRseg), which weights the contribution of the different frequency bands. The fwSNRseg can be defined as:

\begin{equation}\label{eq:fwSNRseg}
fwSNR_{seg}=\frac{10}{M}\sum_{m=0}^{M-1}\frac{\sum\limits_{j=0}^{K-1}W(j,m)\log_{10}\frac{X(j,m)^2}{\{X(j,m)-\hat{X}(j,m)\}^2}}{\sum\limits_{j=0}^{K-1}W(j,m)}
\end{equation}

where $W(j,m)$ is the weight of the $j^{th}$ frequency band in the $m^{th}$ frame, $K$ is the number of frequency bands and $X(j,m)$, $\hat{X}(j,m)$ are the spectral amplitude of the clean and distorted speech respectively. The weights can be chosen in many ways, one of which is the ANSI SII Standard \cite{ANSI_SII}.

\subsubsection{Perceptual Evaluation of Speech Quality}

One of the most popular objective speech quality measures is the ITU-T P.862: Perceptual Evaluation of Speech Quality (PESQ) \cite{PESQ}.

PESQ was developed to model subjective tests commonly used to assess the voice quality by human beings (e.g. MOS), using true voice samples as test signals. It is designed for use over a wide range of conditions. A mapping from PESQ to MOS scores was standardised, allowing PESQ results to model MOS scores that range from 1 (Bad) to 5 (Excellent) (typical of Table \ref{table:ACR_MOS}). The average correlation between PESQ-mapped MOS scores and subjective MOS for a number of tests was a high score of $0.935$ \cite{Evaluate_PESQ}. The block diagram of PESQ is shown in Figure \ref{fig:PESQ_blockdiagram} (taken from \cite{RBHH_PESQ}).

\begin{figure}[ht!]
\centering
\includegraphics[width=0.8\linewidth]{PESQ_model}
\captionof{figure}{Structure of PESQ model (taken from \cite{RBHH_PESQ})}
\label{fig:PESQ_blockdiagram}
\end{figure}

\section{Speech Intelligibility}

Speech intelligibility is the accuracy with which we can hear what is being said, and is a different performance measure as compared to perceived speech quality. Specifically, it is measured as the percentage of correctly identified words relative to the number of words. Instead of words, one may also use phonemes or syllables as the test unit. If words or complete sentences are used, they typically encompass linguistically meaningful units, and thus the choice of test words is important to ensure a fair assessment.

Although there does not exist a completely clear relationship between perceived speech quality and intelligibility, there exists some correlation between the two. Generally, ``good'' quality speech also gives high intelligibility and vice versa. However, this generalisation does not always hold; there are some samples that are highly intelligible yet are perceived as ``poor'' quality and vice versa.

\subsection{Short-Time Objective Intelligibility}

A widely-used method to evaluate intelligibility is the Short-Time Objective Intelligibility (STOI) measure \cite{STOI}. The basic structure is shown in Figure \ref{fig:STOI_model}.

\begin{figure}[ht!]
\centering
\includegraphics[width=0.92\linewidth]{STOI_model}
\captionof{figure}{Structure of STOI model (taken from \cite{STOI_orig})}
\label{fig:STOI_model}
\end{figure}

STOI requires both clean and noisy speech. Both of these are first time-frequency (T-F) decomposed to obtain a representation resembling the transform properties of the auditory system \cite{STOI_orig}.

STOI contains an intermediate-stage intelligibility measure, which compares the temporal envelopes of the clean and degraded speech in short-time segments using a correlation coefficient. Before this comparison, the temporal envelopes are first normalised and clipped. The normalisation process compensates for global level differences, which should not dominate the speech intelligibility, while clipping prevents over-sensitivity of the model towards excessively degraded T-F units. This ensures that if a T-F unit is already deemed unintelligible, further corruption of this unit does not lead to a lower intelligibility prediction.

These intermediate intelligibility measures are then averaged to a single value ranging from $0$ to $1$, where $1$ represents perfect intelligibility i.e. $100\%$ of words can be detected accurately. Results have demonstrated that this value is highly correlated with the true speech intelligibility of noisy speech from multiple listening experiments \cite{STOI_orig}.


\chapter{Problem Analysis}

In this chapter, the project aims are analysed and broken down into specific deliverables. Following that is a discussion on the implementation of the baseline algorithm and how the proposed modifications will be made. Subsequent chapters discuss these modifications and the performance evaluation in greater detail.

\section{Deliverables}

The goal is to modify a Modulation-Domain Kalman Filter (KF) speech enhancement method by integrating data from an ideal binary mask (IBM). This project proposes a few methods to do so.

In the first method, IBM statistics are used to scale the predicted value and variance in the MDKF. This requires an implementation of the MDKF, and also involves obtaining the IBM statistical information. This can be broken down into a few deliverables, as follows:

1) Implement a working IBM (algorithm from \cite{LiLoizou2008})

2) Obtain statistical information from IBM from training data

3) Implement a working MDKF (algorithm from \cite{SP11})

4) Apply IBM information in a useful way to improve MDKF iterations

A second proposed method is to improve the linear prediction coefficients (LPCs) using the IBM information. In the MDKF, LPCs are estimated in each modulation frame. The goal is to study if these LPCs can be improved by incorporating data provided by an IBM:

1) Apply weighted sum to LPC estimation using weights determined from IBM

The final proposed method is to improve the noise estimation of the MDKF with the IBM, using a weighted sum to tweak parts of the noise estimation algorithm, including the Signal-to-Noise Ratio (SNR) and speech presence probability (SPP):

1) Apply weighted sum to SNR and SPP in noise estimation using weights determined from IBM

After implementing these methods, their performance will be evaluated using the Signal-to-Noise Ratio (SNR), Perceptual Evaluation of Speech Quality (PESQ) and Short-Time Objective Intelligibility (STOI) measures:

1) For a range of input noise SNR levels, run all algorithms

2) Evaluate using measures SNR, PESQ, STOI

This set of requirements and tests are necessary to evaluate if the proposed algorithms are able to provide improvements over existing methods. For evaluation, the testing methodology is standardised to ensure a fair assessment, and will be discussed later. All methods are also evaluated and averaged over multiple input speech samples to reduce the effect of outliers. 

\section{Implementation}\label{implementation}

In this project, the MDKF in Section \ref{MDKF} is used as a baseline algorithm. For the base MDKF, an acoustic frame length of $16$ ms was used with a $4$ ms frame shift. For each frequency bin, a modulation frame of $24$ ms was used with a $4$ ms frame increment to determine the LPC coefficients. An MDKF model order of $p=2$ was used. In most experiments, these settings are used for all algorithms to compare and evaluate performance.

\subsection{Optimal Modulation Frame Length}

In \cite{PSW2011}, it was shown that short modulation frame durations of $10-32$ ms retain good intelligibility overall as compared to longer frame lengths. This is briefly verified below.

Using clean speech as the ``noisy'' input speech to the MDKF filter, a plot of the average LPC excitation variance per modulation frame against a range of modulation frame lengths is shown in Figure \ref{fig:compareModFrameLengths}, where the error is normalised against the length of the modulation frame and averaged over 38 randomly chosen speech samples from the TIMIT dataset.

\begin{figure}[ht!]
\centering
\includegraphics[width=0.58\linewidth]{compareModFrameLengths}
\captionof{figure}{Plot of normalised excitation variance vs. modulation frame lengths}
\label{fig:compareModFrameLengths}
\end{figure}

Figure \ref{fig:compareModFrameLengths} clearly shows that the error decreases as the modulation frame length increases. This makes sense intuitively as the modulation domain represents the variation in speech, which changes much more slowly than the actual speech itself.

\begin{figure}[ht!]
\centering
\includegraphics[width=0.58\linewidth]{modframelen_pesq}
\captionof{figure}{Average PESQ scores vs. modulation frame lengths}
\label{fig:modframelen_pesq}
\end{figure}

\begin{figure}[ht!]
\centering
\includegraphics[width=0.58\linewidth]{modframelen_stoi}
\captionof{figure}{Average STOI scores vs. modulation frame lengths}
\label{fig:modframelen_stoi}
\end{figure}

However, the numerical error does not fully represent the human perception of speech. Indeed, Figures \ref{fig:modframelen_pesq} and \ref{fig:modframelen_stoi} show that the PESQ and STOI scores decrease as the modulation frame length increases. It was empirically found through informal listening tests that a $16-24$ ms modulation frame length produced acceptable levels of noise, while being more intelligible than speech enhanced using longer frame lengths, which agrees with the conclusions in \cite{PSW2011}. This project thus uses a modulation frame length of $24$ ms with a $4$ ms frame shift for an overall balance of quality and intelligibility. Given a $4$ ms acoustic frame shift, the modulation domain sampling frequency is $250$ Hz, which gives $6$ samples per modulation frame.

\section{Kalman Filter Framework}

The base framework for the proposed speech enhancement algorithms in this project are shown in Figure \ref{fig:blockdiagramMDKF}, which is based on a Modulation-Domain Kalman Filter (MDKF) as described in Section \ref{MDKF}. Modifications are then proposed to this baseline algorithm.

\begin{figure}[H]
\centering
\includegraphics[width=1\linewidth]{blockdiagramMDKF}
\captionof{figure}{Block diagram of baseline MDKF model}
\label{fig:blockdiagramMDKF}
\end{figure}

The noisy input time-domain speech signal $y(n)$ first undergoes the STFT transform. The amplitude spectrum $|Y(n,k)|$ is used as input to the MDKF, and is also used to estimate the noise power spectrum. For a theoretical investigation of best performance possible, the LPC coefficients are estimated clean speech $s(n)$; specifically, they are estimated from the spectral amplitudes of the clean speech in each frequency bin. After processing through the Kalman filter, the output enhanced magnitudes are combined with the noisy phase spectrum $\theta(n,k)$, then processed through an inverse STFT to produce the output processed speech.


\chapter{Testing Methodology}

In this project, the objective is to propose modifications to existing speech enhancement algorithms, with the goal of improving the two aspects of speech quality: the overall perceived speech quality, and the speech intelligibility \cite{Kazuhiro2012}.

\section{Assessing Speech Quality}

The perceived overall speech quality is how ``good'' the quality of the speech is. The assessment is left to the listener who scores the speech according to a pre-defined rating system.

Methods to assess speech quality can be grouped into subjective and objective measures. As discussed before, many of the objective measures are highly correlated with subjective measures; it is thus common for a test to use objective measures, which are less time-consuming and cheaper, to approximate subjective methods.

In this report, the quality of the enhancement algorithms will be assessed using segSNR and PESQ. The effect of the enhancements on noise level will be assessed by segmental SNR (segSNR) while speech quality will be evaluated by Perceptual Evaluation of Speech Quality (PESQ).

\section{Assessing Speech Intelligibility}

On the other hand, speech intelligibility is the accuracy with which we can hear and identify what is being said. Typically, it is measured as the percentage of correctly identified words relative to the total number of words. In this report, speech intelligibility of the proposed speech enhancers will be evaluated using Short-Time Objective Intelligibility (STOI).

\section{Speech Database: TIMIT}\label{dataset}

The TIMIT Acoustic-Phonetic Continuous Speech Corpus of read speech was designed to provide speech data for acoustic-phonetic studies as well as the development and evaluation of automatic speech recognition systems \cite{TIMIT}. It is widely used in the research and testing of speech enhancement algorithms. A combined effort between the Massachusetts Institute of Technology (MIT), SRI International (SRI) and Texas Instruments, Inc. (TI), the TIMIT database contains recordings of $630$ speakers of eight major dialects of American English, each reading ten phonetically rich sentences, each of which is a few seconds long. The recordings are $16$-bit resolution, $16$ kHz rate speech waveform files, and the database also includes time-aligned phonetic and word transcriptions.

Some of the proposed algorithms require training samples; a set of $38$ randomly chosen samples from the TIMIT training database was used for such training purposes. For general performance testing, a separate set of $38$ randomly chosen speech samples are used. These speech samples are corrupted by white noise at SNRs between $-20$ and $20$ dB.


\chapter{Modified Kalman Filter Inputs}\label{BMMDKF_chapter}

\section{Introduction}

From the modulation-domain Kalman filter (MDKF) iteration equations (Equations \ref{eq:MDKF_predictP} to \ref{eq:MDKF_updateXhat_linearCombi}), we know that two variables are used to form the updated state: the prediction of the current state and the observation of the current state. The observation is noisy; we can improve the accuracy of the updated state, and hence the algorithm performance, if the observation is modified to better represent the actual speech.

In this chapter, we propose to incorporate information from an ideal binary mask (IBM) to improve the accuracy of the updated state. This can be done in two ways: in the first method, the observation is tweaked directly, while in the second method, the updated state is instead constructed by combining information from the observation, the prediction and the mask altogether. Both methods are detailed and evaluated in this chapter.

\section{Incorporating Binary Mask into Observation}\label{IBMobs}

From Section \ref{IBM}, we know that the $0$ dB-threshold IBM produces a mask of $1$s and $0$s, where $1$s represent T-F units where the signal has higher energy than noise, and vice versa for $0$s, and that this threshold gives the best-performing mask overall. The observation used in the MDKF equations (Section \ref{MDKF}) is the original input noisy observation; if it is modified to better represent the inherent underlying speech (or silence in absence of speech), the algorithm can perform better. One way to do so is to incorporate the statistical quantities of an ideal binary mask, by multiplying together the probability density functions (PDFs) of the observation and the mask.

\subsection{Gaussian Product}\label{gaussianpdt}

Let $f(x)$ and $g(x)$ be two Gaussian PDFs with arbitrary means $a$ and $b$ and variances $A$ and $B$, which we represent as $f(x) \sim \mathcal{N}(x;a,A)$ and $g(x) \sim \mathcal{N}(x;b,B)$ respectively. Their product is also a product of two Gaussian PDFs \cite{Bro11}, of which one term is dependent on $x$ while the other is independent of $x$:

\begin{equation}\label{eq:prod2Gaussians}
\mathcal{N}(x;a,A)\ \mathcal{N}(x;b,B)=\mathcal{N}(a;b,A+B)\ \mathcal{N}(x;c,C)
\end{equation}

where $C$ and $c$ are defined as:

\begin{equation}\label{eq:prod2Gaussians_var}
C=(A^{-1}+B^{-1})^{-1}=\frac{AB}{A+B}
\end{equation}

\begin{equation}\label{eq:prod2Gaussians_mean}
c=C(\frac{a}{A}+\frac{b}{B})=\frac{Ab+Ba}{A+B}
\end{equation}

In the context of the MDKF, we are interested in $\mathcal{N}(x;c,C)$, the PDF dependent on $x$. Given an observation $y$ and clean speech $s$, we can represent their joint probability $p(s,y)$ in two ways:

\begin{equation}\label{eq:jointprob}
p(s,y)=p(s)\ p(y|s)=p(y)\, p(s|y)
\end{equation}

Using Bayes' Theorem, we can express this as:

\begin{equation}\label{eq:BayesTheorem}
p(s|y)=\frac{p(y|s)\ p(s)}{p(y)}
\end{equation}

where $p(s)$ is the prior probability of the predicted speech, $p(y)$ is the probability of the observation, $p(s|y)$ is the conditional probability of the speech given the observation and $p(y|s)$ is the conditional probability of the observation given speech.

In the MDKF, we want $p(s|y)$ i.e. we wish to estimate the clean speech given the noisy measurement. To compute this, we require the probabilities $p(y|s)$ and $p(s)$ (Equation \ref{eq:BayesTheorem}). We already have $p(y|s)$ as the noisy observation given clean speech, which comes from the measurement and the assumption of an additive noise model $y=s+n$, where $n$ is the noise. The other term, $p(s)$, can be obtained from an IBM, where we assume that the mask is as accurate as possible; how such a mask is generated in a real-world scenario is not discussed in this report.

Mapping the available and desired probabilities to Equation \ref{eq:prod2Gaussians}, we have available $f(s)=p(s)$ and $g(s)=p(y|s)$. By multiplying these two terms together, we expect a term independent of $s$ and a term dependent on $s$, which are $p(y)$ and $p(s|y)$ respectively. The latter term, which is the conditional probability of the (true) clean speech given the noisy observation, is what we propose to replace the noisy observation term in the MDKF equations.

\subsection{Training Mask Statistics}

The method that we propose to obtain $p(s)$ requires statistical information from an IBM. To obtain the PDF, we need its mean and variance. The mean and variances were calculated separately for mask $1$s and $0$s i.e. separate PDFs were generated for when the mask indicates that speech is dominant and when noise is dominant. This was done separately for each frequency bin.

To generate the IBM, the STFT of the signal is first taken, producing a matrix of time-frequency (T-F) units, and the unit-wise signal-to-noise ratio (SNR) is then computed to produce a binary-valued matrix. In the MDKF setup, the same STFT of the signal is taken; the mask thus provides a speech-dominant/noise-dominant indicator for each T-F unit of the STFT-processed signal.

To generate the speech-dominant PDF for one frequency bin in a speech signal, we picked out the samples of the amplitude spectrum of this frequency bin which had corresponding (the same T-F location) mask values indicating $1$. The mean and variance of these samples was then computed. The same process was done to compute the noise-dominant PDF for the same frequency bin. This was done for each frequency bin, and the values were averaged over all speech samples in the training dataset, which were corrupted by white noise at $5$ dB SNR. To save time, training of mask statistics was done once, with its values stored offline to pull when needed.

\subsection{Modifying Observation}

With the IBM statistics, the observation can then be modified. For reference, the MDKF equations are replicated from Section \ref{MDKF} and shown below:

\begin{equation}\label{eq:MDKF_predictP_repeat}
\textbf{P}(n|n-1,k)=\textbf{A}(k)\textbf{P}(n-1|n-1,k)\textbf{A}(k)^T+\sigma^2_{W(k)}\textbf{d}\textbf{d}^T
\end{equation}

\begin{equation}\label{eq:MDKF_predictXhat_repeat}
\hat{\textbf{X}}(n|n-1,k)=\textbf{A}(k)\hat{\textbf{X}}(n-1|n-1,k)
\end{equation}

\begin{equation}\label{eq:MDKF_updateK_repeat}
\textbf{K}(n,k)=\textbf{P}(n|n-1,k)\textbf{d}[\sigma^2_{V(k)}+\textbf{d}^T\textbf{P}(n|n-1,k)\textbf{d}]^{-1}
\end{equation}

\begin{equation}\label{eq:MDKF_updateP_repeat}
\textbf{P}(n|n,k)=[\textbf{I}-\textbf{K}(n,k)\textbf{d}^T]\textbf{P}(n|n-1,k)
\end{equation}

\begin{equation}\label{eq:MDKF_updateXhat_repeat}
\hat{\textbf{X}}(n|n,k)=\hat{\textbf{X}}(n|n-1,k)+\textbf{K}(n,k)[|Y(n,k)|-\textbf{d}^T\hat{\textbf{X}}(n|n-1,k)]
\end{equation}

where the observation mean is $|Y(n,k)|$ and the observation variance is in $\sigma^2_{V(k)}$. As the MDKF loops through each sample in each modulating signal, it tweaks these parameters for every sample. In each iteration, the algorithm checks the corresponding T-F position in the mask if it indicates a $1$ or $0$, then brings up the relevant mask mean and variance. Using Equations \ref{eq:prod2Gaussians_var} and \ref{eq:prod2Gaussians_mean}, a scaled mean and variance is obtained for the current observation input by multiplying with the relevant mask PDF. This process replaces the original observation mean $|Y(n,k)|$ and variance $\sigma^2_{V(k)}$ in the MDKF equations with $|Y\_scaled|$ and $\sigma^2_{V(k)\_scaled}$ respectively. We thus get modified versions of Equations \ref{eq:MDKF_updateK_repeat} and \ref{eq:MDKF_updateXhat_repeat}:

\begin{equation}\label{eq:MDKF_updateK_IBMmodifyobs}
\textbf{K}(n,k)=\textbf{P}(n|n-1,k)\textbf{d}[\sigma^2_{V(k)\_scaled}+\textbf{d}^T\textbf{P}(n|n-1,k)\textbf{d}]^{-1}
\end{equation}

\begin{equation}\label{eq:MDKF_updateXhat_IBMmodifyobs}
\hat{\textbf{X}}(n|n,k)=\hat{\textbf{X}}(n|n-1,k)+\textbf{K}(n,k)[|Y\_scaled|-\textbf{d}^T\hat{\textbf{X}}(n|n-1,k)]
\end{equation}

\section{Modified Kalman Filter Equations}\label{IBM_KF}

The IBM can be used in a slightly different way; instead of tweaking the observation itself, information from the mask can be directly used in the KF equations. Now, the KF equations use three pieces of information (prediction, observation, binary mask) to estimate the current state, rather than just the former two as in the original KF equations.

\subsection{Decoupling Kalman Filter Equations}

To insert the mask information, the KF equations first need to be decoupled. For the scalar output case i.e. where $|Y(n,k)|$ is a scalar, with $\textbf{d}=[1,0,...,0]^T$, the observation can be decorrelated from the rest of the state vector. Since each modulating signal has a separate Kalman filter, we remove the frequency bin subscript $k$ in this section, and also indicate the time sample index in the subscript for clarity e.g. we represent $\textbf{P}(n|n-1,k)$ as $\textbf{P}_{n|n-1}$. After obtaining the \textit{a priori} error covariance matrix (Equation \ref{eq:MDKF_predictP_repeat}), we can decompose it as:

\begin{equation}\label{eq:decomposePpredicted}
\textbf{P}_{n|n-1}=\left[
                  \begin{array}{ c c }
                  g_n 			& \textbf{g}_n^T \\
                  \textbf{g}_n 	& \textbf{G}_n
                  \end{array} \right]
\end{equation}

noting that the covariance matrix is symmetric. We then apply a transformation $\textbf{R}_n$ to the state vector $\textbf{X}_{n|n-1}$ to obtain:

\begin{equation}\label{eq:transformState}
\textbf{z}_{n|n-1}=\textbf{R}_n\textbf{X}_{n|n-1} = \\ 
					  \left[
                      \begin{array}{ c c }
                      1 						& \textbf{0}^T \\
                      -g_n^{-1}\textbf{g}_n 	& \textbf{I}
                      \end{array} \right]\textbf{X}_{n|n-1}
\end{equation}

The covariance matrix of \textbf{z} is given by (omitting subscript n for clarity):

\begin{equation}\label{eq:zcovariance}
\begin{split}
\langle \textbf{zz}^T \rangle &= \textbf{R} \langle \textbf{X}\textbf{X}^T \rangle \textbf{R}^T = \textbf{RPR}^T \\
							  &=
                                \left[
                                \begin{array}{ c c }
                                1 				  & \textbf{0}^T \\
                                -g^{-1}\textbf{g} & \textbf{I}
                                \end{array} \right]
                                %
                                \left[
                                \begin{array}{ c c }
                                g 			& \textbf{g}^T \\
                                \textbf{g} 	& \textbf{G}
                                \end{array} \right]
                                %
                                \left[
                                \begin{array}{ c c }
                                1 			 & -g^{-1}\textbf{g} \\
                                \textbf{0}^T & \textbf{I}
                                \end{array} \right] \\
                              &=
                                \left[
                                \begin{array}{ c c }
                                g 			& \textbf{0}^T \\
                                \textbf{0} 	& \textbf{G}-g^{-1}\textbf{gg}^T
                                \end{array} \right]  
\end{split}
\end{equation}

which shows that the first element of \textbf{z} is uncorrelated with the rest of \textbf{z}, and is distributed as $\mathcal{N}(\textbf{d}^T\textbf{z}_{n|n-1},g)$. This can then be combined with the observation to obtain the distribution of the updated state, which produces the same formulation as the original Kalman filter equations.

\subsection{Incorporating IBM into decoupled KF equations}

With the mask information available, the mask can be combined with the observation and decoupled state \textbf{z} to improve the KF iterations. Unlike Section \ref{IBMobs}, now the product of three distributions has to be taken, which is done pairwise using the steps in Section \ref{gaussianpdt}.

Assume that the observation has been combined with the relevant mask statistics (recall that speech-dominant ``1''s and ``0''s are distributed separately) using the steps covered in Section \ref{gaussianpdt}, and assume that the result is distributed as $\mathcal{N}(y,r)$. This is then combined with the first element of \textbf{z} to obtain the posterior distribution $\mathcal{N}(\textbf{d}^T\textbf{z}_n;\frac{gy_n+r\textbf{d}^T\textbf{z}_{n|n-1}}{g+r},\frac{gr}{g+r})$.

Its mean value is

\begin{equation}\label{eq:zfirstelementposterior}
\textbf{d}^T\textbf{z}_{n|n}=\frac{1}{g+r}(gy+r\textbf{d}^T\textbf{z}_{n|n-1})=\textbf{d}^T\textbf{z}_{n|n-1}+\frac{g}{g+r}(y-\textbf{d}^T\textbf{z}_{n|n-1})
\end{equation}

For the entire transformed state $\textbf{z}_n$, we can write

\begin{equation}\label{eq:zposterior}
\textbf{z}_{n|n}=\textbf{z}_{n|n-1}+\frac{g}{g+r}(y-\textbf{d}^T\textbf{z}_{n|n-1})\textbf{d}
\end{equation}

Finally, the original state $\textbf{X}_{n|n}$ can be obtained with the reverse transformation:

\begin{equation}\label{eq:Xposterior}
\begin{split}
\textbf{X}_{n|n} &= \textbf{A}^{-1}\textbf{z}_{n|n}\\
                 &=
                  \left[
                  \begin{array}{ c c }
                  1 			   & \textbf{0}^T \\
                  g^{-1}\textbf{g} & \textbf{I}
                  \end{array} \right]\textbf{z}_{n|n}\\
\end{split}
\end{equation}

\section{Performance Results and Discussion}

In this section, the IBM-modified MDKF algorithm uses the modification described in Section \ref{IBM_KF}, and will be termed BMMDKF. The block diagram for the BMMDKF is shown in Figure \ref{fig:blockdiagramBMMDKF}, highlighting the IBM modification in bold.

\begin{figure}[H]
\centering
\includegraphics[width=1\linewidth]{blockdiagramBMMDKF}
\captionof{figure}{Block diagram of IBM-modified MDKF (BMMDKF)}
\label{fig:blockdiagramBMMDKF}
\end{figure}

To evaluate the modified algorithm, the BMMDKF will be compared to a few control algorithms: the original MDKF of Section \ref{MDKF}, the MMSE speech enhancement algorithm in \cite{MMSEspeech} and the original input speech corrupted by white noise. These are respectively named MDKF, MMSE and Noisy in the plots that follow. All methods are evaluated for segSNR, PESQ and STOI, and are tested over the input training dataset described in Section \ref{dataset}, and the results are normalised over all input samples. To standardise testing, the parameters used for all algorithms generally follow those described in Chapter \ref{implementation}. They are shown in Table \ref{table:BMMDKFparams}.

\begin{table}[ht!]
\centering
\begin{tabular}{|c|c|}
\hline
\textbf{Parameter}      		& \textbf{Value} \\ \hline
Sampling frequency      		& 16 kHz         \\ \hline
Acoustic frame length   		& 16 ms          \\ \hline
Acoustic frame shift    		& 4 ms           \\ \hline
Modulation frame length 		& 24 ms          \\ \hline
Modulation frame shift  		& 4 ms           \\ \hline
Windowing function      		& Hamming window \\ \hline
MDKF model order        		& 2              \\ \hline
LPCs generated from     		& clean speech   \\ \hline
Input speech corrupted by     	& white noise    \\ \hline
IBM SNR threshold (LC)  		& 0 dB           \\ \hline
\end{tabular}
\caption{List of parameters used to evaluate BMMDKF and other algorithms}
\label{table:BMMDKFparams}
\end{table}

\clearpage

\begin{figure}[H]
\centering
\includegraphics[width=0.72\linewidth]{BMMDKF_MDKF_MMSE_Noisy_segSNR}
\captionof{figure}{Average segSNR values of BMMDKF and other algorithms vs. speech corrupted by white noise at varying SNR levels}
\label{fig:BMMDKF_MDKF_MMSE_Noisy_segSNR}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.72\linewidth]{BMMDKF_MDKF_MMSE_Noisy_pesq}
\captionof{figure}{Average PESQ values of BMMDKF and other algorithms vs. speech corrupted by white noise at varying SNR levels}
\label{fig:BMMDKF_MDKF_MMSE_Noisy_pesq}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.72\linewidth]{BMMDKF_MDKF_MMSE_Noisy_STOI}
\captionof{figure}{Average STOI values of BMMDKF and other algorithms vs. speech corrupted by white noise at varying SNR levels}
\label{fig:BMMDKF_MDKF_MMSE_Noisy_STOI}
\end{figure}

From Figure \ref{fig:BMMDKF_MDKF_MMSE_Noisy_segSNR}, the segSNR of BMMDKF and MDKF are very similar. However, it is generally true that the segSNR of BMMDKF is slightly larger at input SNRs from $-20$ to $0$ dB.

The enhancement provided by BMMDKF is more obvious in Figures \ref{fig:BMMDKF_MDKF_MMSE_Noisy_pesq} and \ref{fig:BMMDKF_MDKF_MMSE_Noisy_STOI}, in the PESQ and STOI scores respectively. The average PESQ scores of BMMDKF are higher than that of MDKF across all input SNR, and show particularly large increase over the $-20$ to $5$ dB input SNR range: the BMMDKF PESQ scores are on average higher than that of MDKF by $0.1945$, a $10.0174\%$ increase over the MDKF. This improvement rises to $0.2245$ or $11.7359\%$ if we consider the $-20$ to $0$ dB input SNR range. Intelligibility scores also show advancement. Over the $-20$ to $0$ dB input SNR range, STOI scores of BMMDKF are on average higher than that of MDKF by $0.0323$ or $3.8646\%$. Overall, these results show a significant improvement in both perceived quality and intelligibility of enhanced speech.

It is clear from Figures \ref{fig:BMMDKF_MDKF_MMSE_Noisy_pesq} and \ref{fig:BMMDKF_MDKF_MMSE_Noisy_STOI} that the refinement provided by the IBM modification rises with a reduced global input SNR. This trend is unsurprising, as it is generally much more difficult to improve on a very good score as compared to a low score. When the input SNR is high or satisfactory, including the range $0$ to $20$ dB, the baseline MDKF already performs very well. For example, PESQ scores are in the range $3-4$, and STOI scores are at $0.94$ and above; these are very high scores and are difficult to significantly improve on, and thus the BMMDKF shows only small improvements when the input SNR is high. When the input SNR is very low, however, the BMMDKF performance is markedly more enhanced; at $-20$ dB input, its PESQ and STOI scores are $17.86\%$ and $6.43\%$ better than the MDKF respectively.

\section{Conclusion}

This chapter proposes applying an optimal IBM to a training dataset to obtain averaged statistical information corresponding to speech-dominant and noise-dominant regions of white noise-corrupted speech. It is proposed to use this information to improve the parameters used in the MDKF iteration equations. This modification is termed the BMMDKF algorithm.

Performance results show that the BMMDKF shows very similar segSNR results to the MDKF across the board. However, it provides significant improvements over the MDKF in terms of perceived quality and intelligibility when applied on speech corrupted by white noise for a large range of input SNR. The performance gains are much more significant for input signals at much lower SNR. Overall, the BMMDKF has proven to be theoretically preferable to the MDKF in tracking and recovering the underlying clean speech in a noisy input over a range of input noise levels.

With separate PDFs for the signal-dominant and noise-dominant portions of the mask, a condition needs to be checked for each iteration to pull the correct mask statistics. This check, along with the modification of the observation mean and variance, slows the algorithm down by a significant amount. Currently, the enhancer is not being used in a real-time scenario, and thus speed is not crucial. If it is used in a real-time context in the future, however, the code will need to be modified so as to minimise the time delay of the enhancement.


\chapter{Improved LPC Coefficients}\label{LMDKF_chapter}

\section{Introduction}

The framework for the estimation of the linear prediction coefficients (LPCs) used in the MDKF was described earlier in Section \ref{LPC}. Since the LPCs aim to mimic the linear prediction model of speech, the accuracy of LPC estimation is critical to the performance of the enhancer. In this section, we proposed a simple modification to the LPC estimation by incorporating information from an available IBM, with the aim of improving the coefficients generated.

\section{Weighted LPC estimation}

As described in Section \ref{LPC}, LPC estimation using the autocorrelation method minimises the mean squared error by minimising the total prediction error $E$ over all samples.

Instead of a simple summation of all errors, we can instead have a weighted sum that better represents the speech; this information can be provided by a binary mask first applied to the speech. We can then modify Equation \ref{eq:LPC_totalpredictionerror} to get:

\begin{equation}\label{eq:weightedLPC_totalpredictionerror}
E=\sum_{n}w(n)e^2(n)=\sum_{n}w(n)[s(n)+\sum_{k=1}^{p}a_k s(n-k)]^2
\end{equation}

where $w(n)$ represents the weight attached to each speech sample $s(n)$.

This modification essentially represents a multiplication of the autocorrelation function by a scaling factor dependent on an IBM. In this algorithm, unlike the BMMDKF, the binary mask is specifically applied to the input of interest only. In the modulation-domain LPC estimation used in the MDKF, the LPCs are estimated from each modulating signal along with the corresponding binary mask frame, where the modulating signal and the binary mask are windowed in the same way. The windowed mask has values from $0$ to real numbers less than $1$, due to the windowing.

The weights tested were of the form:

\begin{equation}\label{eq:weightsLPCIBM}
    w(n)= 
\begin{cases}
    1+th,& \text{if } IBM(n)==0\\
    1-th,              & \text{otherwise}
\end{cases}
\end{equation}

where $th$ represents a positive threshold value. The optimal threshold was found to be $0.15$ i.e. the optimal weights were determined to be $1+0.15=1.15$ when the mask is zero and $1-0.15=0.85$ when the mask shows a non-zero value. For example, a mask frame of $[0, 0, 0.48, 0]$ gives the weight vector $[1.15, 1.15, 0.85, 1.15]$, which is then multiplied element-wise with the input modulating signal frame. All other steps in the LPC estimation are the same as in Section \ref{LPC}.

It should be noted that these values imply that when the mask indicates noise, the error is given a greater weight as compared to when the mask indicates speech. A possible explanation is that generally, the amplitude of the signal during speech activity is greater than that of speech absence, assuming that the noise is random but always present in the signal. As such, if the mask indicates that a certain time-frequency (T-F) unit contains only noise, the LPC modelled signal should also have noise at the same T-F unit to minimise the overall mean squared error, and likewise for when the mask indicates speech presence. However, speech can generally have a larger variation in amplitude as compared to that of noise only. As such, it can be argued that if the model needs to predict speech absence (i.e. speech absence), it has to do so with greater accuracy than if it needs to predict speech presence. This leads to the need for a greater weight when the mask indicates speech absence as compared to speech presence. Results agree with this weighting, showing some improvements over the original MDKF.

\section{Performance Results and Discussion}
In this section, the enhanced-LPC algorithm will be termed LMDKF, with the other algorithms named as before. The block diagram for the LMDKF is shown in Figure \ref{fig:blockdiagramLMDKF}.

Similarly to previous experiments, the LMDKF is run against other control algorithms, with the parameters used shown in Table \ref{table:LMDKFparams}.

\begin{figure}[H]
\centering
\includegraphics[width=1\linewidth]{blockdiagramLMDKF}
\captionof{figure}{Block diagram of MDKF using IBM-enhanced LPCs (LMDKF)}
\label{fig:blockdiagramLMDKF}
\end{figure}

\begin{table}[ht!]
\centering
\begin{tabular}{|c|c|}
\hline
\textbf{Parameter}      		& \textbf{Value} \\ \hline
Sampling frequency      		& 16 kHz         \\ \hline
Acoustic frame length   		& 16 ms          \\ \hline
Acoustic frame shift    		& 4 ms           \\ \hline
Modulation frame length 		& 24 ms          \\ \hline
Modulation frame shift  		& 4 ms           \\ \hline
Windowing function      		& Hamming window \\ \hline
MDKF model order        		& 2              \\ \hline
LPCs generated from     		& clean speech   \\ \hline
Input speech corrupted by     	& white noise    \\ \hline
IBM SNR threshold (LC)  		& 0 dB           \\ \hline
LPC weight threshold    		& 0.15           \\ \hline
\end{tabular}
\caption{List of parameters used to evaluate LMDKF and other algorithms}
\label{table:LMDKFparams}
\end{table}

\clearpage


\begin{figure}[H]
\centering
\includegraphics[width=0.72\linewidth]{LMDKF_MDKF_MMSE_Noisy_segSNR}
\captionof{figure}{Average segSNR values of LMDKF and other algorithms vs. speech corrupted by white noise at varying SNR levels}
\label{fig:LMDKF_MDKF_MMSE_Noisy_segSNR}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.72\linewidth]{LMDKF_MDKF_MMSE_Noisy_pesq}
\captionof{figure}{Average PESQ values of LMDKF and other algorithms vs. speech corrupted by white noise at varying SNR levels}
\label{fig:LMDKF_MDKF_MMSE_Noisy_pesq}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.72\linewidth]{LMDKF_MDKF_MMSE_Noisy_STOI}
\captionof{figure}{Average STOI values of LMDKF and other algorithms vs. speech corrupted by white noise at varying SNR levels}
\label{fig:LMDKF_MDKF_MMSE_Noisy_STOI}
\end{figure}

Figure \ref{fig:LMDKF_MDKF_MMSE_Noisy_segSNR} shows that the segSNR performance of the LMDKF and MDKF are very similar. In fact, the LMDKF performs very slightly worse, with segSNR scores an average of $-0.1797$ dB relative to the MDKF.

The STOI scores of LMDKF are also generally lower than MDKF (Figure \ref{fig:LMDKF_MDKF_MMSE_Noisy_STOI}). This decrease is small, but peaks out at inputs between $-10$ and $0$ dB, with an average reduction of $0.5418\%$. Overall, the average STOI scores of LMDKF are lower by $0.2514\%$.

Across the board, Figure \ref{fig:LMDKF_MDKF_MMSE_Noisy_pesq} demonstrates that the PESQ scores of the LMDKF are higher than MDKF by an average of $1.5349\%$. For $0$ to $20$ dB input SNR, the LMDKF shows PESQ scores $1.06\%$ better than MDKF, while the improvement rises to $2.01\%$ for $-20$ to $0$ dB input. As with the BMMDKF, the LMDKF performs shows greater performance gains over the MDKF for a lower input SNR, with an improvement of $3.0224\%$ at $-20$ dB input and $0.5585\%$ at $20$ dB.

On the surface, it appears that the changes to segSNR, PESQ and STOI balance out overall. However, it is arguable that segSNR is relatively unimportant compared to the other two measures when it comes to human listeners and their subjective opinions. Furthermore, the largest decrease in STOI score compared to the MDKF is $0.0092$, which is a $1\%$ decrease in intelligibility scores i.e. MDKF is estimated to accurately detect $1\%$ more words than LMDKF, which can be regarded as almost negligible. On the other hand, while the improvement in PESQ score is not huge, it is still significant, and possibly more important to a real listener than a $1\%$ reduction in words recognised. Overall, if we look past the raw numbers and consider what they constitute, the LMDKF is very similar to the MDKF, but it is very possible that the LMDKF is regarded slightly more favourably than the original MDKF in real listening tests.

\section{Conclusion}

This chapter discusses modifying the LPC estimation part of the MDKF by using a weighted sum to compute the mean squared error, using an IBM applied on the input signal to determine these weights. Unlike the BMMDKF of Chapter \ref{BMMDKF_chapter}, the IBM here is specifically applied to the signal of interest, to determine the time-frequency (T-F) areas of the signal to modify in order to come up with useful weighting.

This modification shows very minor degradation in segSNR and STOI scores, and a small but moderately significant improvement in PESQ scores. Overall, the LMDKF is very similar to the MDKF. However, the various numerical scores were analysed further, and it is conceivable that the LMDKF would be preferred over the MDKF.



\chapter{IBM-improved Noise Estimation}\label{NMDKF_chapter}

\section{Introduction}

Noise estimation is a critical part of speech enhancement, and the performance of enhancement algorithms is heavily affected by the accuracy of the noise estimation. In this chapter, a modification of an existing noise spectral estimation method based on an ideal binary mask (IBM) is proposed.

A popular method to estimate the noise power spectral density (PSD) is to use a minimum mean-squared-error (MMSE) optimal estimation method, which can be interpreted as a VAD-based noise power estimator. This chapter proposes a few tweaks to the parameters used in the algorithm described in \cite{MMSEnoiseBetter}, based on the information provided by an IBM.

\section{MMSE Noise Estimation}

To aid the understanding of the modifications made in this chapter, we first present a summary of the MMSE noise estimator in \cite{MMSEnoiseBetter}. A majority of the equations are left out for clarity, and only those critical to the proposed modification will be inserted in this report.

Let $y$, $s$ and $n$ be the noisy speech, clean speech and noise respectively, and assume that they are additive in the short-time Fourier domain, giving the noisy observed speech as $Y=S+N$, where time and frequency indices are omitted for convenience and $S$, $N$ are the complex spectral speech and noise components respectively. The clean speech and noise are assumed independent, such that $\mathbb{E}(|Y|^2)=\mathbb{E}(|S|^2)+\mathbb{E}(|N|^2)$. Define the spectral speech and noise power as $\mathbb{E}(|S|^2)=\sigma_S^2$ and $\mathbb{E}(|N|^2)=\sigma_N^2$ respectively, and the \textit{a priori} and \textit{a posteriori} Signal-to-Noise Ratio (SNR) by $\zeta=\sigma_S^2/\sigma_N^2$ and $\gamma=|y|^2/\sigma_N^2$ respectively.

It is assumed that the noise and speech spectral coefficients ($p_N(n)$ and $p_S(s)$ respectively) have complex Gaussian distributions. This gives a complex Gaussian distribution for the noisy speech $p_Y(y)$ which depends on the true SNR. The noise power estimator shown in \cite{MMSEnoise} is based on an MMSE estimate of the noise periodogram, which can be obtained by calculating the conditional expectation $\mathbb{E}(|N|^2|y)$ (equation omitted for clarity), which is a function of the power of the noisy observation, the \textit{a priori} SNR and the spectral noise power.

In practice, the \textit{a priori} SNR and the spectral noise power have to be estimated. When estimating noise power, it is a common assumption that the noise signal varies more slowly than speech \cite{noiseVarySlower}. Therefore, \cite{MMSEnoise} uses the spectral noise power estimate of the previous time frame ($l-1$) i.e. $\hat{\sigma}_N^2=\hat{\sigma}_N^2(l-1)$, assuming some correlation between the noise in adjacent frames of speech.

Estimating the \textit{a priori} SNR is more complicated, as speech tends to vary more rapidly between successive frames. In \cite{MMSEnoise}, the proposed method to estimate $\hat{\zeta}$ uses a maximum-likelihood (ML) estimate followed by bias compensation. When doing so, the MMSE estimator can be viewed as a hard-threshold voice activity detector (VAD) based decision between the noisy observation and the spectral noise power estimate. The resultant estimator is biased and requires bias compensation.

Finally, after estimating the noise periodogram from $\mathbb{E}(|N|^2|y)$, the noise PSD is obtained via recursive smoothing with a parameter $\alpha_{pow}=0.8$ \cite{MMSEnoise}:

\begin{equation}\label{eq:noisePSDfromPeriodogram}
\hat{\sigma}_N^2(l)=\alpha_{pow}\hat{\sigma}_N^2(l-1)+(1-\alpha_{pow})\mathbb{E}(|N|^2|y(l))
\end{equation}

\subsection{Assumptions in Unbiased MMSE Estimator}

The previous section dealt with the original noise power MMSE estimator as described in \cite{MMSEnoise}. A modification was made in \cite{MMSEnoiseBetter} to produce an unbiased estimator based on a soft speech presence probability (SPP) estimate with fixed priors, instead of a hard VAD threshold. This section covers part of the modification, and in particular highlights certain assumptions made that can be improved using IBM information.

When speech presence is uncertain, an MMSE estimator for the noise periodogram is given by:

\begin{equation}\label{eq:noisePeriodogramUncertainSPP}
\mathbb{E}(|N|^2|y)=P(H_0|y)\mathbb{E}(|N|^2|y,H_0)+P(H_1|y)\mathbb{E}(|N|^2|y,H_1)
\end{equation}

where $H_0$ and $H_1$ indicate speech absence and speech presence respectively. Using Bayes' Theorem, the \textit{a posteriori} SPP can be expressed as:

\begin{equation}\label{eq:aposterioriSPP}
P(H_1|y)=\frac{P(H_1)p_{Y|H1}(y)}{P(H_0)p_{Y|H0}(y)+P(H_1)p_{Y|H1}(y)}
\end{equation}

Thus, computing the \textit{a posteriori} SPP requires the \textit{a priori} probabilities $P(H_1)=1-P(H_0)$ and the likelihood functions for speech presence $p_{Y|H1}(y)$ and speech absence $p_{Y|H0}(y)$. Without an observation, \cite{MMSEnoiseBetter} assumes that a time-frequency point being considered is equally likely to contain speech or not contain speech i.e. uniform priors $P(H_1)=P(H_0)=0.5$ were chosen, independent of the observation. Given an IBM, this can be improved.

The likelihood functions $p_{Y|H1}(y)$ and $p_{Y|H0}(y)$ in Equation \ref{eq:aposterioriSPP} indicate how well the observation $y$ fits the modelling parameters for speech presence and absence respectively, and can be modelled with complex Gaussian distributions (equations omitted). The likelihood under speech presence depends on the \textit{a priori} SNR. While the algorithm in \cite{MMSEnoise} uses a complex Gaussian distribution for the noisy observation $p_Y(y)$ which depends on the true local \textit{a priori} SNR, the likelihood under speech presence is instead a function of a parameterised \textit{a priori} SNR, reflecting the typical SNR when speech is present. In \cite{MMSEnoiseBetter}, this typical SNR is fixed at $15$ dB. This is another area in which information from an IBM can be useful.

Substituting the likelihood functions for speech absence and speech presence (equations omitted in this report) into Equation \ref{eq:aposterioriSPP}, an expression for the \textit{a posteriori} SPP can be obtained as:

\begin{equation}\label{eq:aposterioriSPP_likelihood}
P(H_1|y)=\Bigg(1+\frac{P(H_0)}{P(H_1)}(1+\zeta_{H_1})e^{-\frac{|y|^2}{\hat{\sigma}_N^2}\frac{\zeta_{H_1}}{1+\zeta_{H_1}}}\Bigg)^{-1}
\end{equation}

where the spectral noise power estimate of the previous time frame is used i.e. $\hat{\sigma}_N^2=\hat{\sigma}_N^2(l-1)$ as before.

If the noise power estimate $\hat{\sigma}_N^2$ underestimates the true noise power $\sigma_N^2$, the \textit{a posteriori} SPP in Equation \ref{eq:aposterioriSPP_likelihood} will be overestimated, and the noise power will not be tracked as quickly as needed. In the worst case, the noise power might remain the same. To avoid this stagnation due to underestimated noise power, a check is done in \cite{MMSEnoiseBetter} to verify if the \textit{a posteriori} SPP has been close to $1$ for a long time. The \textit{a posteriori} SPP is first recursively smoothed over time: $\bar{P}(l)=0.9\bar{P}(l-1)+0.1P(H_1|y(l))$. If this smoothed quantity is larger than $0.99$, the update is deemed to have stagnated; the current \textit{a posteriori} SPP estimate $P(H_1|y(l))$ is then forced to be lower than $0.99$.

The noise power spectrum can then be computed from the estimated \textit{a posteriori} SPP $P(H_1|y(l))$, which is used to weight the noisy input power spectrum $|Y|^2$ and the previous estimate of the noise power $\hat{\sigma}_N^2(l-1)$ accordingly:

\begin{equation}\label{eq:rawNoiseSpectrum}
\mathbb{E}(|N|^2|y(l))=P(H_1|y(l))\hat{\sigma}_N^2(l-1)+[1-P(H_1|y(l))]|Y(l)|^2
\end{equation}

Finally, this raw noise estimate is smoothed as in Equation \ref{eq:noisePSDfromPeriodogram} to obtain the estimated noise PSD.

\section{Modifying Noise Estimation}

In the previous section which briefly reviewed the MMSE noise estimators of \cite{MMSEnoise} and \cite{MMSEnoiseBetter}, some parameters are notably fixed. In this section, we use an IBM to provide information that allows us to vary these parameters.

Firstly, a typical speech-present SNR of $15$ dB is used when estimating the noise power for each frame. This provides an overall optimal result, but can be improved if a binary mask can indicate speech absence or presence. This SNR was therefore adaptively modified to be increased or decreased depending on what is indicated by the IBM for each frame. The IBM frame was not used directly, but mapped to a set of weights determined by the formula:

\begin{equation}\label{eq:weightsNoiseIBM_withvalues}
    w(n)= 
\begin{cases}
    th,& \text{if } IBM(n)==0\\
    1.5+th,              & \text{otherwise}
\end{cases}
\end{equation}

where $th$ is an empirically-determined positive constant. Notice that if the mask indicates speech presence, the corresponding weight is larger than $1$, essentially boosting the speech. On the whole, this was found to produce better results.

The speech-present SNR was then multiplied by the mean of the current IBM frame, and this modified SNR was used for the current frame to calculate the \textit{a posteriori} SPP.

However, this alone was found to be insufficient in improving the noise estimation. Thus, the IBM frame was additionally used to weight the \textit{a posteriori} SPP found in Equation \ref{eq:aposterioriSPP_likelihood}. Unlike the adjustment to the SNR, the SPP frame, which is used to obtain the noise estimate, was multiplied element-wise by the weighting IBM with values as determined in Equation \ref{eq:weightsNoiseIBM}. Combining these two adjustments showed the best results.

\section{Optimal IBM-Modified Estimator Threshold}

Given the modification, an optimal threshold for the binary mask needed to be found. A few threshold values were tested and compared in the plots below, using a $0$ dB LC threshold IBM. The following plots compare the ratio of an MDKF using the modified noise estimation and the original MDKF, plotted over a range of input SNR values and comparing segSNR, PESQ and STOI: a ratio of $>1$ means that the IBM-modified noise estimation performs better than the original.

\begin{figure}[H]
\centering
\includegraphics[width=0.72\linewidth]{estnoise_segSNRratios}
\captionof{figure}{Average segSNR ratios comparing IBM-modified noise estimation with original MMSE noise estimation vs. speech corrupted by white noise at varying SNR levels}
\label{fig:estnoise_segSNRratios}
\end{figure}

\begin{figure}[ht!]
\centering
\includegraphics[width=0.72\linewidth]{estnoise_PESQratios}
\captionof{figure}{Average PESQ ratios comparing IBM-modified noise estimation with original MMSE noise estimation vs. speech corrupted by white noise at varying SNR levels}
\label{fig:estnoise_PESQratios}
\end{figure}

\begin{figure}[ht!]
\centering
\includegraphics[width=0.72\linewidth]{estnoise_STOIratios}
\captionof{figure}{Average STOI ratios comparing IBM-modified noise estimation with original MMSE noise estimation vs. speech corrupted by white noise at varying SNR levels}
\label{fig:estnoise_STOIratios}
\end{figure}

Figures \ref{fig:estnoise_segSNRratios} to \ref{fig:estnoise_STOIratios} show that $th=1.0$ is the best value. Although it has the worst-performing segSNR over most of the input SNR range tested, it performs best for PESQ and STOI over most of the input SNR range, and these measures are arguably more representative of how a human listener perceives the enhanced speech as compared to an SNR measure such as segSNR. Therefore, $th=1.0$ was used as the threshold, producing the IBM-mapped weights:

\begin{equation}\label{eq:weightsNoiseIBM}
    w(n)= 
\begin{cases}
    1.0,& \text{if } IBM(n)==0\\
    2.5,              & \text{otherwise}
\end{cases}
\end{equation}

\subsection{Estimated Global SNR}

\begin{figure}[ht!]
\centering
\includegraphics[width=0.7\linewidth]{estnoise_compareglobalSNR}
\captionof{figure}{Estimated global SNR of different noise estimation methods vs. actual global SNR of input speech}
\label{fig:estnoise_compareglobalSNR}
\end{figure}

In Figure \ref{fig:estnoise_segSNRratios}, we saw that the segSNR of the optimal-threshold ($th=1.0$) IBM-modified noise estimator is largely lower than that of the original MMSE noise estimator of \cite{MMSEnoiseBetter}. This is verified in Figure \ref{fig:estnoise_compareglobalSNR}.

Figure \ref{fig:estnoise_compareglobalSNR} compares the averaged estimated global SNR of the IBM-modified noise estimation algorithm and the original MMSE noise estimator, plotted against the actual input global SNR. In the plot, the true $45^{\circ}$ line is represented by a green dotted line i.e. the closer to the green line, the more accurate the SNR estimation. The plot shows that both algorithms tend to underestimate the noise level (overestimate SNR) when the noise is very large ($-15$ dB input SNR or lower), and vice versa when the noise is small (high input SNR). However, an across-the-board feature is that the original MMSE noise estimator largely matches the real value better than the IBM-modified noise estimator.

A possible explanation for this is that the noise is random, and the MMSE noise estimator in \cite{MMSEnoiseBetter} uses only a fixed \textit{a priori} SNR value typical of speech. However, incorporating IBM data meant tweaking the original spectral components, which meant modifying this SNR value adaptively. Using the threshold values determined in Figures \ref{fig:estnoise_PESQratios} and \ref{fig:estnoise_STOIratios} better represented the actual speech information, but might not represent the numerical SNR as well as the original algorithm.

\section{Performance Results and Discussion}

In this section, the algorithm using the IBM-enhanced noise estimate will be termed NMDKF, with the other algorithms named as before. The block diagram for the proposed modification is shown in Figure \ref{fig:blockdiagramNMDKF}.

\begin{figure}[H]
\centering
\includegraphics[width=1\linewidth]{blockdiagramNMDKF}
\captionof{figure}{Block diagram of MDKF using IBM-modified noise estimation (NMDKF)}
\label{fig:blockdiagramNMDKF}
\end{figure}

As with all previous experiments, the algorithm discussed in this section is run against other control algorithms, with the parameters used shown in Table \ref{table:NMDKFparams}.

\begin{table}[ht!]
\centering
\begin{tabular}{|c|c|}
\hline

\textbf{Parameter}      		& \textbf{Value} \\ \hline
Sampling frequency      		& 16 kHz         \\ \hline
Acoustic frame length   		& 16 ms          \\ \hline
Acoustic frame shift    		& 4 ms           \\ \hline
Modulation frame length 		& 24 ms          \\ \hline
Modulation frame shift  		& 4 ms           \\ \hline
Windowing function      		& Hamming window \\ \hline
MDKF model order        		& 2              \\ \hline
LPCs generated from     		& clean speech   \\ \hline
Input speech corrupted by     	& white noise    \\ \hline
IBM SNR threshold (LC)  		& 0 dB           \\ \hline
IBM zeros threshold     		& 1.0            \\ \hline
IBM ones threshold      		& 2.5            \\ \hline
\end{tabular}
\caption{List of parameters used to evaluate NMDKF and other algorithms}
\label{table:NMDKFparams}
\end{table}

\clearpage

\begin{figure}[H]
\centering
\includegraphics[width=0.72\linewidth]{NMDKF_MDKF_MMSE_Noisy_segSNR}
\captionof{figure}{Average segSNR values of NMDKF and other algorithms vs. speech corrupted by white noise at varying SNR levels}
\label{fig:NMDKF_MDKF_MMSE_Noisy_segSNR}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.72\linewidth]{NMDKF_MDKF_MMSE_Noisy_pesq}
\captionof{figure}{Average PESQ values of NMDKF and other algorithms vs. speech corrupted by white noise at varying SNR levels}
\label{fig:NMDKF_MDKF_MMSE_Noisy_pesq}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.72\linewidth]{NMDKF_MDKF_MMSE_Noisy_STOI}
\captionof{figure}{Average STOI values of NMDKF and other algorithms vs. speech corrupted by white noise at varying SNR levels}
\label{fig:NMDKF_MDKF_MMSE_Noisy_STOI}
\end{figure}

The NMDKF performs very similar to the MDKF in terms of segSNR, as shown by Figure \ref{fig:NMDKF_MDKF_MMSE_Noisy_segSNR}. Up to and including $10$ dB input, its segSNR is marginally worse than MDKF by $0.1244$ dB on average. From $10$ to $20$ dB input, the segSNR of NMDKF is lower by $0.7076$ dB on average.

In terms of PESQ scores, the NMDKF shows improvements over the input SNR range of $-20$ to $5$ dB compared to the MDKF by an average of $0.0323$ or $1.68\%$ (Figure \ref{fig:NMDKF_MDKF_MMSE_Noisy_pesq}). This is similar to the LMDKF. However, above $5$ dB input, the NMDKF PESQ scores begin to peak out, unlike the MDKF which continues to steadily rise. From $10$ to $20$ dB input, the average NMDKF PESQ scores are $0.1468$ lower than the MDKF, with the discrepancy at $-0.2714$ for $20$ dB input.

From Figure \ref{fig:NMDKF_MDKF_MMSE_Noisy_STOI}, the STOI scores of NMDKF and MDKF are very similar: the largest difference is at $-20$ dB input, where the NMDKF score is $0.0012$ higher than MDKF. Overall, the discrepancies are minor, but the general trend is that the NDMKF performs slightly better at low input SNR ($-20$ to $0$ dB), while it performs similarly or marginally worse at higher input SNR ($0$ to $20$ dB).

For the most part, the NMDKF performs similarly or shows small improvements over the MDKF at low input SNR, while it performs similarly or worse at higher input SNR. This can lead to the conclusion that the NMDKF would not be preferred over the MDKF. However, consider that at high input SNR, the MDKF already posts very high scores by all measures. While the PESQ score of NMDKF is lower by MDKF by $0.2714$ at $20$ dB input, the scores are $3.5857$ and $3.8572$ for the NMDKF and MDKF respectively. On the PESQ scale, these are fairly high scores, and while the reduction in PESQ admittedly hurts quality and user experience, it is still a generally satisfactory score. Furthermore, the NMDKF PESQ scores are better where it is arguably more important to improve the scores: at lower input SNR. As mentioned previously, the NMDKF shows improvements from $-20$ to $5$ dB input SNR as compared to the MDKF by an average of $0.0323$. Informal listening tests showed a general preference for the NMDKF at these lower input SNR, while the difference at higher input SNR was significantly less perceptible.

\section{Conclusion}

This chapter proposes a modification to the MMSE noise estimation method done in \cite{MMSEnoiseBetter}, by incorporating information from an IBM to tweak certain fixed parameters such as SPP and \textit{a priori} SNR. By and large, performance results show that the NMDKF performs better than the MDKF at lower input SNR and vice versa at higher input SNR. Overall, the modified algorithm performs similarly to the MDKF.

However, the NMDKF generally performs better than the original MDKF up to an input signal SNR of approximately $5$ dB. It is possible that an adaptive algorithm could be used to apply the noise estimation that performs better depending on the input noise level. Doing so will take extra time, and thus this selective framework could be used if speed was not of concern e.g. this might not be suitable for a real-time processing application.



\chapter{Conclusion and Future Work}

In this project, we explored three approaches to modifying a speech enhancement method based on a modulation-domain Kalman filter (MDKF), by using information extracted from an ideal binary mask (IBM). The aim was to improve the perceived quality and intelligibility of the enhanced noisy speech. The first approach is to include IBM information into the parameters of the MDKF iteration equations directly, covered in Chapter \ref{BMMDKF_chapter}. The next method, proposed in Chapter \ref{LMDKF_chapter}, proposes to modify estimation of the linear prediction coefficients (LPCs) by using a weighted sum to calculate the total error. In Chapter \ref{NMDKF_chapter}, the final method uses the IBM to tweak noise estimation parameters.

In Chapter \ref{BMMDKF_chapter}, the IBM is applied to a set of training input samples to obtain separate averaged probability distribution statistics corresponding to speech-dominant and noise-dominant regions of the mask. This information is used to modify the observation mean and variance of an input noisy test signal to better track the underlying clean speech (BMMDKF method). Performance results show that the BMMDKF shows very similar segSNR results to the MDKF. However, it demonstrates significant improvements over the MDKF in terms of both perceived quality (PESQ) and intelligibility (STOI) when applied on noisy speech for a large range of input SNR, with larger performance gains at lower input SNR. Overall, the BMMDKF has proven to be preferable to the MDKF in enhancing noisy speech for a large range of input SNR.

The next modification, discussed in Chapter \ref{LMDKF_chapter}, is concerned with tweaking the LPC estimation, which is used to obtain LPC coefficients used in the MDKF filter equations. The original LPC estimation aims to minimise the sum of errors; this method (LMDKF method) suggests to instead minimise a weighted sum of errors, with the weights determined from an IBM applied directly to the test signal. Unlike the method in Chapter \ref{BMMDKF_chapter}, the IBM here is applied to the specific input test signal only. The LMDKF shows very minor degradation in segSNR and STOI scores compared to the MDKF, and a small but moderately significant improvement in PESQ scores. Overall, it is similar to the MDKF, but when the various numerical scores were analysed further, it is conceivable that the LMDKF would be preferred over the MDKF due to the importance of the PESQ improvement.

Finally, Chapter \ref{NMDKF_chapter} investigates a method (NMDKF method) to improve the MMSE noise estimation of \cite{MMSEnoiseBetter}, which is used to determine the error covariance matrix in the MDKF equations. The modification incorporates information from an IBM to tweak certain fixed parameters such as SPP and \textit{a priori} SNR, with the aim  of calculating the actual noise in each frame more accurately. On the whole, the modified algorithm performs similarly to the MDKF. However, the NMDKF shows improvements over the MDKF up to an input SNR of $5$ dB. An adaptive algorithm could be used to apply the better-performing noise estimation method depending on the input noise level. Doing so will take more time, and so this may not be acceptable for a real-time speech enhancer.

\section{Future Work}

\subsection{LPC Estimation}

In the ``oracle'' baseline model used for all modifications, shown in Figure \ref{fig:blockdiagramMDKF}, LPC coefficients are calculated from the clean speech, as this project is more concerned with investigating the theoretical upper bound performance. In a real-world context, the clean speech is not available. A possible area for further study is to instead estimate the LPCs from the noisy speech passed through the MMSE speech enhancement algorithm used in \cite{MMSEspeech}. Although not as accurate as clean-speech LPCs, the effect of noise on the model can be greatly reduced when calculating LPCs from MMSE-enhanced speech rather than from the original noisy speech. If the results are favourable, this can be used for practical applications as it does not involve clean speech.

\subsection{Real Listening Tests}

In this project, PESQ and STOI routines were used to try and mimic the perceived quality and intelligibility measures of human listeners. While these routines have been proven to be highly correlated with the subjective scores, there is still merit to holding listening tests with real human listeners, to provide a more accurate performance measures and also allow for individual feedback.

\subsection{Probability Distribution}

In Chapter \ref{BMMDKF_chapter}, the BMMDKF algorithm assumes that the observation, prediction and IBM statistics are all Gaussian distributed. This simplifies the computation when combining these three parameters, and is a fairly accurate assumption. However, if allowed for additional computational complexity and algorithm runtime, it might be possible to investigate further the properties of these parameters, and find a better matching distribution. It is possible that these provide better performance gains that the BMMDKF, but at the end of the day the trade-offs between performance gains and algorithm complexity must be weighted and balanced.

\subsection{Combining Modifications}

This report discusses a few modifications to a baseline MDKF that provide varying degrees of improvements. It is possible that combining two or more of them could produce results better than any individual modification could achieve, and could be further investigated.


\newpage
\addcontentsline{toc}{chapter}{Bibliography}
\printbibliography


\end{document}