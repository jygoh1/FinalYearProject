\documentclass{report}

\usepackage{blindtext}
\usepackage{titlesec}
\usepackage{listings}
\usepackage{geometry}
\usepackage{mathtools}
\usepackage[utf8]{inputenc}
\usepackage{siunitx}
\usepackage{soul}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{float}
\usepackage{units}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{minted}
\usepackage{caption}
\usepackage{csquotes}
\usepackage{nomencl}
\usepackage{fancyhdr}
 
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{\rightmark}
\fancyhead[R]{\thepage}

\renewcommand{\sectionmark}[1]{\markright{\thesection~#1}}

\usepackage[
backend=biber,
style=numeric,
sorting=none        % sort by order of appearance
]{biblatex}
\addbibresource{biblio.bib}

\setlength{\parskip}{0.7em}   		%paragraph separation = 0.9em
\setlength{\parindent}{0pt} 		% 0 paragraph indent


\begin{document}


\begin{titlepage}
\newgeometry{top=36mm,bottom=36mm,left=40mm,right=40mm}
\setlength{\parindent}{0pt}
\setlength{\parskip}{0pt}

{
\Large
\raggedright
  Imperial College London\\[17pt]
  Department of Electrical and Electronic Engineering\\[17pt]
  Final Year Project 2017: Final Report\\[17pt]
}

\rule{\columnwidth}{3pt}
\vfill
\centering
\includegraphics[width=0.7\columnwidth,height=60mm,keepaspectratio]{ic_crest.png}
\vfill
\setlength{\tabcolsep}{0pt}

\begin{tabular}{p{40mm}p{\dimexpr\columnwidth-40mm}}
Project Title: & \textbf{Quality-preserving Speech Intelligibility Enhancement using a Kalman Filter} \\[12pt]
Student: & \textbf{Jia Ying Goh} \\[12pt]
CID: & \textbf{00749529} \\[12pt]
Course: & \textbf{4T} \\[12pt]
Project Supervisor: & \textbf{Brookes, D.M.} \\[12pt]
Second Marker: & \textbf{Evers, C.} \\[12pt]
\end{tabular}
\end{titlepage}

\restoregeometry


\newpage
\pagenumbering{roman}
\begin{abstract}
Speech enhancement algorithms aim to reduce the background noise of a noise-corrupted speech input without distorting the original clean speech. In real-world situations, this can be very challenging. Although many algorithms have been developed to improve the Signal-to-Noise Ratio (SNR) of the noisy input, they also introduce speech distortion and artifacts such as musical noise, damaging speech quality and intelligibility. Recently, there has been growing psychoacoustic and physiological evidence to support the use of the modulation domain for speech enhancement, where the modulation domain is defined as the temporal variations of the acoustic spectral components. This report proposes modifications to existing modulation-domain speech processing methods, where an Ideal Binary Mask (IBM) will be applied to training samples of noisy speech to obtain averaged statistical information that can then be applied on new test samples. The goal is to use this data to improve the performance of an existing modulation-domain Kalman Filter (MDKF). The performance of these proposed modifications is assessed by measuring the SNR, speech quality (using Perceptual Evaluation of Speech Quality or PESQ) and speech intelligibility (Short-Time Objective Intelligibility or STOI) of the enhanced speech. Results show that the developed algorithms provide some improvements in both speech quality and intelligibility over a range of input noise levels.
\end{abstract}


\newpage
\tableofcontents
\clearpage


\nomlabelwidth=30mm				% indentation between nomenclature acronym and description
\makenomenclature
\nomenclature{\textbf{LMS}}{Least Mean Squares}
\nomenclature{\textbf{NLMS}}{Normalised Least Mean Squares}
\nomenclature{\textbf{RLS}}{Recursive Least Squares}
\nomenclature{\textbf{STFT}}{Short-Time Fourier Transform}
\nomenclature{\textbf{MMSE}}{Minimum Mean Squared Error}
\nomenclature{\textbf{MS}}{Minimum Statistics}
\nomenclature{\textbf{VAD}}{Voice Activity Detector}
\nomenclature{\textbf{IBM}}{Ideal Binary Mask}
\nomenclature{\textbf{TBM}}{Target Binary Mask}
\nomenclature{\textbf{AMS}}{Analysis-Modification-Synthesis}
\nomenclature{\textbf{KF}}{Kalman Filter}
\nomenclature{\textbf{LPC}}{Linear Prediction Coefficients}
\nomenclature{\textbf{TDKF}}{Time-Domain Kalman Filter}
\nomenclature{\textbf{MDKF}}{Modulation-Domain Kalman Filter}
\nomenclature{\textbf{PESQ}}{Perceptual Evaluation of Speech Quality}
\nomenclature{\textbf{STOI}}{Short-Time Objective Intelligibility}
\nomenclature{\textbf{SNR}}{Signal-to-Noise Ratio}
\nomenclature{\textbf{segSNR}}{Segmental Signal-to-Noise Ratio}
\nomenclature{\textbf{fwSNRseg}}{Frequency-Weighted Signal-to-Noise Ratio}
\nomenclature{\textbf{MOS}}{Mean Opinion Score}
\nomenclature{\textbf{PDF}}{Probability Density Function}
\nomenclature{\textbf{ML}}{Maximum Likelihood}
\nomenclature{\textbf{PSD}}{Power Spectral Density}
\printnomenclature


\chapter{Introduction}
\pagenumbering{arabic}

\section{Motivation}

In today's highly interconnected world, communication between people, as well as with the world around them, is a major and critical aspect of their lives. Among the methods of communication (including but not limited to speech, text, images and bodily cues), speech generally stands out as the most efficient. Other methods such as visual indicators are sometimes useful to communicate ideas and thoughts, but a complex message is often best brought across via speech.

Applications utilising speech are thus widespread and numerous, and are generally designed to make use of clean speech. In a real-world environment, however, when speech is recorded, the recording inherently picks up not just the speech signal of interest, but also undesired background noise and channel noise. This damages the quality and intelligibility of the recorded speech, which poses a major problem for these applications requiring undamaged speech. Speech enhancement is hence often needed, with the goal of restoring the desired speech signal from the noisy mix, ideally by eliminating this noise while retaining the quality and intelligibility of the original speech signal.

There are various types of noise, including but not limited to additive noise, convolution noise and transcoding noise \cite{Loizou2007}. Additive acoustic noise that is uncorrelated with the speech signal generally degrades the intelligibility and quality of the perceived speech, and in cases of large noise may dominate and mask out the original speech. Convolution noise, on the other hand, manifests as reverberation, which is introduced by acoustic reflection, degrading intelligibility. Unlike additive noise, reverberation is highly correlated with the speech signal. Finally, transcoding noise can occur due to amplitude clipping in a microphone and appears as distortion. This project is concerned with the removal of additive acoustic noise.

Speech enhancement methods can be broadly classified into two types. Single-channel methods consider a single signal source. On the other hand, multi-channel methods consider multiple speech signals obtained from multiple microphones, where additional noise reduction can be achieved using information unavailable to a method relying on a single source, such as phase alignment from multiple microphones, leading to better overall noise reduction. However, this introduces additional costs and complexity, and in many applications such as hearing aids and mobile phones, single-channel methods are necessary due to constraints such as size. This project focuses on single-channel speech enhancement methods.

However, speech enhancement is complex. Traditional speech enhancement techniques such as spectral subtraction have very successfully improved speech quality by attenuating noise, but they tend to introduce speech spectral distortion \cite{FukaneSahare2011}, thus damaging its intelligibility. This project therefore aims to modify existing techniques to improve both the quality and intelligibility of speech.

\section{Project Objectives}

In this project, the objective is to improve both speech quality and intelligibility by modifying an existing speech enhancement algorithm. Standard tests for quality and intelligibility, include the Perceptual Evaluation of Speech Quality (PESQ, \cite{PESQ}) and Short-Time Objective Intelligibility (STOI, \cite{STOI}), will be used to quantify the enhanced speech. 

Specifically, this project aims to modify an existing speech enhancement algorithm based on a Kalman filter, by further incorporating additional information obtained from a so-called ``ideal binary mask''. The proposed method is to scale the predicted value in the Kalman filter and modify its variance by an amount pre-determined from training data, with the goal that PESQ and STOI increase.

\section{Project Scope}

This project assumes the binary mask is already provided; how it is generated is therefore out of scope of this project. This project focuses on incorporating a given estimated binary mask into an existing Kalman filter speech enhancement implementation.

This project makes use of MATLAB and signal processing techniques. In particular, the project utilises VOICEBOX, a speech processing toolbox for MATLAB \cite{VOICEBOX}, which is included in the Imperial College London Software Library.

\section{Report Overview}

This report is categorised into a few main chapters. Chapter 1 introduces and provides context to the problem, while providing a high-level overview of the project objectives and scope. Chapter 2 provides detailed description of the required background information, particularly providing in-depth information regarding the relevant algorithms and performance evaluation methods.

Chapter 3 analyses the project, breaking down the requirements into specific deliverables. It also describes the implementation of the baseline algorithm and how the improvements will be made. Chapter 4 briefly covers the testing methodology and how the algorithms will be evaluated.

Chapter 5 covers the first proposed modification, whereby an ideal binary mask is applied to a set of training speech samples. This gives useful statistical information that is then used to improve on a baseline Kalman filter implementation. Its performance relative to existing methods is evaluated and discussed.

Chapter 6 explores a different approach. Here, the linear prediction coefficients used in a Kalman filter algorithm are modified by using a weighted sum to calculate the autocorrelation function, with weights determined from an ideal binary mask applied to the input signal. As before, this enhancer is evaluated and discussed relative to existing methods.

A third modification is proposed in Chapter 7. Noise estimation is an important part of speech enhancement, and a method is proposed to incorporate binary mask information to improve the accuracy of noise estimation in a Kalman filter-based speech enhancer. Finally, Chapter 8 concludes the project and briefly explores possible areas for future work.


\chapter{Background}

The goal of a speech enhancement algorithm is to reduce the background noise of a noise-corrupted speech input without distorting the underlying clean speech \cite{SpeechEnhancement2005}. Traditionally, speech enhancement algorithms for noise reduction can be grouped into three main categories: noise reduction via filtering techniques, noise reduction via spectral restoration, and speech-model-based noise reduction methods \cite{Springer2007}. Overall, speech enhancement techniques aim to improve the speech using audio signal processing techniques.

Some widely-used speech enhancement methods are described in this chapter. This chapter also provides the background to some relevant aspects of speech enhancement, including possible operating domains, noise estimation, and performance evaluation measures.

\section{Enhancement Domains}

Speech enhancement can be performed in one of several domains. The following sections briefly describe these domains.

\subsection{Time Domain}

In the time domain, speech is usually enhanced using fixed or adaptive filtering techniques \cite{Widrow1975}. Fixed filters require prior knowledge of both the clean signal and noise, while this is not required for adaptive filters, which are able to adjust their parameters according to an optimisation algorithm, with little to no knowledge of the signal or noise characteristics, thus being more practical.

There are different approaches to adaptive filtering, one of which is the Least-Mean-Squares (LMS) filter. LMS algorithms aim to mimic a desired filter by finding a set of filter coefficients to minimise the mean squared error, where the error is the difference between the desired and actual signal \cite{WidrowHoff60}. The basic idea is to iteratively update the filter coefficients to approach the optimum coefficients, using a certain step size at each iteration. The LMS is a stochastic gradient descent approach, meaning that it is adapted based on the current error. It is, however, sensitive to input scaling, making it difficult to find an optimum step size to guarantee convergence. This limitation motivated the development of a variant, the Normalised Least Mean Squares (NLMS) algorithm, which is a variant of LMS that solves this problem by normalising with the power of the input \cite{DAG2013}.

Another popular approach is the Recursive Least Squares (RLS) algorithm, which recursively finds the filter coefficients to minimize a weighted least squares cost function relating to the input signal. This is unlike LMS and NLMS, which aim to reduce the mean squared error. Compared to LMS and NLMS, the RLS exhibits very fast convergence, but at the cost of higher computational complexity.

Generally, adaptive filters are used when there is some quantity to be minimised. For example, an adaptive filter can be implemented iteratively with a time delay to estimate and remove mains noise, which is periodic and thus can be estimated from previous samples. This is not as applicable in typical speech enhancement, which is concerned with reducing random noise.

\subsection{Time-Frequency Domain}\label{TF}

Speech enhancement can be performed in the time-frequency (T-F) domain, which analyses signals in both time and frequency domains simultaneously, using various T-F representations \cite{Cohen94}. Assuming speech is quasi-stationary over sufficiently short periods \cite{Riley1989book}, the noisy input speech signal is divided into overlapping short frames, typically using a Hamming window, where the frame length is a compromise between temporal and frequency resolution \cite{GriffinLim84}. These frames will be called acoustic frames, and are separate from the modulation frames referred to in the modulation domain in Section \ref{ModulationDomain}. Performing the Fourier transform on these frames produces a T-F matrix, on which processing can then be done. This entire process is called the short-time Fourier Transform (STFT).

Generally, T-F enhancement methods apply a gain function to suppress T-F regions which are noise-dominated while preserving speech-dominated regions, typically on the magnitude spectrum only. Computing the gain function depends on the noisy power spectrum, which needs to be estimated separately. After processing, inverse STFT followed by overlap-add reconstruction is performed to produce the enhanced time-domain speech signal. This approach works because speech is relatively sparse, due to limitations of the human ability in terms of speaking and listening i.e. with reasonable levels of noise, the speech can be divided into speech-dominated and noise-dominated regions.

Although this approach can improve the calculated signal-to-noise (SNR) of noisy speech, it can lead to undesired ``musical noise'' artifacts. These appear as isolated spectral components of noise and manifest as brief tones in the enhanced speech, which are generally deemed unnatural and disturbing \cite{Cappe94}. This is because the amplitude of the short-time spectrum exhibits large fluctuations in noisy regions. After processing, the enhanced spectrogram consists of randomly located spectral peaks corresponding to the maxima of the original spectrogram, where the regions between these peaks have been suppressed as they are close to or below the averaged estimated noise spectrum. The result is residual noise comprising of sinusoids of random frequencies between each time frame.

This is used in the setup of the algorithm described in Section \ref{MDKF}.

\subsection{Modulation Domain}\label{ModulationDomain}

Modulation-domain processing starts off similarly to T-F processing, in that the noisy input signal undergoes STFT analysis to produce time-varying frequency components.

For speech enhancement, the amplitudes envelope of each frequency band is regarded as one modulation signal; the spectral amplitudes of each frequency band are windowed into overlapping modulation frames, with a separate modulation frame length and frame overlap compared to the acoustic frame length and overlap of STFT, where each acoustic frame provides one modulation-domain sample for each frequency bin. If each modulation frame contains $M$ samples (i.e. $M$ acoustic frames form one modulation frame) and each acoustic frame contains $N$ time-domain samples, each modulation frame is constructed from $MN$ time-domain samples. The modulation-domain signal has a frequency determined by the acoustic frame increment: since each acoustic frame provides one modulation sample, successive modulation samples are spaced apart by the acoustic frame shift. If the time-domain signal has a sampling frequency of $f_s$ Hz and the acoustic frame shift is $L$ Hz, the modulation-domain sampling frequency is $(f_s/L)$ Hz.

A processing algorithm then estimates the modulation frames of clean speech, which are then overlap-added to reconstruct the modified modulation signals. Combining this with the phase spectrum of the noisy input signal and performing the inverse STFT then produces the enhanced time-domain speech signal.

Even though the acoustic envelope directly contains the speech information, the temporal dynamics of the envelope better represent the information contained in speech \cite{Hermansky98}. These dynamics, which are at significantly lower frequencies than the speech signal itself, are provided in the modulation spectrum, suggesting that working in the modulation domain for speech processing can produce better results. More detailed description is provided in Section \ref{KF}.

\section{Noise Estimation}\label{noiseEstimation}

Noise estimation is an important part of speech processing. In many algorithms including those described in this report, performance is heavily affected by the accuracy of the noise estimation. The algorithms proposed in this project are rely on the estimation of the spectral noise power.

When estimating noise power, because the noise power may change rapidly over time, the spectral estimate has to be updated as often as possible. In speech enhancement, an overestimate or underestimate of the noise power will lead to an over-suppression or under-suppression of the noisy signal, which can lead to excessive residual noise or reduced intelligibility in the enhanced signal.

One way to estimate spectral noise power is to exploit the time periods where speech is absent, which requires detection of speech presence using a voice activity detector (VAD) \cite{VAD}. This method encounters problems when the noise is non-stationary, however, as a sudden increase in noise power may instead be interpreted as the onset of speech. Additionally, if the noise power varies during speech presence, this change can only be detected with some time delay.

To improve noise estimation, methods have been proposed based on minimum statistics (MS) \cite{noiseMS}. The method in \cite{noiseMS} does not use a VAD. Instead, it tracks spectral minima in each frequency band without distinguishing between between speech presence and absence. The power spectrum of the noisy signal is tracked frame-by-frame and observed over a short time period. A general assumption is that within the observed time frame, speech is absent for a non-zero portion of the total time period. The spectral noise power is then obtained from the minimum of the estimated power spectrum of the noisy signal. Similar to VADs, if the noise power rises within the observed time period, it can be tracked with some time delay. Due to the delay, the local noise power estimates tend to be underestimated. This results in residual noise and musical noise artifacts when the noise estimation is used in a speech enhancer.

A more recent method to estimate the noise power spectral density (PSD) is to use a minimum mean-squared-error (MMSE) optimal estimation method \cite{MMSEnoise}, which can be interpreted as a VAD-based noise power estimator \cite{MMSEnoiseBetter}. Generally, MMSE-based estimators are more robust to non-stationary noise and are less computationally intensive as compared to MS-based methods \cite{MMSEnoise}. The estimator in \cite{MMSEnoiseBetter} replaces the hard VAD of the estimator in \cite{MMSEnoise} with a soft speech presence probability (SPP) with a fixed \textit{a priori} SNR as a parameter of the likelihood of speech presence, using a value typical in speech presence. This soft estimation overcomes the issue of random fluctuations in the estimated SNR. This modification automatically makes the estimator unbiased, and it retains similar performance while achieving lower computational complexity compared to \cite{MMSEnoise}.

\section{Spectral Subtraction}\label{specSub}

Spectral subtraction is a widely-used filtering technique which operates in the time-frequency domain. In this method, stationary or slowly-varying noise is attenuated from noisy speech by subtracting the magnitude noise spectrum, estimated during periods where speech is absent \cite{Boll1979}. It is also possible to estimate the noise using a secondary sensor \cite{Widrow1975}. The estimated noise spectrum is then subtracted from the noisy spectrum to produce an approximated spectrum of the clean speech. The spectral error can then be computed and reduced separately. The algorithm can be further refined by incorporating residual noise reduction and non-speech signal attenuation \cite{Boll1979}.

Spectral subtraction works on the back of a few assumptions: firstly, that the background noise is additive to the clean signal \cite{Boll1979}. This assumption means that the complex spectrum of the input noisy signal can be expressed as the sum of the speech spectrum and the noise spectrum. Next, it is assumed that the noise is a stationary or a slowly varying process (locally stationary). This allows the algorithm enough time to accurately formulate an updated estimate for the new noise magnitude spectrum before speech activity starts again. Lastly, the underlying assumption is that noise can be significantly reduced by removing its effect in the magnitude spectrum only i.e. phase spectrum is untouched, and the estimate of the clean speech magnitude spectrum is combined with the phase spectrum of the noisy input signal \cite{ADSP2009}.

As mentioned in Section \ref{TF}, the local stationarity assumption means the processing should be done on small-enough chunks of the input. Therefore, the input must first be split into overlapping frames using overlap-add processing. In the final step after processing, these frames are reassembled to form the continuous output signal.

To avoid signal distortion introduced by data segmentation \cite{Aydin2000}, each frame is first multiplied by a windowing function before performing the Fourier Transform (typically using the Fast Fourier Transform or FFT). The output signal is then formed by the sum of these overlapping windowed frames. After processing, when the signal is being reassembled, the window is applied again.

For the signal to remain undistorted, multiplying by these windows should not change its magnitude. To achieve this, particular overlap factor/window pairs must be used; for example, if a Hamming window is chosen, applying the window twice requires that the overlapped windows approximately sum to unity for an overlap factor of 4 i.e. each windowed frame overlaps each of its neighbours by 50\%, ensuring the output signal remains undistorted.

Spectral subtraction is popular largely because it is simple and easy to implement, requiring mainly the forward and inverse Fourier Transforms. However, this comes at a cost to performance. Subtracting the noise spectrum from the noisy input spectrum introduces distortion in the signal, known as musical noise \cite{Loizou2007}, as mentioned in Section \ref{TF}. Variations have been developed in attempts to mitigate this. A common variation involves over-subtraction and a noise floor. This method involves an over-subtraction factor, whereby an overestimate of the noise power spectrum is subtracted from that of the input, and using a noise spectral floor, which prevents the processed spectrum from going below a preset minimum value, to control both the amount of residual noise and musical noise \cite{Berouti1979}. However, it is generally evaluated that these modifications improve speech quality further but do not significantly affect the intelligibility of the input signals \cite{Loizou2007}.

% \section{MMSE Speech Enhancement}

\section{Ideal Binary Mask}\label{IBM}

Sound is generated by acoustic sources, and these sources are typically complex, containing multiple frequency components. In a typical environment, multiple acoustic sources are simultaneously active, including undesired background noise, and a listener's ear will pick up only the sum of all these sources. There are various types of corrupting background noise, including but not limited to acoustic noise (e.g. vehicle vibration), speech-shaped noise, industrial noise and multi-talker babble (e.g. noisy cafeteria with other speakers) \cite{Kozou2005}. For the listener to distinguish between the different sounds in the incoming mix, such as picking out a particular speaker in a busy supermarket, the incoming audio signal has to be partitioned and categorised accurately into individual sounds.
 
Human beings have auditory systems that are remarkably capable at doing this, and are thus able to understand speech in many of these noisy conditions. The signal separation process, known as auditory scene analysis, is typically performed in two stages, to understand the message spoken by the target speaker. Firstly, the input sound is decomposed into a matrix of time-frequency (T-F) units, where each unit represents the signal occurring at a particular instance in time with a particular frequency component. These T-F units are then analysed, and the auditory system utilises a combination of cues, learned patterns and other prior knowledge about the target to pick out the T-F units of the target signal, and group these individual units into a single recognisable ``image'' of the desired signal \cite{Bregman1990}. Essentially, the auditory system employs an analysis-synthesis strategy to organise the input into separate streams corresponding to different audio sources.

To model the human auditory system, computational auditory scene analysis (CASA) was proposed to approach sound separation in two stages: segregation and grouping \cite{WangBrown2006}. The aim of using these CASA techniques was to pick out the target signal from the noisy mix, and the computational method of choice was the ideal T-F binary mask \cite{Wang2005}.

The ideal binary mask (IBM) is defined in the T-F domain as a matrix of binary numbers, and is constructed by comparing the local signal-to-noise ratio (SNR), defined as the difference between the target signal energy and the noise energy, in each T-F unit against a threshold known as a local criterion (LC). In the IBM, the T-F units with local SNR exceeding the LC (in decibels) are assigned $1$, and $0$ otherwise. If a $0$ dB SNR threshold is used to generate the mask, a T-F unit being assigned $1$ indicates that the energy of the target signal is stronger than that of the interference (masker) within that particular T-F unit, which is a particularly intuitive implementation. Let $T(t,f)$ and $M(t,f)$ denote the target and masker signal power measured in dB respectively, at time $t$ and frequency $f$; the IBM is then defined as

\begin{equation}\label{eq:IBM}
IBM(t,f) = 
\begin{cases}
    1 & \text{if $T(t,f)-M(t,f)>LC$} \\
    0 & \text{otherwise}
\end{cases}
\end{equation}

This mask can then be applied to the T-F representation of the incoming noisy signal; it acts as a selective filter, allowing some parts of the signal to pass through (those T-F units assigned to $1$) while eliminating other parts (those assigned to $0$). This means that at each T-F unit, the IBM either retains target energy or discards interference energy. The IBM therefore offers an indication of the T-F areas of audible target speech, and offers significant improvements in intelligibility \cite{Brungart2006}. 

\begin{figure}[ht!]
\centering
\includegraphics[width=0.75\linewidth]{IBM_example1}
\captionof{figure}{Top to bottom: clean speech, noisy speech, IBM and IBM-processed speech}
\label{fig:IBM_eg1}
\end{figure}

An example of the IBM at work is shown in Figure \ref{fig:IBM_eg1}, with a clean sentence obtained from the TIMIT database \cite{TIMIT}. From top to bottom, the spectrograms shown are that of: a) clean speech; b) clean speech corrupted with white Gaussian noise at $5$ dB SNR; c) IBM constructed using LC threshold of $0$ dB, where black pixels denote $1$ (target stronger than interference masker) and white pixels denote $0$ (target weaker than masker); d) segregated mixture obtained with the $0$ dB LC IBM, obtained by multiplying the spectrograms in (b) and (c), one T-F unit at a time.

The $0$ dB LC IBM, a particularly simple and intuitive comparison, is theoretically optimal in terms of SNR gain (\cite{HuWang2004}, \cite{Ellis2006}); Figure \ref{fig:IBM_eg1} shows its good performance, whereby the spectrogram of the processed speech is nearly identical to that of clean speech. It was later shown that while it is not optimal due to certain constraints, it performs almost as well as the proposed alternative, and is in fact more practical for real-world implementation \cite{LiWang2009}. Multiple studies have examined further the effects of the LC, input SNR level and masker type on the performance of the IBM. For example, a technique called ideal T-F segregation (ITFS) has been effective in making use of the IBM to improve the intelligibility of human speech masked by competing voices \cite{Brungart2006}. It is argued that the ITFS removes informational masking caused by the IBM-eliminated T-F units with large masker energy, where informational masking refers to the inability to accurately distinguish the target signal from the noisy mixture.

To demonstrate the benefits of IBM processing, various studies carried out intelligibility tests, in which listeners listen to a set of IBM-processed sentences and write down the words they hear; results produced are in terms of the percentage of words identified correctly.

A typical IBM intelligibility test result is shown in Figure \ref{fig:IBM_int}, where UN represents the unprocessed noisy speech (replicated from \cite{LiLoizou2008}). Here, STFT was used to process the input noisy signal, using multitalker babble as the masker. As shown, the performance peaks out between roughly $-20$ dB and $5$ dB for an input SNR of $-5$ dB, with a slightly smaller range for an input SNR of $-10$ dB.

\begin{figure}[ht!]
\centering
\includegraphics[width=0.64\linewidth]{IBM_intelligibilityTest}
\captionof{figure}{Performance (percentage of words identified accurately) as a function of LC (dB) for two input SNR levels, masked in multitalker babble (replicated from \cite{LiLoizou2008})}
\label{fig:IBM_int}
\end{figure}

Large intelligibility gains were demonstrated in \cite{LiLoizou2008}, but the range of LC values for near-perfect intelligibility (performance plateaus of $\approx100$\% accuracy) were different to that in \cite{Brungart2006}. Attributing this to differences in the setup and signals used, it was suggested that the pattern of the IBM was the critical factor for intelligibility, rather than the local SNR of individual T-F units \cite{LiLoizou2008}.

The significant improvements to intelligibility made IBM a notable candidate for speech enhancement applications such as hearing aids, provided the IBM could be approximated to a high degree of accuracy. However, to apply it, it is important to understand how IBM enhances intelligibility. In \cite{Brungart2006}, it is argued that the IBM suppresses informational masking by directing the listener's attention to the T-F units containing target information i.e. \emph{where} the target signal is, in a T-F auditory space \cite{LiLoizou2008}. This led to the conclusion that listeners need not extract specific knowledge from individual T-F units, but rather the overall pattern of the IBM, i.e. pattern of target-dominated and masker-dominated T-F units, was the most important factor for intelligibility, which was also concluded in \cite{LiLoizou2008}. However, this interpretation is limited to the range of LCs where the IBM pattern represents the T-F units that are audible to normal human listeners i.e. LCs close to 0 dB \cite{KBP10}.

An alternative ideal mask definition was proposed in \cite{Anzalone2006}, which also produced large intelligibility improvements. This alternative mask was named the target binary mask (TBM), as the mask was calculated based on the target signal only. TBM depends on the long-term spectrum of the target speaker, and compares with an average spectrum i.e. a time-invariant threshold. The mask pattern naturally resembles the target signal and is unaffected by the masker specifically. Instead, the TBM generated in this manner can be applied to a mixture of the target signal and a different masker. On the other hand, the IBM pattern depends on the masking signal; IBM compares with the actual noise in the T-F units, which is time-dependent.

In certain applications, it may be easier to estimate the TBM than the IBM, and so it was of interest to investigate the intelligibility performance of the TBM: it was shown that the TBM has comparable performance to the IBM \cite{KBP09}. A noise-robust method based on target sound estimation to estimate the TBM was proposed in \cite{KLPB10}.

\subsection{Musical Noise}

The largest calculated SNR gain is achieved by using a fully-binary mask of $1$s and $0$s. However, doing so generally degrades the quality of the enhanced speech due to the introduction of musical noise, which refers to random, short tone-like bursts that, in some situations, can be more bothersome than the original noise. In \cite{StokesIBM}, the mask values were instead $1$ and $0.1$: if the mask indicates that speech is present, the gain is $1$, otherwise the gain is set to $0.1$ instead of cutting completely. By reducing the severity of the mask switching, the amount of musical noise artifacts introduced by the mask was reduced overall.

\subsection{Practical Considerations}

By definition, the IBM depends on oracle knowledge, as the mask is constructed based on the target and interfering signals before mixing. In a real-world situation, the target signal is of course unavailable, meaning the IBM has to be estimated from noisy data only. In the presence of significant noise, this can be a difficult task, and it is impossible to compute the IBM for all T-F units with complete accuracy. The effect of overall binary mask estimation error was investigated further in \cite{LiLoizou2008}, and it was demonstrated that the estimation needs to be very accurate overall. As an example, $>90\%$ accuracy is required to estimate the IBM for the case of $-5$ dB input masked with multitalker babble to yield significant gains in intelligibility.

While it is of interest to further investigate the effects of estimation uncertainty and error on speech intelligibility improvements, this project focuses on the Kalman filter algorithm, and assumes that an ideal or estimated binary mask has already been computed and is available.

\section{Linear Prediction Analysis}\label{LPC}

In linear prediction, future values of a signal are estimated as a linear combination of previous samples. In speech enhancement, the temporal variation of a speech signal is modelled using a linear predictive model. Equation \ref{eq:general_speechLinearPredictor} shows a $p$-th order all-pole linear predictor (autoregressive model), where the current state $\hat{s}(n)$ is estimated as a linear combination of $p$ previous states \cite{LPCanalysis}.

\begin{equation}\label{eq:general_speechLinearPredictor}
\hat{s}(n)=-\sum_{k=1}^{p}a_k s(n-k)
\end{equation}

where $s(n)$ is a speech sequence that is windowed to ensure the quasi-stationarity of the speech. In speech enhancement, obtaining an accurate model of the speech is useful for noise attenuation. This entails obtaining accurate estimates of the coefficients $a_k$ in Equation \ref{eq:general_speechLinearPredictor}, known collectively as the linear prediction coefficients (LPCs). This method is known as linear prediction analysis, where the goal is to obtain estimates of $a_k$ that minimise the mean squared error \cite{LPCanalysis}.

The prediction error (prediction residual) is computed as the difference between the actual sample $s(n)$ and the predicted sample $\hat{s}(n)$:

\begin{equation}\label{eq:LPC_predictionerror}
e(n)=s(n)-\hat{s}(n)=s(n)+\sum_{k=1}^{p}a_k s(n-k)
\end{equation}

A popular method to minimise the mean squared error is the least squares autocorrelation method, achieved by minimising the total prediction error $E$ over all samples:

\begin{equation}\label{eq:LPC_totalpredictionerror}
E=\sum_{n}e^2(n)=\sum_{n}[s(n)+\sum_{k=1}^{p}a_k s(n-k)]^2
\end{equation}

The coefficients $a_k$ that minimise Equation \ref{eq:LPC_totalpredictionerror} can be found by taking the derivative of $E$ with respect to $a_k$ and setting it to $0$, thus solving:

\begin{equation}\label{eq:derivativeE}
\frac{\partial E}{\partial a_i}=0, \quad i=1,2,...,p
\end{equation}

From Equations \ref{eq:LPC_totalpredictionerror} and \ref{eq:derivativeE}, we have

\begin{equation}\label{eq:differentialExpression}
\sum_{n}s(n-i)s(n)=\sum_{k=1}^{p}a_k \sum_{n}s(n-i)s(n-k), \quad i=1,2,...,p
\end{equation}

which produces a set of $p$ linear equations and $p$ unknowns which can be solved for the LPC coefficients $\{a_k, k=1,2,...,p\}$ that minimise $E$ in Equation \ref{eq:LPC_totalpredictionerror}.

In the autocorrelation method, the error is assumed to be minimised over the infinite duration $-\infty<n<\infty$ \cite{LPCanalysis}. In practice, there is typically a finite interval of interest: $0<n<N-1$. Equation \ref{eq:differentialExpression} can then be written as:

\begin{equation}\label{eq:diffEqnAutoCorr}
R(i)=\sum_{k=1}^{p}a_kR(i-k), \quad i=1,2,...,p
\end{equation}

where

\begin{equation}\label{eq:autoCorr}
R(i)=\sum_{n=i}^{N-1}s(n-i)s(n)
\end{equation}

is the autocorrelation function of the signal $s(n)$. The coefficients $R(i-k)$ form the autocorrelation matrix $\textbf{R}$, which is a symmetric Toeplitz matrix \cite{LPCanalysis} since the autocorrelation function is even i.e. $R(i)=R(-i)$. In matrix notation, Equation \ref{eq:diffEqnAutoCorr} can be written as:

\begin{equation}\label{eq:autoCorrMatrix}
\textbf{R}\textbf{A}=-\textbf{r}
\end{equation}

where \textbf{A} contains the desired LPC coefficients $\{a_k, k=1,2,...,p\}$. These coefficients can then obtained by solving the matrix inverse equation:

\begin{equation}\label{eq:matrixgetLPCs}
\textbf{A}=-\textbf{R}^{-1}\textbf{r}
\end{equation}

Finally, the prediction residual (Equation \ref{eq:LPC_predictionerror}) can be obtained by inverse filtering the noisy speech signal using the computed LPCs \cite{LPCresidual}. 

LPC analysis is based on the source-filter model of speech production, which models speech as a combination of a source of sound (vocal cords) and a linear filter (vocal tract)  \cite{VowelsBook}. This assumes that speech is produced by a buzzer at the end of a tube, which is a close approximation of real speech production \cite{PhoneticsBook}. The glottis, which produces the buzz, is the opening between the vocal folds; as the vocal folds vibrate, a ``buzzing'' sound is produced, which is what we term ``pronunciation'' \cite{PhonologyBook}. This buzz is characterised by its intensity (loudness) and frequency (pitch). The vocal tract, comprising the throat and mouth, forms the tube, which is characterised by its resonances, which produce formants (enhanced frequency harmonics) in the speech. The vocal tract transfer function can be modelled by an all-pole filter, which we use to estimate the current state of speech as a linear function of previous states.

In this project, LPC analysis is performed on modulation frames in the modulation domain, using the autocorrelation method described in \cite{LPCanalysis}. The prediction residual is used to calculate the excitation variance in the algorithm described in Section \ref{MDKF}, which is the basis of the algorithms discussed in this report.

\section{Kalman Filter}\label{KF}

The Kalman filter \cite{Kalman1960} is a recursive optimal data processing algorithm. Under certain assumptions, it is optimal with respect to any practical measure. This is because the Kalman filter (KF) makes use of all data available to it, processing all available information to estimate the current value of the desired variables. In the context of speech enhancement, speech signals are modelled as autoregressive processes using the state space method, where the processed speech is recursively estimated, one sample at a time \cite{WuChen1998}.

The filter has a recursive ``predictor-corrector'' structure \cite{Maybeck1979}; firstly, a prediction of the desired variable at the next measurement time is made, based on all previously available data, producing a prediction value and its associated uncertainty. When the next measurement is actually taken, the difference between the measurement and the predicted value is used to ``correct'' the prediction, to produce the new estimate. Note that this recorded measurement comes with its associated uncertainty, arising from imperfections of measuring instruments. The new estimate is thus updated using a linear combination of the prediction and the measurement, with more weight given to estimates with lower uncertainty.

The KF was initially proposed for speech enhancement by Paliwal and Basu in 1987 \cite{PaliwalBasu1987}, where excellent noise reduction was achieved when linear prediction coefficients (LPCs) were estimated from clean speech. The KF is of particular interest for speech enhancement, as the speech model is inbuilt into the KF recursion equations, and the enhanced speech contains little to no musical noise, assuming clean LPCs are available \cite{MBG2006}; the performance of the KF is highly dependent on the accurate estimation of LPCs. However, for practical use, these parameters have to be estimated from noisy speech since the clean speech is not known \textit{a priori}, causing a significant drop in performance. Better performance has been demonstrated in variations of the original KF algorithm, such as a cascaded estimator/encoder structure which improves LPC estimates \cite{Gibson1991}.

In recent years, the focus has shifted away from the traditional KF methods which utilise the acoustic domain, defined as the short-time Fourier Transform (STFT) of the signal. Instead, there has been growing interest in the modulation domain, defined as the variation over time of the magnitude spectrum at all acoustic frequencies \cite{Atlas2003}. Studies have increasingly shown the importance of the modulation domain for speech analysis; for example, very low frequency modulations of sound have been shown to be the fundamental carriers of information in speech \cite{Atlas2003}, due to physiological limitations on how rapidly the vocal tract is able to change with time \cite{PWS2010}. The slowly-varying modulation domain hence represents how the vocal tract changes over time \cite{SP11}.

The KF is capable of handling non-stationary signals as well as estimating both magnitude and phase spectra \cite{Li2006Phd}, which puts it at an advantage over STFT-based, acoustic domain-based methods for speech processing, as phase information has been shown to be more important in the modulation domain than in the acoustic domain \cite{GreenBergArai2001}. It was also noted in \cite{SP11} that the low order linear predictor KF was more appropriate for enhancing slower-varying modulating signals than for enhancing time-domain speech, as the time-domain signals contain long-term correlation which the low order linear predictor cannot capture. This is important for the KF, as its optimality works on the basis of incorporating and using all data available to the algorithm. These results suggest the use of the KF in the modulation domain as an improved method of speech enhancement \cite{SP11}.

\subsection{Modulation-domain Kalman filter}\label{MDKF}

The modulation-domain KF (MDKF) is an adaptive minimum mean-squared error (MMSE) estimator that uses the statistics of time-varying changes in the magnitude spectrum of both speech and noise \cite{SP11}. In the MDKF, an analysis-modification-synthesis (AMS) framework is used to obtain the modulation domain in three steps. In the analysis stage, the input speech signal is processed using STFT; next, the noisy input spectrum undergoes some modification or processing; and lastly, the output processed signal is synthesised by inverse STFT followed by the overlap-add method. 

\subsubsection{Analysis-modification-synthesis framework in the acoustic domain}

Considering an additive noise model, where $y(n)$, $x(n)$ and $v(n)$ represent zero-mean signals of noisy speech, clean speech and noise respectively:

\begin{equation}\label{eq:additive_noise_time}
y(n)=x(n)+v(n)
\end{equation}

Assuming speech is quasi-stationary means that it can be analysed in frames using the STFT (analysis), thus obtaining the STFT of the noisy signal $y(n)$:

\begin{equation}\label{eq:noisy_STFT_conv}
Y(n,k)=\sum_{l=-\infty}^{\infty}y(l)w(n-l)e^{-j\frac{2\pi kl}{N}}
\end{equation}

which can be represented using STFT analysis as Equation \ref{eq:additive_noise_STFT}:

\begin{equation}\label{eq:additive_noise_STFT}
Y(n,k)=X(n,k)+V(n,k)
\end{equation}

where $Y(n,k)$, $X(n,k)$ and $V(n,k)$ denote the STFTs of noisy speech, clean speech and noise respectively and $k$ refers to the discrete acoustic frequency index, $N$ is the acoustic frame duration in number of samples and $w(n)$ is a window analysis function. For speech enhancement, a Hamming window is typically used. Note that this model is noise-additive in the complex STFT domain.

Each one of $Y(n,k)$, $X(n,k)$ and $V(n,k)$ is a complex spectrum, and can be expressed in terms of their acoustic magnitude and acoustic phase spectra. For example, $Y(n,k)$ can be represented as:

\begin{equation}\label{eq:noisy_STFT_complex}
Y(n,k)=|Y(n,k)|e^{j\angle Y(n,k)}
\end{equation}

where $|Y(n,k)|$ is the acoustic magnitude spectrum and $\angle Y(n,k)$ is the acoustic phase spectrum.

Traditionally, AMS-based methods only modify the noisy acoustic magnitude spectrum $|Y(n,k)|$ to obtain a processed magnitude spectrum $|\hat{X}(n,k)|$; the modified spectrum is thus obtained by combining the enhanced magnitude spectrum with the original noisy phase spectrum $\angle Y(n,k)$:

\begin{equation}\label{eq:AMS_STFT}
\hat{X}(n,k)=|\hat{X}(n,k)|e^{j\angle Y(n,k)}
\end{equation}

The enhanced speech $\hat{x}(n)$ is then reconstructed by performing the inverse STFT of the enhanced acoustic spectrum $\hat{X}(n,k)$ followed by synthesis windowing and overlap-add \cite{Quatieri2002}.

\subsubsection{Kalman filter model in the modulation domain}

As briefly introduced in Section \ref{ModulationDomain}, in the modulation domain, the acoustic magnitude spectrum of noisy speech is interpreted as a series of modulating signals spanning across time, where each modulating signal $|Y(n,k)|$ represents the variation of one frequency component over time, with $k=1,2,...,N$ where $N$ is the number of frequency bins. Each modulating signal is individually processed with a separate KF \cite{SP11}.

To visualise this, imagine that a time-domain noisy speech signal sampled at $8$ kHz is windowed with a $64$ ms frame (window) length and $4$ ms frame shift. Taking the STFT, each window is analysed individually: the samples within a $64$ ms window are viewed as a frequency-domain signal with (for example) $256$ frequency bins. When the next window is taken (original window shifted by $4$ ms), the samples are again analysed into a set of $256$ frequency bins. Doing this for the entire signal produces $256$ time-varying signals (modulating signals), one for each frequency component and processed with its own KF, where the samples in each signal are $4$ ms apart. Within each KF, the modulating signal is further windowed, but the signal now has a much lower frequency: in this case, $\frac{1}{0.004}=250$ Hz. Assuming a modulating window of $64$ ms, each window only contains $\frac{64}{4}=16$ samples, compared to $512$ samples for a $64$ ms window of a $8$ kHz time-domain signal.

Returning to the model, an additive noise model is assumed for each modulating signal, assuming white Gaussian noise (Equation \ref{eq:MDKF_additive_noise}). Recall that the noisy phase spectrum is left untouched.

\begin{equation}\label{eq:MDKF_additive_noise}
|Y(n,k)|=|X(n,k)|+|V(n,k)|
\end{equation}

In the KF autoregressive model, a $p$-order linear predictor is used to model the evolution of speech over time (Equation \ref{eq:KF_linear_predictor_speech}), where ${a_{j,k}; j=1,2,...,p}$ are the LPCs and $W(n,k)$ is a random white excitation with a variance of $\sigma^2_{W(k)}$.

\begin{equation}\label{eq:KF_linear_predictor_speech}
|X(n,k)|=-\sum_{j=1}^{p}a_{j,k}|X(n-j,k)|+W(n,k)
\end{equation}

Including the noise signal, the overall state space representation for noisy speech can be written as:

\begin{equation}\label{eq:KF_X_statespace}
\textbf{X}(n,k)=\textbf{A}(k)\textbf{X}(n-1,k)+\textbf{d}W(n,k)
\end{equation}

\begin{equation}\label{eq:KF_Y_statespace}
|Y(n,k)|=\textbf{d}^{T}\textbf{X}(n,k)+|V(n,k)|
\end{equation}

where $\textbf{X}(n,k)=[|X(n,k)|,|X(n-1,k)|,...|X(n-p+1,k)|]^T$ is the clean speech modulation state vector, $\textbf{d}=[1,0,...,0]^T$ is the measurement vector for both the excitation noise $W(n,k)$ and observation, and $\textbf{A}(k)$ is the state transition matrix utilising the LPCs:

\begin{equation}\label{eq:KF_statetransition}
\textbf{A}(k)=
\begin{bmatrix}
    -a_{1,k} & -a_{2,k} & \dots  & -a_{p-1,k} & -a_{p,k} \\
    1        & 0        & \dots  & 0          & 0        \\
    0        & 1        & \dots  & 0          & 0        \\
    \vdots   & \vdots   & \ddots & \vdots     & \vdots   \\
    0        & 0        & \dots  & 1          & 0        \\
\end{bmatrix}
\end{equation}

The Kalman filter recursively calculates a linear unbiased MMSE estimate $\hat{\textbf{X}}(n|n,k)$ of the $k$-th modulation state vector at time $n$, given the noisy modulating signal up to and including time $n$ (i.e. $|Y(1,k)|,|Y(2,k)|,...|Y(n,k)|$) using the following equations:

\begin{equation}\label{eq:MDKF_predictP}
\textbf{P}(n|n-1,k)=\textbf{A}(k)\textbf{P}(n-1|n-1,k)\textbf{A}(k)^T+\sigma^2_{W(k)}\textbf{d}\textbf{d}^T
\end{equation}

\begin{equation}\label{eq:MDKF_predictXhat}
\hat{\textbf{X}}(n|n-1,k)=\textbf{A}(k)\hat{\textbf{X}}(n-1|n-1,k)
\end{equation}

\begin{equation}\label{eq:MDKF_updateK}
\textbf{K}(n,k)=\textbf{P}(n|n-1,k)\textbf{d}[\sigma^2_{V(k)}+\textbf{d}^T\textbf{P}(n|n-1,k)\textbf{d}]^{-1}
\end{equation}

\begin{equation}\label{eq:MDKF_updateP}
\textbf{P}(n|n,k)=[\textbf{I}-\textbf{K}(n,k)\textbf{d}^T]\textbf{P}(n|n-1,k)
\end{equation}

\begin{equation}\label{eq:MDKF_updateXhat}
\hat{\textbf{X}}(n|n,k)=\hat{\textbf{X}}(n|n-1,k)+\textbf{K}(n,k)[|Y(n,k)|-\textbf{d}^T\hat{\textbf{X}}(n|n-1,k)]
\end{equation}

where $\sigma^2_{V(k)}$ is the variance of the corrupting noise and $\textbf{P}(n|n,k)$ is the error covariance matrix. These equations can be categorised into two main steps: prediction and updating. Equations \ref{eq:MDKF_predictP} and \ref{eq:MDKF_predictXhat} predict the error covariance and state based on past samples respectively, while the other equations update the Kalman gain, error covariance and state based on the predicted values.

In particular, Equation \ref{eq:MDKF_updateXhat} is the main updating step, whereby a linear combination of the estimate based on previous samples $|\hat{X}(n|n-1,k)|$ and the current measurement $|Y(n,k)|$ is used to compute the current estimate $|\hat{X}(n|n,k)|$. To view this more clearly, we can rewrite Equation \ref{eq:MDKF_updateXhat} as:

\begin{equation}\label{eq:MDKF_updateXhat_linearCombi}
\hat{\textbf{X}}(n|n,k)=[\textbf{I}-\textbf{K}(n,k)\textbf{d}^T]\hat{\textbf{X}}(n|n-1,k)+\textbf{K}(n,k)|Y(n,k)|
\end{equation}

The accuracy of the weighted sum producing the updated state is critical in determining the correctness of the algorithm. Hence, it is imperative that the noise power is estimated as accurately as possible. There are a number of ways to do so, as discussed in Section \ref{noiseEstimation}.

As the algorithm is running, each modulating signal $|Y(n,k)|$ is windowed into modulation frames, and the LPCs and excitation variance $\sigma^2_{W(k)}$ are estimated. Within each frame, the LPCs are kept constant, whereas the Kalman gain $\textbf{K}(n,k)$, error covariance matrix $\textbf{P}(n|n,k)$ and estimated state vector $\hat{\textbf{X}}(n|n,k)$ are updated every sample, regardless of frame.

\subsection{Comparison with time-domain Kalman filter}

For comparison purposes, the time-domain Kalman filter (TDKF) equations are shown below:

\begin{equation}\label{eq:TDKF_predictP}
\textbf{P}(n|n-1)=\textbf{A}\textbf{P}(n-1|n-1)\textbf{A}^T+\sigma^2_{w}\textbf{d}\textbf{d}^T
\end{equation}

\begin{equation}\label{eq:TDKF_predictXhat}
\hat{\textbf{x}}(n|n-1)=\textbf{A}\hat{\textbf{x}}(n-1|n-1)
\end{equation}

\begin{equation}\label{eq:TDKF_updateK}
\textbf{K}(n)=\textbf{P}(n|n-1)\textbf{d}[\sigma^2_{v}+\textbf{d}^T\textbf{P}(n|n-1)\textbf{d}]^{-1}
\end{equation}

\begin{equation}\label{eq:TDKF_updateP}
\textbf{P}(n|n)=[\textbf{I}-\textbf{K}(n)\textbf{d}^T]\textbf{P}(n|n-1)
\end{equation}

\begin{equation}\label{eq:TDKF_updateXhat}
\hat{\textbf{x}}(n|n)=\hat{\textbf{x}}(n|n-1)+\textbf{K}(n)[y(n)-\textbf{d}^T\hat{\textbf{x}}(n|n-1)]
\end{equation}

where $\hat{\textbf{x}}(n|n-1)$ and $\hat{\textbf{x}}(n|n)$ are the \textit{a priori} (predicted) and \textit{a posteriori} (updated) state vectors respectively, $\textbf{P}(n|n-1)$ and $\textbf{P}(n|n)$ are the \textit{a priori} and \textit{a posteriori} error covariance matrices respectively, $\textbf{K}(n)$ is the Kalman gain, and $\sigma^2_{v}$, $\sigma^2_{w}$ are the noise and excitation variances respectively.

Figure \ref{fig:compareTDKF_MDKF} compares the spectrograms of TDKF (order $18$) and MDKF (order $2$) applied on speech from the TIMIT database \cite{TIMIT}, corrupted by white Gaussian noise at $5$ dB SNR, and sampled at $16$ kHz. For the purposes of comparing performance limits, clean speech LPCs were used in the filters. Generally, both algorithms perform well in removing noise, especially when speech is absent. However, there is visibly some noise in the TDKF-enhanced speech; particularly, frequency components above $1.8$ kHz have been noticeably degraded by noise. A listening test confirmed this, detecting the presence of high-frequency artifacts.

\begin{figure}[ht!]
\centering
\includegraphics[width=1\linewidth]{compareTDKF_MDKF}
\captionof{figure}{Top row: clean speech (left), speech corrupted with white Gaussian noise (right); bottom row: TDKF-enhanced speech (left), MDKF-enhanced speech (right)}
\label{fig:compareTDKF_MDKF}
\end{figure}

This TDKF noise is a limitation of using the KF for speech enhancement. Similarly to how we rearranged Equation \ref{eq:MDKF_updateXhat}, Equation \ref{eq:TDKF_updateXhat} can be rewritten to show that the enhanced output is a weighted combination of the estimated speech and measured speech, where the relative weight depends on the Kalman gain $\textbf{K}(n)$. When speech is absent, $\textbf{P}(n|n-1)=\textbf{0}$, meaning that $\textbf{K}(n)=\textbf{0}$ and the estimated state is equal to the predicted state, being unaffected by the noisy measurement.

When speech is present, however, the algorithm does not work quite so perfectly. Using a low model order means that the TDKF linear predictor cannot fully replicate the harmonic structure of speech, which requires autocorrelation lags in the order of the number of samples in a pitch period \cite{SP11}. The prediction thus has unvoiced and noise-like characteristics, and the result is that the updated output only preserves the speech component below $1.8$ kHz \cite{SP11}. The resultant noise will be especially prevalent in regions of low SNR, where the prediction is weighted more heavily due to Equation \ref{eq:TDKF_updateK} producing a smaller $\textbf{K}(n)$.

MDKF has an inherent advantage over the TDKF due to the linear predictor model used. Compared to the TDKF, the MDKF models the temporal variation of the acoustic magnitude spectrum of speech, which represents the changes in the vocal tract over time. This is more comprehensive since the low-order MDKF linear predictors are able to model the modulating signal dynamics, due to physiological limitations of how quickly the vocal tract can change \cite{PWS2010}. This also better represents speech information overall, as it has been shown that low-frequency modulations of sound are the fundamental carriers of speech information \cite{Atlas2003}.

\subsection{Performance of MDKF}

Overall, experimental results from the TIMIT corpus (Figure \ref{fig:compareTDKF_MDKF}) show that under ideal conditions where clean speech LPCs can be obtained accurately, the linear predictor is sufficient to model the modulating signals of clean speech. As described earlier, the vocal tract tends to change slowly due to physiological constraints, and thus low LPC orders ($p=2$) were found to be sufficient. Using this, the MDKF was by far the best performing algorithm, doing better than all acoustic and time-domain methods tested, including the TDKF, for both white and coloured noise \cite{SP11}. This was despite both algorithms having access to clean speech LPCs.

However, clean speech is not available in reality; the presence of noise generally degrades the LPC estimates, worsening the performance of the MDKF algorithm. In \cite{SP11}, a practical MDKF algorithm was evaluated, which used an acoustic-domain pre-processor for LPC estimation to reduce the effect of noise degradation.

\section{Speech Quality}

The perceived overall speech quality is how ``good'' the quality of the speech is. The definition of ``good'' is typically left to the listener, who then gives a score to the speech. Methods to assess speech quality can be grouped into subjective and objective measures.

\subsection{Subjective Speech Quality Measures}

Subjective quality measures typically compare the original and processed speech by a listener or a group of listeners. The listeners subjectively rate or rank the speech according to a predetermined scale. Since every listener is unique, their ratings will vary; this variation in results can be reduced by averaging the scores from a group of listeners.

\subsubsection{Mean Opinion Score}

A widely used subjective quality measure is the Mean Opinion Score (MOS) \cite{ITU830}. Each listener gives a numeric MOS score, typically in the range $1-5$, where $1$ is the lowest perceived quality and $5$ is the highest perceived quality. The ``Absolute Category Rating'' scale is commonly used, as shown in Table \ref{table:ACR_MOS} \cite{ITU800}. The overall score is obtained by averaging the ratings from all listeners, representing an overall perceived quality of the speech. With a large number of speech files, this test can be costly and time consuming.

\begin{table}[ht!]
\centering
\begin{tabular}{|c|c|}
\hline
\textbf{Rating} & \textbf{Label} \\ \hline
1               & Excellent      \\ \hline
2               & Good           \\ \hline
3               & Fair           \\ \hline
4               & Poor           \\ \hline
5               & Bad            \\ \hline
\end{tabular}
\caption{Categories of MOS: Absolute Category Rating}
\label{table:ACR_MOS}
\end{table}

\subsection{Objective Speech Quality Measures}

On the other hand, objective speech quality use physical measurements and some calculated values from these measurements. Typically, these calculations compare objective measurements for the reference clean speech and the distorted speech.

Many of the objective measures are highly correlated with subjective measures; it is thus common for a test to use objective measures to estimate subjective methods, which are usually more time-consuming and costly as they involve human listeners. However, as noted previously, there are situations in which high objective scores do not produce high subjective scores and vice versa.

\subsubsection{SNR}

Signal-to-Noise Ratio (SNR) is one of the oldest and most widely used objective quality methods. It has low computational complexity, but requires both clean and distorted speech. The classic formula is calculated (in dB) as:

\begin{equation}\label{eq:SNR}
SNR=10\log_{10}\frac{\sum\limits_{n=1}^{N}x^2(n)}{\sum\limits_{n=1}^{N}\{x(n)-\hat{x}(n)\}^2}
\end{equation}

where $x(n)$ is the clean speech, $\hat{x}(n)$ is the distorted speech and $N$ is the number of time-domain samples.

However, this formula does not represent actual speech quality well as it averages over the entire signal even though speech is non-stationary. Speech energy fluctuates over time, and this formula is dominated by parts where speech energy is large and noise energy is small, which is not representative of the entire signal.

Modifications have thus been proposed. To better represent the temporal variation of speech, segmental SNR (segSNR) was proposed to calculate SNR in short frames and take the average:

\begin{equation}\label{eq:segSNR}
SNR_{seg}=\frac{10}{M}\sum_{m=0}^{M-1}\log_{10}\frac{\sum\limits_{n=Lm}^{Lm+L-1}x^2(n)}{\sum\limits_{n=Lm}^{Lm+L-1}\{x(n)-\hat{x}(n)\}^2}
\end{equation}

where $L$ is the frame length in number of samples and $M$ is the number of frames in the signal ($N=ML$). The logarithm of the ratio is computed before averaging, which means that frames with unusually large ratios are weighted less while frames with lower ratios are weighted more. This matches the perceptual quality better, ensuring that frames with large speech and low noise do not unfairly dominate the overall ratio.

However, if the speech contains too much silence, the overall segSNR value decreases significantly as silent frames usually give large negative segSNR values. In this case, silent frames should be excluded from the averaging by using speech activity detectors. Similarly, frames which exhibit excessively large or small speech values should also be excluded. These modifications give segSNR values that match the subjective quality better. As a result, segSNR often has upper and lower bounds of $35$ dB and $-10$ dB respectively \cite{HP1998}.

A separate variation of SNR is the frequency-weighed SNR (fwSNRseg), which weights the contribution of the different frequency bands. The fwSNRseg can be defined as:

\begin{equation}\label{eq:fwSNRseg}
fwSNR_{seg}=\frac{10}{M}\sum_{m=0}^{M-1}\frac{\sum\limits_{j=0}^{K-1}W(j,m)\log_{10}\frac{X(j,m)^2}{\{X(j,m)-\hat{X}(j,m)\}^2}}{\sum\limits_{j=0}^{K-1}W(j,m)}
\end{equation}

where $W(j,m)$ is the weight of the $j^{th}$ frequency band in the $m^{th}$ frame, $K$ is the number of frequency bands and $X(j,m)$, $\hat{X}(j,m)$ are the spectral amplitude of the clean and distorted speech respectively. The weights can be chosen in many ways, one of which is the ANSI SII Standard \cite{ANSI_SII}.

\subsubsection{Perceptual Evaluation of Speech Quality}

One of the most popular objective speech quality measures is the ITU-T P.862: Perceptual Evaluation of Speech Quality (PESQ) \cite{PESQ}.

PESQ was developed to model subjective tests commonly used to assess the voice quality by human beings (e.g. MOS), using true voice samples as test signals. It is designed for use over a wide range of conditions. A mapping from PESQ to MOS scores was standardised, allowing PESQ results to model MOS scores that range from 1 (Bad) to 5 (Excellent) (typical of Table \ref{table:ACR_MOS}). The average correlation between PESQ-mapped MOS scores and subjective MOS for a number of tests was a high score of $0.935$ \cite{Evaluate_PESQ}. The block diagram of PESQ is shown in Figure \ref{fig:PESQ_blockdiagram} (taken from \cite{RBHH_PESQ}).

\begin{figure}[ht!]
\centering
\includegraphics[width=0.8\linewidth]{PESQ_model}
\captionof{figure}{Structure of PESQ model (taken from \cite{RBHH_PESQ})}
\label{fig:PESQ_blockdiagram}
\end{figure}

\section{Speech Intelligibility}

Speech intelligibility is the accuracy with which we can hear what is being said, and is a different performance measure as compared to perceived speech quality. Specifically, it is measured as the percentage of correctly identified words relative to the number of words. Instead of words, one may also use phonemes or syllables as the test unit. If words or complete sentences are used, they typically encompass linguistically meaningful units, and thus the choice of test words is important to ensure a fair assessment.

Although there does not exist a completely clear relationship between perceived speech quality and intelligibility, there exists some correlation between the two. Generally, ``good'' quality speech also gives high intelligibility and vice versa. However, this generalisation does not always hold; there are some samples that are highly intelligible yet are perceived as ``poor'' quality and vice versa.

\subsection{Short-Time Objective Intelligibility}

A widely-used method to evaluate intelligibility is the Short-Time Objective Intelligibility (STOI) measure \cite{STOI}. The basic structure is shown in Figure \ref{fig:STOI_model}.

\begin{figure}[ht!]
\centering
\includegraphics[width=0.99\linewidth]{STOI_model}
\captionof{figure}{Structure of STOI model (taken from \cite{STOI_orig})}
\label{fig:STOI_model}
\end{figure}

STOI requires both clean and noisy speech. Both of these are first time-frequency (T-F) decomposed to obtain a representation resembling the transform properties of the auditory system \cite{STOI_orig}.

STOI contains an intermediate-stage intelligibility measure, which compares the temporal envelopes of the clean and degraded speech in short-time segments using a correlation coefficient. Before this comparison, the temporal envelopes are first normalised and clipped. The normalisation process compensates for global level differences, which should not dominate the speech intelligibility, while clipping prevents over-sensitivity of the model towards excessively degraded T-F units. This ensures that if a T-F unit is already deemed unintelligible, further corruption of this unit does not lead to a lower intelligibility prediction.

These intermediate intelligibility measures are then averaged to a single value. Results have demonstrated that this value is highly correlated with the true speech intelligibility of noisy speech from multiple listening experiments \cite{STOI_orig}.


\chapter{Problem Analysis}

In this chapter, the project aims are analysed and broken down into specific deliverables. Following that is a discussion on the implementation of the baseline algorithm and how the proposed modifications will be made. Subsequent chapters discuss these modifications and the performance evaluation in greater detail.

\section{Deliverables}

The goal is to modify a Modulation-Domain Kalman Filter (KF) speech enhancement method by integrating data from an ideal binary mask (IBM). This project proposes a few methods to do so.

In the first method, IBM statistics are used to scale the predicted value and variance in the MDKF. This requires an implementation of the MDKF, and also involves obtaining the IBM statistical information. This can be broken down into a few deliverables, as follows:

1) Implement a working IBM (algorithm from \cite{LiLoizou2008})

2) Obtain statistical information from IBM from training data

3) Implement a working MDKF (algorithm from \cite{SP11})

4) Apply IBM information in a useful way to improve MDKF iterations

A second proposed method is to improve the linear prediction coefficients (LPCs) using the IBM information. In the MDKF, LPCs are estimated in each modulation frame. The goal is to study if these LPCs can be improved by incorporating data provided by an IBM:

1) Apply weighted sum to LPC estimation using weights determined from IBM

After implementing these methods, their performance will be evaluated using the Signal-to-Noise Ratio (SNR), Perceptual Evaluation of Speech Quality (PESQ) and Short-Time Objective Intelligibility (STOI) measures:

1) For a range of input noise SNR levels, run all algorithms

2) Evaluate using measures SNR, PESQ, STOI

This set of requirements and tests are necessary to evaluate if the proposed algorithms are able to provide improvements over existing methods. For evaluation, the testing methodology is standardised to ensure a fair assessment, and will be discussed later. All methods are also evaluated and averaged over multiple input speech samples to reduce the effect of outliers. 

\section{Implementation}\label{implementation}

In this project, the MDKF in Section \ref{MDKF} is used as a baseline algorithm. For the base MDKF, an acoustic frame length of $16$ ms was used with a $4$ ms frame shift. For each frequency bin, a modulation frame of $24$ ms was used with a $4$ ms frame increment to determine the LPC coefficients. An MDKF model order of $p=2$ was used. In most experiments, these settings are used for all algorithms to compare and evaluate performance.

\subsection{Optimal Modulation Frame Length}

In \cite{PSW2011}, it was shown that short modulation frame durations of $10-32$ ms retain good intelligibility overall as compared to longer frame lengths. This is briefly verified below.

Using clean speech as the ``noisy'' input speech to the MDKF filter, a plot of the average LPC excitation variance per modulation frame against a range of modulation frame lengths is shown below in Figure \ref{fig:compareModFrameLengths}, where the error is normalised against the length of the modulation frame and averaged over 40 randomly chosen speech samples from the TIMIT dataset.

\begin{figure}[ht!]
\centering
\includegraphics[width=0.6\linewidth]{compareModFrameLengths}
\captionof{figure}{Plot of normalised excitation variance vs. modulation frame lengths}
\label{fig:compareModFrameLengths}
\end{figure}

Figure \ref{fig:compareModFrameLengths} clearly shows that the error decreases as the modulation frame length increases, which makes sense intuitively as the modulation domain represents the variation in speech, which changes much more slowly than the actual speech itself. However, the numerical error does not fully represent the human perception of speech. Indeed, it was empirically found through informal listening tests that a $16-24$ ms modulation frame length produced acceptable levels of noise, while being much more intelligible as compared to speech enhanced using longer frame lengths, which agrees with the conclusions in \cite{PSW2011}. This project thus uses a modulation frame length of $24$ ms with a $4$ ms frame shift. Given a $4$ ms acoustic frame shift, the modulation domain sampling frequency is $250$ Hz, which gives $6$ samples per modulation frame.

\section{Kalman Filter Post Processing}

The framework for the proposed speech enhancement algorithms in this project are shown in Figure ??. Similarly to \cite{WB16}, the Kalman filter is applied not to the original noisy input signal, but to the output of an MMSE-enhanced spectrum, using the algorithm of \cite{MMSEspeech}.

The baseline system is the same as described in Section \ref{MDKF}: the noisy input time-domain speech signal undergoes the STFT transform and is enhanced by the MMSE algorithm. The amplitude spectrum of the enhanced speech is then used as input to the MDKF. The noise is estimated from the MMSE-enhanced amplitudes using the algorithm in \cite{MMSEnoiseBetter}. After processing through the Kalman filter, the output enhanced magnitudes are combined with the noisy phase spectrum, then processed through an inverse STFT to produce the output processed speech. In the ``oracle'' model, LPC coefficients are calculated from the clean speech, while in the practical model, LPCs are computed from the MMSE-enhanced speech, Although not as accurate as clean-speech LPCs, the effect of noise on the model is reduced when calculating LPCs from MMSE-enhanced speech rather than from the original noisy speech. In both cases, LPCs are computed from the spectral amplitudes within each frequency bin.


\chapter{Testing Methodology}

In this project, the objective is to propose modifications to existing speech enhancement algorithms, with the goal of improving the two aspects of speech quality: the overall perceived speech quality, and the speech intelligibility \cite{Kazuhiro2012}.

\section{Assessing Speech Quality}

The perceived overall speech quality is how ``good'' the quality of the speech is. The assessment is left to the listener who scores the speech according to a pre-defined rating system.

Methods to assess speech quality can be grouped into subjective and objective measures. As discussed before, many of the objective measures are highly correlated with subjective measures; it is thus common for a test to use objective measures, which are less time-consuming and cheaper, to approximate subjective methods.

In this report, the quality of the enhancement algorithms will be assessed using segSNR and PESQ. The effect of the enhancements on noise level will be assessed by segmental SNR (segSNR) while speech quality will be evaluated by Perceptual Evaluation of Speech Quality (PESQ).

\section{Assessing Speech Intelligibility}

On the other hand, speech intelligibility is the accuracy with which we can hear and identify what is being said. Typically, it is measured as the percentage of correctly identified words relative to the total number of words. In this report, speech intelligibility of the proposed speech enhancers will be evaluated using Short-Time Objective Intelligibility (STOI).

\section{Speech Database: TIMIT}

The TIMIT Acoustic-Phonetic Continuous Speech Corpus of read speech was designed to provide speech data for acoustic-phonetic studies as well as the development and evaluation of automatic speech recognition systems \cite{TIMIT}. It is widely used in the research and testing of speech enhancement algorithms. A combined effort between the Massachusetts Institute of Technology (MIT), SRI International (SRI) and Texas Instruments, Inc. (TI), the TIMIT database contains recordings of $630$ speakers of eight major dialects of American English, each reading ten phonetically rich sentences, each of which is a few seconds long. The recordings are $16$-bit resolution, $16$ kHz rate speech waveform files, and the database also includes time-aligned phonetic and word transcriptions.

% REWRITE

% For evaluating the algorithms proposed in this report, the core test set of the TIMIT database will be used which contains 16 male and 8 female speakers each reading 8 sentences for a total of 192 sentences all with distinct texts. This test set is the abridged version of the complete TIMIT test set which consists of 1344 sentences from 168 speakers. Also, in order to optimize the parameters of the algorithms, a development set is formed which consists of 200 speech sentences randomly selected from the test set of the TIMIT database and does not have any overlap with the core test set. The speech sentences in the development set are corrupted by white noise, car noise, factory noise, F16 noise and babble noise at SNRs between -10 and 15 dB at a 10

Some of the proposed algorithms require training samples; a set of $38$ randomly chosen samples from the TIMIT training database was used for such training purposes.

For general performance testing, a separate set of $38$ randomly chosen speech samples are used. These speech samples are corrupted by white noise at SNRs between $-20$ and $20$ dB.


\chapter{Modified Kalman Filter Inputs}

\section{Introduction}

From the modulation-domain Kalman filter (MDKF) iteration equations (Equations \ref{eq:MDKF_predictP} to \ref{eq:MDKF_updateXhat_linearCombi}), we know that two variables are used to form the updated state: the prediction of the current state and the observation of the current state. The observation is noisy; we can improve the accuracy of the updated state, and hence the algorithm performance, if the observation is modified to better represent the actual speech.

In this chapter, we propose to incorporate information from an ideal binary mask (IBM) to improve the accuracy of the updated state. This can be done in two ways: in the first method, the observation is tweaked directly, while in the second method, the updated state is instead constructed by combining information from the observation, the prediction and the mask altogether. Both methods are detailed and evaluated in this chapter.

\section{Incorporating Binary Mask into Observation}\label{IBMobs}

From Section \ref{IBM}, we know that the $0$ dB-threshold IBM produces a mask of $1$s and $0$s, where $1$s represent T-F units where the signal has higher energy than noise, and vice versa for $0$s, and that this threshold gives the best-performing mask overall. The observation used in the MDKF equations (Section \ref{MDKF}) is the original input noisy observation; if it is modified to better represent the inherent underlying speech (or silence in absence of speech), the algorithm can perform better. One way to do so is to incorporate the statistical quantities of an ideal binary mask, by multiplying together the probability density functions (PDFs) of the observation and the mask.

\subsection{Gaussian Product}\label{gaussianpdt}

Let $f(x)$ and $g(x)$ be two Gaussian PDFs with arbitrary means $a$ and $b$ and variances $A$ and $B$, which we represent as $f(x) \sim \mathcal{N}(x;a,A)$ and $g(x) \sim \mathcal{N}(x;b,B)$ respectively. Their product is also a product of two Gaussian PDFs \cite{Bro11}, of which one term is dependent on $x$ while the other is independent of $x$:

\begin{equation}\label{eq:prod2Gaussians}
\mathcal{N}(x;a,A)\ \mathcal{N}(x;b,B)=\mathcal{N}(a;b,A+B)\ \mathcal{N}(x;c,C)
\end{equation}

where $C$ and $c$ are defined as:

\begin{equation}\label{eq:prod2Gaussians_var}
C=(A^{-1}+B^{-1})^{-1}=\frac{AB}{A+B}
\end{equation}

\begin{equation}\label{eq:prod2Gaussians_mean}
c=C(\frac{a}{A}+\frac{b}{B})=\frac{Ab+Ba}{A+B}
\end{equation}

In the context of the MDKF, we are interested in $\mathcal{N}(x;c,C)$, the PDF dependent on $x$. Given an observation $y$ and clean speech $s$, we can represent their joint probability $p(s,y)$ in two ways:

\begin{equation}\label{eq:jointprob}
p(s,y)=p(s)\ p(y|s)=p(y)\, p(s|y)
\end{equation}

Using Bayes' Theorem, we can express this as:

\begin{equation}\label{eq:BayesTheorem}
p(s|y)=\frac{p(y|s)\ p(s)}{p(y)}
\end{equation}

where $p(s)$ is the prior probability of the predicted speech, $p(y)$ is the probability of the observation, $p(s|y)$ is the conditional probability of the speech given the observation and $p(y|s)$ is the conditional probability of the observation given speech.

In the MDKF, we want $p(s|y)$ i.e. we wish to estimate the clean speech given the noisy measurement. To compute this, we require the probabilities $p(y|s)$ and $p(s)$ (Equation \ref{eq:BayesTheorem}). We already have $p(y|s)$ as the noisy observation given clean speech, which comes from the measurement and the assumption of an additive noise model $y=s+n$, where $n$ is the noise. The other term, $p(s)$, can be obtained from an IBM, where we assume that the mask is as accurate as possible; how such a mask is generated in a real-world scenario is not discussed in this report.

Mapping the available and desired probabilities to Equation \ref{eq:prod2Gaussians}, we have available $f(s)=p(s)$ and $g(s)=p(y|s)$. By multiplying these two terms together, we expect a term independent of $s$ and a term dependent on $s$, which are $p(y)$ and $p(s|y)$ respectively. The latter term, which is the conditional probability of the (true) clean speech given the noisy observation, is what we propose to replace the noisy observation term in the MDKF equations.

\subsection{Training Mask Statistics}

The method that we propose to obtain $p(s)$ requires statistical information from an IBM. To obtain the PDF, we need its mean and variance. The mean and variances were calculated separately for mask $1$s and $0$s i.e. separate PDFs were generated for when the mask indicates that speech is dominant and when noise is dominant. This was done separately for each frequency bin.

To generate the IBM, the STFT of the signal is first taken, producing a matrix of time-frequency (T-F) units, and the unit-wise signal-to-noise ratio (SNR) is then computed to produce a binary-valued matrix. In the MDKF setup, the same STFT of the signal is taken; the mask thus provides a speech-dominant/noise-dominant indicator for each T-F unit of the STFT-processed signal.

To generate the speech-dominant PDF for one frequency bin in a speech signal, we picked out the samples of the amplitude spectrum of this frequency bin which had corresponding (the same T-F location) mask values indicating $1$. The mean and variance of these samples was then computed. The same process was done to compute the noise-dominant PDF for the same frequency bin. This was done for each frequency bin, and the values were averaged over all speech samples in the training dataset. All training samples were corrupted by white noise at $5$ dB SNR, which was found to maximise the IBM accuracy. To save time, the training of the mask statistics was computed once offline from the MDKF algorithm, with its values stored offline to pull when needed.

\subsection{Modifying Observation}

With the IBM statistics, the observation can then be modified. For reference, the MDKF equations are replicated from Section \ref{MDKF} and shown below:

\begin{equation}\label{eq:MDKF_predictP_repeat}
\textbf{P}(n|n-1,k)=\textbf{A}(k)\textbf{P}(n-1|n-1,k)\textbf{A}(k)^T+\sigma^2_{W(k)}\textbf{d}\textbf{d}^T
\end{equation}

\begin{equation}\label{eq:MDKF_predictXhat_repeat}
\hat{\textbf{X}}(n|n-1,k)=\textbf{A}(k)\hat{\textbf{X}}(n-1|n-1,k)
\end{equation}

\begin{equation}\label{eq:MDKF_updateK_repeat}
\textbf{K}(n,k)=\textbf{P}(n|n-1,k)\textbf{d}[\sigma^2_{V(k)}+\textbf{d}^T\textbf{P}(n|n-1,k)\textbf{d}]^{-1}
\end{equation}

\begin{equation}\label{eq:MDKF_updateP_repeat}
\textbf{P}(n|n,k)=[\textbf{I}-\textbf{K}(n,k)\textbf{d}^T]\textbf{P}(n|n-1,k)
\end{equation}

\begin{equation}\label{eq:MDKF_updateXhat_repeat}
\hat{\textbf{X}}(n|n,k)=\hat{\textbf{X}}(n|n-1,k)+\textbf{K}(n,k)[|Y(n,k)|-\textbf{d}^T\hat{\textbf{X}}(n|n-1,k)]
\end{equation}

where the observation mean is $|Y(n,k)|$ and the observation variance is in $\sigma^2_{V(k)}$. As the MDKF loops through each sample in each modulating signal, it tweaks these parameters for every sample. In each iteration, the algorithm checks the corresponding T-F position in the mask if it indicates a $1$ or $0$, then brings up the relevant mask mean and variance. Using Equations \ref{eq:prod2Gaussians_var} and \ref{eq:prod2Gaussians_mean}, a scaled mean and variance is obtained for the current observation input by multiplying with the relevant mask PDF. This process replaces the original observation mean $|Y(n,k)|$ and variance $\sigma^2_{V(k)}$ in the MDKF equations with $|Y\_scaled|$ and $\sigma^2_{V(k)\_scaled}$ respectively. We thus get modified versions of Equations \ref{eq:MDKF_updateK_repeat} and \ref{eq:MDKF_updateXhat_repeat}:

\begin{equation}\label{eq:MDKF_updateK_IBMmodifyobs}
\textbf{K}(n,k)=\textbf{P}(n|n-1,k)\textbf{d}[\sigma^2_{V(k)\_scaled}+\textbf{d}^T\textbf{P}(n|n-1,k)\textbf{d}]^{-1}
\end{equation}

\begin{equation}\label{eq:MDKF_updateXhat_IBMmodifyobs}
\hat{\textbf{X}}(n|n,k)=\hat{\textbf{X}}(n|n-1,k)+\textbf{K}(n,k)[|Y\_scaled|-\textbf{d}^T\hat{\textbf{X}}(n|n-1,k)]
\end{equation}

\section{Modified Kalman Filter Equations}\label{IBM_KF}

The IBM can be used in a slightly different way; instead of tweaking the observation itself, information from the mask can be directly used in the KF equations. Now, the KF equations use three pieces of information (prediction, observation, binary mask) to estimate the current state, rather than just the former two as in the original KF equations.

\subsection{Decoupling Kalman Filter Equations}

To insert the mask information, the KF equations first need to be decoupled. For the scalar output case i.e. where $|Y(n,k)|$ is a scalar, with $\textbf{d}=[1,0,...,0]^T$, the observation can be decorrelated from the rest of the state vector. Since each modulating signal has a separate Kalman filter, we remove the frequency bin subscript $k$ in this section, and also indicate the time sample index in the subscript for clarity e.g. we represent $\textbf{P}(n|n-1,k)$ as $\textbf{P}_{n|n-1}$. After obtaining the \textit{a priori} error covariance matrix (Equation \ref{eq:MDKF_predictP_repeat}), we can decompose it as:

\begin{equation}\label{eq:decomposePpredicted}
\textbf{P}_{n|n-1}=\left[
                  \begin{array}{ c c }
                  g_n 			& \textbf{g}_n^T \\
                  \textbf{g}_n 	& \textbf{G}_n
                  \end{array} \right]
\end{equation}

noting that the covariance matrix is symmetric. We then apply a transformation $\textbf{R}_n$ to the state vector $\textbf{X}_{n|n-1}$ to obtain:

\begin{equation}\label{eq:transformState}
\textbf{z}_{n|n-1}=\textbf{R}_n\textbf{X}_{n|n-1} = \\ 
					  \left[
                      \begin{array}{ c c }
                      1 						& \textbf{0}^T \\
                      -g_n^{-1}\textbf{g}_n 	& \textbf{I}
                      \end{array} \right]\textbf{X}_{n|n-1}
\end{equation}

The covariance matrix of \textbf{z} is given by (omitting subscript n for clarity):

\begin{equation}\label{eq:zcovariance}
\begin{split}
\langle \textbf{zz}^T \rangle &= \textbf{R} \langle \textbf{X}\textbf{X}^T \rangle \textbf{R}^T = \textbf{RPR}^T \\
							  &=
                                \left[
                                \begin{array}{ c c }
                                1 				  & \textbf{0}^T \\
                                -g^{-1}\textbf{g} & \textbf{I}
                                \end{array} \right]
                                %
                                \left[
                                \begin{array}{ c c }
                                g 			& \textbf{g}^T \\
                                \textbf{g} 	& \textbf{G}
                                \end{array} \right]
                                %
                                \left[
                                \begin{array}{ c c }
                                1 			 & -g^{-1}\textbf{g} \\
                                \textbf{0}^T & \textbf{I}
                                \end{array} \right] \\
                              &=
                                \left[
                                \begin{array}{ c c }
                                g 			& \textbf{0}^T \\
                                \textbf{0} 	& \textbf{G}-g^{-1}\textbf{gg}^T
                                \end{array} \right]  
\end{split}
\end{equation}

which shows that the first element of \textbf{z} is uncorrelated with the rest of \textbf{z}, and is distributed as $\mathcal{N}(\textbf{d}^T\textbf{z}_{n|n-1},g)$. This can then be combined with the observation to obtain the distribution of the updated state, which produces the same formulation as the original Kalman filter equations.

\subsection{Incorporating IBM into decoupled KF equations}

With the mask information available, the mask can be combined with the observation and decoupled state \textbf{z} to improve the KF iterations. Unlike Section \ref{IBMobs}, now the product of three distributions has to be taken, which is done pairwise using the steps in Section \ref{gaussianpdt}.

Assume that the observation has been combined with the relevant mask statistics (recall that speech-dominant ``1''s and ``0''s are distributed separately) using the steps covered in Section \ref{gaussianpdt}, and assume that the result is distributed as $\mathcal{N}(y,r)$. This is then combined with the first element of \textbf{z} to obtain the posterior distribution $\mathcal{N}(\textbf{d}^T\textbf{z}_n;\frac{gy_n+r\textbf{d}^T\textbf{z}_{n|n-1}}{g+r},\frac{gr}{g+r})$.

Its mean value is

\begin{equation}\label{eq:zfirstelementposterior}
\textbf{d}^T\textbf{z}_{n|n}=\frac{1}{g+r}(gy+r\textbf{d}^T\textbf{z}_{n|n-1})=\textbf{d}^T\textbf{z}_{n|n-1}+\frac{g}{g+r}(y-\textbf{d}^T\textbf{z}_{n|n-1})
\end{equation}

For the entire transformed state $\textbf{z}_n$, we can write

\begin{equation}\label{eq:zposterior}
\textbf{z}_{n|n}=\textbf{z}_{n|n-1}+\frac{g}{g+r}(y-\textbf{d}^T\textbf{z}_{n|n-1})\textbf{d}
\end{equation}

Finally, the original state $\textbf{X}_{n|n}$ can be obtained with the reverse transformation:

\begin{equation}\label{eq:Xposterior}
\begin{split}
\textbf{X}_{n|n} &= \textbf{A}^{-1}\textbf{z}_{n|n}\\
                 &=
                  \left[
                  \begin{array}{ c c }
                  1 			   & \textbf{0}^T \\
                  g^{-1}\textbf{g} & \textbf{I}
                  \end{array} \right]\textbf{z}_{n|n}\\
\end{split}
\end{equation}

\section{Performance Results and Discussion}

In this section, the IBM-modified MDKF algorithm will be termed BMMDKF, and uses the modification described in Section \ref{IBM_KF}. For testing, the performance of the BMMDKF will be compared to a few control algorithms: the original MDKF of Section \ref{MDKF}, the MMSE speech enhancement algorithm and the original noisy input speech. These are respectively named MDKF, MMSE and Noisy in the plots that follow. As a theoretical comparison, all Kalman filter implementations evaluated utilise LPCs generated from clean speech.

For standardisation of testing, the parameters used generally follow those described in Chapter \ref{implementation}. For clarity, these parameters are shown in Table \ref{table:BMMDKFparams}.

\begin{table}[ht!]
\centering
\begin{tabular}{|c|c|}
\hline
\textbf{Parameter}      & \textbf{Value} \\ \hline
Sampling frequency      & 16 kHz         \\ \hline
Acoustic frame length   & 16 ms          \\ \hline
Acoustic frame shift    & 4 ms           \\ \hline
Modulation frame length & 24 ms          \\ \hline
Modulation frame shift  & 4 ms           \\ \hline
Windowing function      & Hamming window \\ \hline
MDKF model order        & 2              \\ \hline
LPCs generated from     & clean speech   \\ \hline
IBM SNR threshold (LC)  & 0 dB           \\ \hline
\end{tabular}
\caption{List of parameters used to evaluate algorithms}
\label{table:BMMDKFparams}
\end{table}

\newpage


\begin{figure}[ht!]
\centering
\includegraphics[width=0.7\linewidth]{BMMDKF_MDKF_MMSE_Noisy_segSNR}
\captionof{figure}{Average segSNR values of different algorithms vs. speech corrupted by white noise at varying SNR levels}
\label{fig:BMMDKF_MDKF_MMSE_Noisy_segSNR}
\end{figure}

\begin{figure}[ht!]
\centering
\includegraphics[width=0.7\linewidth]{BMMDKF_MDKF_MMSE_Noisy_pesq}
\captionof{figure}{Average PESQ values of different algorithms vs. speech corrupted by white noise at varying SNR levels}
\label{fig:BMMDKF_MDKF_MMSE_Noisy_pesq}
\end{figure}

\section{Evaluation}

\subsection{Algorithm Runtime}

With separate PDFs for the signal-dominant and noise-dominant portions of the mask, a condition needs to be checked for each iteration to pull the correct mask statistics. This check, along with the modification of the observation mean and variance, slows the algorithm down by a significant amount. Currently, the enhancer is not being used in a real-time scenario, and thus speed is not crucial. If it is used in a real-time context in the future, however, the code will need to be modified so as to minimise the time delay of the enhancement.



\chapter{Improved LPC Coefficients}

\section{Introduction}

The framework for the estimation of the LPC coefficients used in the MDKF was described earlier in Section \ref{LPC}. Since the LPCs aim to mimic the linear prediction model of speech, the accuracy of LPC estimation is critical to the performance of the enhancer. In this section, we proposed a simple modification to the LPC estimation by incorporating information from an available IBM, with the aim of improving the coefficients generated.

\section{Weighted LPC estimation}

As described in Section \ref{LPC}, LPC estimation using the autocorrelation method minimises the mean squared error by minimising the total prediction error $E$ over all samples.

Instead of a simple summation of all errors, we can instead have a weighted sum that better represents the speech; this information can be provided by a binary mask first applied to the speech. We can then modify Equation \ref{eq:LPC_totalpredictionerror} to get:

\begin{equation}\label{eq:weightedLPC_totalpredictionerror}
E=\sum_{n}w(n)e^2(n)=\sum_{n}w(n)[s(n)+\sum_{k=1}^{p}a_k s(n-k)]^2
\end{equation}

where $w(n)$ represents the weight attached to each speech sample $s(n)$.



\section{Performance Results and Discussion}

Similarly to previous experiments, the algorithm modified in this section is run against other control algorithms, with the parameters used shown in Table \ref{table:LMDKFparams}. As before, LPCs for all Kalman filter algorithms are generated from clean speech.

In this section, the enhanced-LPC algorithm will be termed LMDKF, with the other algorithms named as before.

\begin{table}[ht!]
\centering
\begin{tabular}{|c|c|}
\hline
\textbf{Parameter}      & \textbf{Value} \\ \hline
Sampling frequency      & 16 kHz         \\ \hline
Acoustic frame length   & 16 ms          \\ \hline
Acoustic frame shift    & 4 ms           \\ \hline
Modulation frame length & 24 ms          \\ \hline
Modulation frame shift  & 4 ms           \\ \hline
Windowing function      & Hamming window \\ \hline
MDKF model order        & 2              \\ \hline
LPCs generated from     & clean speech   \\ \hline
LPC weight threshold    & 0.15            \\ \hline
\end{tabular}
\caption{List of parameters used to evaluate algorithms}
\label{table:LMDKFparams}
\end{table}


\newpage

\begin{figure}[ht!]
\centering
\includegraphics[width=0.7\linewidth]{LMDKF_MDKF_MMSE_Noisy_segSNR}
\captionof{figure}{Average segSNR values of different algorithms vs. speech corrupted by white noise at varying SNR levels}
\label{fig:LMDKF_MDKF_MMSE_Noisy_segSNR}
\end{figure}

\begin{figure}[ht!]
\centering
\includegraphics[width=0.7\linewidth]{LMDKF_MDKF_MMSE_Noisy_pesq}
\captionof{figure}{Average PESQ values of different algorithms vs. speech corrupted by white noise at varying SNR levels}
\label{fig:LMDKF_MDKF_MMSE_Noisy_pesq}
\end{figure}

\section{Evaluation}


\chapter{IBM-improved Noise Estimation}

\section{Introduction}

Noise estimation is a critical part of speech enhancement, and the performance of enhancement algorithms is heavily affected by the accuracy of the noise estimation. In this chapter, a modification of an existing noise spectral estimation method based on an ideal binary mask (IBM) is proposed.

A popular method to estimate the noise power spectral density (PSD) is to use a minimum mean-squared-error (MMSE) optimal estimation method, which can be interpreted as a VAD-based noise power estimator. This chapter proposes a few tweaks to the parameters used in the algorithm described in \cite{MMSEnoiseBetter}, based on the information provided by an IBM.

\section{MMSE Noise Estimation}

To aid the understanding of the modifications made in this chapter, we first present a summary of the MMSE noise estimator in \cite{MMSEnoiseBetter}. A majority of the equations are left out for clarity, and only those critical to the proposed modification will be inserted in this report.

Let $y$, $s$ and $n$ be the noisy speech, clean speech and noise respectively, and assume that they are additive in the short-time Fourier domain, giving the noisy observed speech as $Y=S+N$, where time and frequency indices are omitted for convenience and $S$, $N$ are the complex spectral speech and noise components respectively. The clean speech and noise are assumed independent, such that $\mathbb{E}(|Y|^2)=\mathbb{E}(|S|^2)+\mathbb{E}(|N|^2)$. Define the spectral speech and noise power as $\mathbb{E}(|S|^2)=\sigma_S^2$ and $\mathbb{E}(|N|^2)=\sigma_N^2$ respectively, and the \textit{a priori} and \textit{a posteriori} Signal-to-Noise Ratio (SNR) by $\zeta=\sigma_S^2/\sigma_N^2$ and $\gamma=|y|^2/\sigma_N^2$ respectively.

It is assumed that the noise and speech spectral coefficients ($p_N(n)$ and $p_S(s)$ respectively) have complex Gaussian distributions. This gives a complex Gaussian distribution for the noisy speech $p_Y(y)$ which depends on the true SNR. The noise power estimator shown in \cite{MMSEnoise} is based on an MMSE estimate of the noise periodogram, which can be obtained by calculating the conditional expectation $\mathbb{E}(|N|^2|y)$ (equation omitted for clarity), which is a function of the power of the noisy observation, the \textit{a priori} SNR and the spectral noise power.

In practice, the \textit{a priori} SNR and the spectral noise power have to be estimated. When estimating noise power, it is a common assumption that the noise signal varies more slowly than speech \cite{noiseVarySlower}. Therefore, \cite{MMSEnoise} uses the spectral noise power estimate of the previous time frame ($l-1$) i.e. $\hat{\sigma}_N^2=\hat{\sigma}_N^2(l-1)$, assuming some correlation between the noise in adjacent frames of speech.

Estimating the \textit{a priori} SNR is more complicated, as speech tends to vary more rapidly between successive frames. In \cite{MMSEnoise}, the proposed method to estimate $\hat{\zeta}$ uses a maximum-likelihood (ML) estimate followed by bias compensation. When doing so, the MMSE estimator can be viewed as a hard-threshold voice activity detector (VAD) based decision between the noisy observation and the spectral noise power estimate. The resultant estimator is biased and requires bias compensation.

Finally, after estimating the noise periodogram from $\mathbb{E}(|N|^2|y)$, the noise PSD is obtained via recursive smoothing with a parameter $\alpha_{pow}=0.8$ \cite{MMSEnoise}:

\begin{equation}\label{eq:noisePSDfromPeriodogram}
\hat{\sigma}_N^2(l)=\alpha_{pow}\hat{\sigma}_N^2(l-1)+(1-\alpha_{pow})\mathbb{E}(|N|^2|y(l))
\end{equation}

\subsection{Assumptions in Unbiased MMSE Estimator}

The previous section dealt with the original noise power MMSE estimator as described in \cite{MMSEnoise}. A modification was made in \cite{MMSEnoiseBetter} to produce an unbiased estimator based on a soft speech presence probability (SPP) estimate with fixed priors, instead of a hard VAD threshold. This section covers part of the modification, and in particular highlights certain assumptions made that can be improved using IBM information.

When speech presence is uncertain, an MMSE estimator for the noise periodogram is given by:

\begin{equation}\label{eq:noisePeriodogramUncertainSPP}
\mathbb{E}(|N|^2|y)=P(H_0|y)\mathbb{E}(|N|^2|y,H_0)+P(H_1|y)\mathbb{E}(|N|^2|y,H_1)
\end{equation}

where $H_0$ and $H_1$ indicate speech absence and speech presence respectively. Using Bayes' Theorem, the \textit{a posteriori} SPP can be expressed as:

\begin{equation}\label{eq:aposterioriSPP}
P(H_1|y)=\frac{P(H_1)p_{Y|H1}(y)}{P(H_0)p_{Y|H0}(y)+P(H_1)p_{Y|H1}(y)}
\end{equation}

Thus, computing the \textit{a posteriori} SPP requires the \textit{a priori} probabilities $P(H_1)=1-P(H_0)$ and the likelihood functions for speech presence $p_{Y|H1}(y)$ and speech absence $p_{Y|H0}(y)$. Without an observation, \cite{MMSEnoiseBetter} assumes that a time-frequency point being considered is equally likely to contain speech or not contain speech i.e. uniform priors $P(H_1)=P(H_0)=0.5$ were chosen, independent of the observation. Given an IBM, this can be improved.

The likelihood functions $p_{Y|H1}(y)$ and $p_{Y|H0}(y)$ in Equation \ref{eq:aposterioriSPP} indicate how well the observation $y$ fits the modelling parameters for speech presence and absence respectively, and can be modelled with complex Gaussian distributions (equations omitted). The likelihood under speech presence depends on the \textit{a priori} SNR. While the algorithm in \cite{MMSEnoise} uses a complex Gaussian distribution for the noisy observation $p_Y(y)$ which depends on the true local \textit{a priori} SNR, the likelihood under speech presence is instead a function of a parameterised \textit{a priori} SNR, reflecting the typical SNR when speech is present. In \cite{MMSEnoiseBetter}, this typical SNR is fixed at $15$ dB. This is another area in which information from an IBM can be useful.

Substituting the likelihood functions for speech absence and speech presence (equations omitted in this report) into Equation \ref{eq:aposterioriSPP}, an expression for the \textit{a posteriori} SPP can be obtained as:

\begin{equation}\label{eq:aposterioriSPP_likelihood}
P(H_1|y)=\Bigg(1+\frac{P(H_0)}{P(H_1)}(1+\zeta_{H_1})e^{-\frac{|y|^2}{\hat{\sigma}_N^2}\frac{\zeta_{H_1}}{1+\zeta_{H_1}}}\Bigg)^{-1}
\end{equation}

where the spectral noise power estimate of the previous time frame is used i.e. $\hat{\sigma}_N^2=\hat{\sigma}_N^2(l-1)$ as before.

If the noise power estimate $\hat{\sigma}_N^2$ underestimates the true noise power $\sigma_N^2$, the \textit{a posteriori} SPP in Equation \ref{eq:aposterioriSPP_likelihood} will be overestimated, and the noise power will not be tracked as quickly as needed. In the worst case, the noise power might remain the same. To avoid this stagnation due to underestimated noise power, a check is done in \cite{MMSEnoiseBetter} to verify if the \textit{a posteriori} SPP has been close to $1$ for a long time. The \textit{a posteriori} SPP is first recursively smoothed over time: $\bar{P}(l)=0.9\bar{P}(l-1)+0.1P(H_1|y(l))$. If this smoothed quantity is larger than $0.99$, the update is deemed to have stagnated; the current \textit{a posteriori} SPP estimate $P(H_1|y(l))$ is then forced to be lower than $0.99$.

The noise power spectrum can then be computed from the estimated \textit{a posteriori} SPP $P(H_1|y(l))$, which is used to weight the noisy input power spectrum $|Y|^2$ and the previous estimate of the noise power $\hat{\sigma}_N^2(l-1)$ accordingly:

\begin{equation}\label{eq:rawNoiseSpectrum}
\mathbb{E}(|N|^2|y(l))=P(H_1|y(l))\hat{\sigma}_N^2(l-1)+[1-P(H_1|y(l))]|Y(l)|^2
\end{equation}

Finally, this raw noise estimate is smoothed as in Equation \ref{eq:noisePSDfromPeriodogram} to obtain the estimated noise PSD.


\section{Modifying Noise Estimation}

In the previous section which briefly reviewed the MMSE noise estimators of \cite{MMSEnoise} and \cite{MMSEnoiseBetter}, some parameters are notably fixed. In this section, we use an IBM to provide information that allows us to vary these parameters.

LINK THIS TO FINDING OPTIMAL THRESHOLD BELOW

\section{Optimal IBM-Modified Estimator Threshold}

Given the modification, a set of thresholds for the binary mask needed to be found. An optimal ratio was found to be:



\begin{figure}[ht!]
\centering
\includegraphics[width=0.7\linewidth]{estnoise_segSNRratios}
\captionof{figure}{Average segSNR ratios comparing IBM-modified noise estimation with original MMSE noise estimation vs. speech corrupted by white noise at varying SNR levels}
\label{fig:estnoise_segSNRratios}
\end{figure}

\begin{figure}[ht!]
\centering
\includegraphics[width=0.7\linewidth]{estnoise_PESQratios}
\captionof{figure}{Average PESQ ratios comparing IBM-modified noise estimation with original MMSE noise estimation vs. speech corrupted by white noise at varying SNR levels}
\label{fig:estnoise_PESQratios}
\end{figure}

\begin{figure}[ht!]
\centering
\includegraphics[width=0.7\linewidth]{estnoise_STOIratios}
\captionof{figure}{Average STOI ratios comparing IBM-modified noise estimation with original MMSE noise estimation vs. speech corrupted by white noise at varying SNR levels}
\label{fig:estnoise_STOIratios}
\end{figure}

\subsection{Estimated Global SNR}

\begin{figure}[ht!]
\centering
\includegraphics[width=0.7\linewidth]{estnoise_compareglobalSNR}
\captionof{figure}{Estimated global SNR of different noise estimation methods vs. actual global SNR of input speech}
\label{fig:estnoise_compareglobalSNR}
\end{figure}

In Figure \ref{fig:estnoise_segSNRratios}, we saw that the segSNR of the optimal-threshold ($th=1.0$) IBM-modified noise estimator is largely lower than that of the original MMSE noise estimator of \cite{MMSEnoiseBetter}. This is verified in Figure \ref{fig:estnoise_compareglobalSNR}.

Figure \ref{fig:estnoise_compareglobalSNR} compares the averaged estimated global SNR of the IBM-modified noise estimation algorithm and the original MMSE noise estimator, plotted against the actual input global SNR. In the plot, the true $45^{\circ}$ line is represented by a green dotted line i.e. the closer to the green line, the more accurate the SNR estimation. The plot shows that both algorithms tend to underestimate the noise level (overestimate SNR) when the noise is very large ($-15$ dB input SNR or lower), and vice versa when the noise is small (high input SNR). However, an across-the-board feature is that the original MMSE noise estimator largely matches the real value better than the IBM-modified noise estimator.

A possible explanation for this is that the noise is random, and the MMSE noise estimator in \cite{MMSEnoiseBetter} uses only a fixed \textit{a priori} SNR value typical of speech. However, incorporating IBM data meant tweaking the original spectral components, which meant modifying this SNR value adaptively. Using the threshold values determined in Figures \ref{fig:estnoise_PESQratios} and \ref{fig:estnoise_STOIratios} better represented the actual speech information, but might not represent the numerical SNR as well as the original algorithm.


\section{Performance Results and Discussion}

As with all previous experiments, the algorithm discussed in this section is run against other control algorithms, with the parameters used shown in Table \ref{table:NMDKFparams}.

In this section, the algorithm using the IBM-enhanced noise estimate will be termed NMDKF, with the other algorithms named as before.

\begin{table}[ht!]
\centering
\begin{tabular}{|c|c|}
\hline
\textbf{Parameter}      & \textbf{Value} \\ \hline
Sampling frequency      & 16 kHz         \\ \hline
Acoustic frame length   & 16 ms          \\ \hline
Acoustic frame shift    & 4 ms           \\ \hline
Modulation frame length & 24 ms          \\ \hline
Modulation frame shift  & 4 ms           \\ \hline
Windowing function      & Hamming window \\ \hline
MDKF model order        & 2              \\ \hline
LPCs generated from     & clean speech   \\ \hline
IBM zeros threshold     & 1.0            \\ \hline
IBM ones threshold      & 2.5            \\ \hline
\end{tabular}
\caption{List of parameters used to evaluate algorithms}
\label{table:NMDKFparams}
\end{table}

\section{Evaluation}



\chapter{Conclusion and Future Work}


\section{Future Work}


\subsection{}

for coloured noise/non-stationary noise assumption etc. better estimate of noise?

better noise distribution? currently using gaussian for modification

pesq/stoi highly correlated, but if possible would prefer to do human tests? lack of time and resources, as well as access to a highly specific test group (native english speaker, same place of origin and growing up etc.)


\newpage
\addcontentsline{toc}{chapter}{Bibliography}
\printbibliography


\appendix


\chapter{MATLAB}



\end{document}